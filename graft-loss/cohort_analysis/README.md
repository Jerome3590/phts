## Cohort Analytic Options (COAs) and IPCW Weighting

This repository prepares three 1-year outcome datasets (COAs) for modeling and evaluation. The data are generated by knitting `phts_dataset.qmd` and saved as CSVs in the project root.

### Outputs

- `preprocessed_model_data_coa1.csv`: Observed-only labels at 1 year
- `preprocessed_model_data_coa2.csv`: Observed-only labels at 1 year, restricted to `txpl_year < 2023`
- `preprocessed_model_data_coa3.csv`: Same labels as COA 1 plus an `ipcw_weight` column for inverse-probability-of-censoring weighting (IPCW)

All three datasets carry the engineered feature set used by downstream modeling notebooks.

### Labels and Cohort Definitions

- Event indicator is `outcome` (1 = event, 0 = no event by 1 year)
- Event time is `ev_time` (years).
- Event type is `ev_type` (1 = event occurred, 0 = censored/no event at last follow-up).

COA details:

- COA 1 (Observed-only):
  - outcome = 1 if `ev_type == 1` and `ev_time < 1`
  - outcome = 0 if `ev_time >= 1`
  - otherwise outcome = NA (drop; censored before 1 year)

- COA 2 (Observed-only, full follow-up):
  - Same label rule as COA 1, then filter to `txpl_year < 2023` to maximize 1-year follow-up completeness.

- COA 3 (IPCW-weighted):
  - Same label rule as COA 1 (observed-only labels), but adds an inverse-probability-of-censoring weight `ipcw_weight` at horizon τ = 1 year.
  - `ipcw_weight = 1 / Ĝ(t*)`, where `Ĝ(·)` is the Kaplan–Meier estimate of the censoring survival function. We estimate `Ĝ` from `survfit(Surv(ev_time, I(ev_type == 0)) ~ 1)`; i.e., treating censoring as the “event” in the censoring model.
  - For events before 1 year, `t* = min(ev_time, 1)`; for non-events, `t* = 1`.
  - We bound the denominator away from zero using `pmax(Ĝ(t*), .Machine$double.eps)` to avoid extreme weights. You may optionally truncate weights (e.g., to [0.01, 100]) in modeling code if needed.

Why IPCW for classification?

- For 1-year classification, patients censored before 1 year have unknown labels and must be excluded. IPCW corrects the selection bias among the included (observed-only) patients by up-weighting those with higher censoring risk.

### Using IPCW weights in models

Use `preprocessed_model_data_coa3.csv` and pass `ipcw_weight` as observation/case weights. Examples:

R (glmnet, logistic classification):

```r
library(glmnet)
fit <- cv.glmnet(x, y, family = "binomial", weights = df$ipcw_weight)
```

R (CatBoost classification):

```r
library(catboost)
pool_tr <- catboost.load_pool(data = x_tr, label = y_tr, weight = df_tr$ipcw_weight)
pool_te <- catboost.load_pool(data = x_te, label = y_te)
model <- catboost.train(pool_tr, pool_te, params = list(loss_function = 'Logloss'))
```

Python (CatBoost classification):

```python
from catboost import CatBoostClassifier, Pool
train_pool = Pool(X_tr, y_tr, weight=w_tr)
test_pool = Pool(X_te, y_te)
model = CatBoostClassifier(loss_function='Logloss')
model.fit(train_pool, eval_set=test_pool)
```

Random Forest:

- Base `randomForest` does not natively support per-row case weights for classification. Prefer `ranger::ranger(..., case.weights=...)` in R, or use CatBoost RF-mode and supply weights via pool objects.

Survival models (if extended to IPCW):

- Many survival learners accept observation weights. When training a survival model specifically for 1-year risk, align the weight scheme with the target horizon and pass `weights = ipcw_weight` (glmnet), or pool weights (CatBoost).

### Reproducing the files

1. Ensure the raw SAS inputs are available under `../data/`.
2. Knit `phts_dataset.qmd`.
3. Confirm the presence of:
   - `preprocessed_model_data_coa1.csv`
   - `preprocessed_model_data_coa2.csv`
   - `preprocessed_model_data_coa3.csv`

### Key assumptions and caveats

- `ev_time` is in years; τ = 1 year. Adjust if your time unit changes.
- `ev_type` and `outcome` are coded 1 = event, 0 = censored/no event.
- IPCW corrects for censoring among included cases; it does not impute labels for those censored before τ.
- Consider weight truncation in highly censored regimes to stabilize training.


