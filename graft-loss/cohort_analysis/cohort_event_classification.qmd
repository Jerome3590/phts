---
title: "Event Classification Models by Cohort"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 6
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
editor: 
  markdown: 
    wrap: 72
---

## Overview

This analysis examines predictive models for pediatric heart transplant
outcomes by comparing two distinct patient cohorts: Congenital Heart
Disease (CHD) and Myocarditis/Cardiomyopathy. We implement four
classification models (LASSO, CatBoost, CatBoost Random Forest, and
Traditional Random Forest) to provide comprehensive risk assessment
and identify cohort-specific predictive factors for event classification.

### Modeling Approaches

This study employs four distinct machine learning methodologies, each
offering unique advantages for pediatric heart transplant outcome
prediction:

**1. LASSO (Least Absolute Shrinkage and Selection Operator) for Classification**

-   **Method**: A logistic regression model regularized with an L1
    penalty for feature selection and binary outcome prediction.
-   **Strengths for Our Use Case**:
    -   **Clinical Interpretability**: Provides clear coefficient values
        (log-odds ratios) that clinicians can easily understand.
    -   **Feature Selection**: Automatically identifies the most
        impactful variables on outcomes, reducing model complexity.
    -   **Transparent Decision Making**: Linear relationships in the
        log-odds allow for straightforward risk factor quantification.
    -   **Regulatory Compliance**: The logistic model is a well-established
        method with extensive validation in medical literature.
    -   **Handles Imbalanced Data**: Can be adapted for imbalanced
        outcome distributions common in medical datasets.
-   **Limitations**:
    -   **Linear Assumptions**: Models the log-odds as a linear
        combination of predictors, potentially missing complex
        non-linear relationships.
    -   **Feature Interactions**: Limited ability to capture
        interactions between clinical variables without explicit
        specification.
    -   **Outcome Threshold**: Requires setting a probability threshold
        for binary classification decisions.

**2. CatBoost (Categorical Boosting) for Classification**

-   **Method**: Gradient boosting classification model using binary
    cross-entropy loss for outcome prediction.
-   **Strengths for Our Use Case**:
    -   **Categorical Handling**: Excellent performance with medical
        coding systems (diagnoses, procedures) without extensive
        preprocessing.
    -   **Non-linear Relationships**: Captures complex interactions
        between donor, recipient, and procedural factors.
    -   **Robust Performance**: Advanced regularization techniques
        prevent overfitting in medical datasets.
    -   **Missing Data Tolerance**: Handles incomplete medical records
        gracefully.
    -   **Classification Performance**: Optimized for binary classification
         metrics like AUC, Brier Score, accuracy, precision, and recall.
         **Calibration**: Provides probability estimates that can be assessed
         for calibration quality.
-   **Limitations**:
    -   **Black Box Nature**: Less interpretable than a standard logistic
        model for individual patient risk explanation.
    -   **Computational Complexity**: Requires more computational
        resources and hyperparameter tuning.

**3. CatBoost Random Forest for Classification**

-   **Method**: CatBoost configured with Random Forest-like parameters
    to create an ensemble of independent decision trees for outcome prediction.
-   **Strengths for Our Use Case**:
    -   **Advanced Tree Algorithms**: Leverages CatBoost's sophisticated
        tree construction algorithms while maintaining Random Forest
        ensemble properties.
    -   **Categorical Feature Handling**: Inherits CatBoost's excellent
        categorical variable processing capabilities.
    -   **Ensemble Diversity**: Multiple independent trees provide
        robust predictions across different patient populations.
    -   **Feature Importance**: Provides clear feature importance rankings
        for clinical interpretation.
    -   **Classification Performance**: Optimized for binary classification
         metrics with ensemble stability. **Calibration**: Ensemble predictions
         provide well-calibrated probability estimates.
-   **Limitations**:
    -   **Symmetric Trees**: CatBoost's tree structure may limit
        the diversity of tree shapes compared to traditional Random Forest.
    -   **Parameter Sensitivity**: Requires careful tuning of Random
        Forest-specific parameters within CatBoost framework.

**4. Traditional Random Forest for Classification**

-   **Method**: Classic ensemble classification using independent decision
    trees with majority voting for outcome prediction, implemented via
    the `randomForest` package.
-   **Strengths for Our Use Case**:
    -   **True Ensemble Diversity**: Independent tree construction
        ensures maximum diversity in the ensemble.
    -   **Non-parametric**: Makes no assumptions about underlying
        distributions or relationships.
    -   **Feature Importance**: Provides clear Mean Decrease in Gini
        importance rankings.
    -   **Classification Performance**: Optimized for classification
        metrics, ideal for binary outcome prediction. **Calibration**: True ensemble diversity often leads to
        well-calibrated probability estimates.
    -   **Proven Methodology**: Well-established ensemble method with
        extensive validation in medical literature.
-   **Limitations**:
    -   **Interpretability**: Tree ensemble methods are less
        interpretable than linear models for individual predictions.
    -   **Computational Intensity**: More complex than traditional
        classification methods, requiring careful parameter tuning.

### Cohort-Specific Analysis Rationale

The analysis compares CHD patients (congenital conditions) versus
Myocarditis/Cardiomyopathy patients (acquired conditions) because:

-   **Different Pathophysiology**: Congenital versus acquired heart
    disease may respond differently to transplantation.
-   **Age-Related Factors**: CHD patients are often transplanted at
    younger ages with different growth considerations.
-   **Surgical Complexity**: CHD cases may involve more complex anatomy
    requiring different risk stratification.
-   **Long-term Outcomes**: Different disease etiologies may have
    distinct outcome patterns.
-   **Clinical Decision Making**: Cohort-specific models may provide
    more accurate risk prediction for treatment planning.

This multi-method classification approach allows for comprehensive
model validation, with LASSO providing interpretable linear
relationships, CatBoost capturing complex non-linear patterns,
CatBoost Random Forest leveraging advanced tree algorithms, and
Traditional Random Forest ensuring true ensemble diversity. All four
methods utilize classification metrics for consistent evaluation of
binary outcomes across different modeling frameworks.

### Data Loading

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(readr)
library(rlang)
library(dplyr)
library(glmnet)
library(caret)
library(tidyverse)
library(tibble)
library(DT)
library(randomForest)
library(catboost)
library(pROC)
library(cutpointr)
library(here)

set.seed(1997)

source("scripts/R/classification_helpers.R")

```


```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Load the preprocessed model data
model_data <- read_csv("preprocessed_model_data.csv", show_col_types = FALSE)

# Display basic information about the dataset
cat("=== Dataset Overview ===\n")
cat("Dataset dimensions:", paste(dim(model_data), collapse = " x "), "\n")
cat("Columns:", ncol(model_data), "\n")
cat("Rows:", nrow(model_data), "\n\n")

# Check if ev_type variable exists (1-year survival classification target)
if ("outcome" %in% names(model_data)) {
  cat("outcome variable found ✓\n")
  cat("outcome variable type:", class(model_data$outcome), "\n")
  cat("outcome summary:\n")
  print(table(model_data$outcome))
  cat("Event rate (outcome = 1):", round(mean(model_data$outcome, na.rm = TRUE) * 100, 2), "%\n")
} else {
  stop("outcome variable not found in the dataset!")
}

```

### Model Data

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# For classification models, we use `outcome` as the 1-year survival classification target
# No survival analysis variables (ev_time, days_to_last_followup) are needed

# Verify the `outcome` variable is properly formatted
cat("=== Classification Data Overview ===\n")
cat("Total observations:", nrow(model_data), "\n")
cat("outcome variable summary:\n")
print(table(model_data$outcome))
cat("Event rate (outcome = 1):", round(mean(model_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

# Check for any missing values in outcome
cat("Missing values in outcome:", sum(is.na(model_data$outcome)), "\n")

# Ensure outcome is numeric (0 or 1)
model_data <- model_data %>%
  mutate(outcome = as.numeric(outcome))

# Verify outcome is binary
if (!all(model_data$outcome %in% c(0, 1, NA))) {
  warning("outcome variable contains non-binary values!")
  cat("Unique outcome values:", unique(model_data$outcome), "\n")
}

```

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

model_features <- model_data %>% 
  colnames() %>% 
  as_tibble()

# View 
datatable(
  model_features,
  rownames = FALSE,
  options = list(
    pageLength = 15,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)
```


### Methods: Cohort Pipeline Summary

-   **Data**: Load `preprocessed_model_data.csv` into `model_data`.

-   **Data Integration**: Use the `outcome` variable as the binary
    classification target (1 = death or graft loss occurred, 0 = censored/no event).

-   **Cohorts**: Define `chd_data`
    (`primary_etiology == "Congenital HD"`) and `mc_data`
    (`primary_etiology %in% c("Cardiomyopathy", "Myocarditis")`).

-   **Feature filtering (classification models)**: Create `*_class_model` by
    removing variables matching `classification_lagging_keywords` for all
    models. Exclude identifiers, `outcome`, and `transplant_year` from
    predictors.

-   **Unified train/test split**: Use
    `create_unified_train_test_split()` function with `set.seed(1997)`
    for reproducible 80/20 splits across all models.

-   **Reproducibility**: Set `set.seed(1997)` before each model fit.

-   **LASSO (classification)**: `cv.glmnet(..., family = "binomial", alpha = 1)`
    using logistic regression with L1 penalty. Features exclude
    identifiers, `outcome`, `transplant_year`. Evaluation: AUC, Brier Score, Accuracy,
    Precision, Recall, F1; report feature importance with actual feature names.

-   **CatBoost (classification)**: Gradient boosting with binary cross-entropy loss
    (`loss_function = "Logloss"`). Evaluation: AUC, Brier Score, Accuracy, Precision,
    Recall, F1; report feature importance with normalized values from 0 to 1.

-   **CatBoost Random Forest (classification)**: CatBoost configured with Random
    Forest parameters (`iterations = 500`, `depth = 8`, `learning_rate = 1.0`,
    `bootstrap_type = "Bernoulli"`, `subsample = 0.8`, `rsm = 0.5`) for
    ensemble classification. Evaluation: AUC, Brier Score, Accuracy, Precision, Recall, F1;
    report feature importance with normalized values from 0 to 1.

-   **Traditional Random Forest (classification)**: `randomForest(outcome ~ .)` after
    removing constant columns and excluding identifiers from predictors. Evaluation:
    AUC, Brier Score, Accuracy, Precision, Recall, F1; report feature importance with Mean
    Decrease in Gini values.

-   **Evaluation and reporting**: Predictions and metrics computed on
     the held-out test set. **Calibration analysis** performed to assess
     probability calibration quality using calibration plots, slope, intercept,
     and calibration Brier score. Tabular outputs rendered with
     `DT::datatable`.

-   **Comparison tables**: Aggregate metrics across cohorts and models
    into summary tables for side-by-side comparison.

**Unified Classification Modeling**: All four modeling frameworks (LASSO,
CatBoost, CatBoost Random Forest, and Traditional Random Forest) now use
classification approaches, for direct comparison of classification metrics
and consistent evaluation of binary outcomes across different modeling
frameworks.

```{r classification-lagging-keywords, eval=TRUE}

# Unified classification modeling keywords (variables to exclude from all models)

classification_lagging_keywords <- c(
  # Identifiers and outcomes
  #"ptid_e",
  "transplant_year", "primary_etiology",
  
  # Time variables (not needed for classification)
  "ev_", "days_to_last_followup",
  
  # Donor-specific variables
  "graft_loss", "int_graft_loss", "dtx_", "cc_", "isc_oth",
  "dcardiac", "dcon", "dpri", "dpricaus", "rec_", "papooth",
  "dneuro", "sdprathr", "int_dead", "listing_year", "cpathneg",
  "dcauseod",
  
  # Demographics (if not clinically relevant)
  "race", "sex", "drace_b", "rrace_a", "hisp", "lscntry",
  
  # Transplant-specific variables
  "dreject", "dsecaccsEmpty", "dmajbldEmpty", "pishltgr1R", 
  "drejectEmpty", "drejectHyperacute", "pishltgrEmpty",
  "pishltgr", "dmajbld", "dsecaccs", "dsecaccs_bin", 
  
  # Clinical variables to exclude
  "dx_cardiomyopathy", "deathspc", "dlist", "pmorexam", 
  "patsupp", "concod", "pcadrem", "pcadrec", "pathero", 
  "pdiffib", "dmalcanc", "alt_tx", "age_death", "pacuref",
  "cpbypass",
  
  # Additional variables
  "lsvcma"
)


```


### Cohort-Specific Modeling Pipeline

#### CHD Cohort Analysis

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_data <- model_data %>% filter(primary_etiology == "Congenital HD")

# CHD Cohort summary
cat("CHD Cohort Size:", nrow(chd_data), "patients\n")
cat("outcome Distribution:\n")
print(table(chd_data$outcome))
cat("Event Rate (outcome = 1):", round(mean(chd_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

# CHD Cohort descriptive statistics
chd_summary <- chd_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), list(
    mean = ~ round(mean(.x, na.rm = TRUE), 2),
    median = ~ round(median(.x, na.rm = TRUE), 2),
    sd = ~ round(sd(.x, na.rm = TRUE), 2)
  ))) %>%
  pivot_longer(everything(), names_to = "stat", values_to = "value") %>%
  separate(stat, into = c("variable", "statistic"), sep = "_(?=[^_]*$)") %>%
  pivot_wider(names_from = statistic, values_from = value)

# Display CHD summary table
datatable(
  chd_summary,
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

```{r}

# Create unified train/test split for CHD cohort
chd_split <- create_unified_train_test_split(chd_data, "CHD", seed = 1997)

# Extract split info
chd_train_indices <- chd_split$split_info$train_indices
chd_test_indices <- chd_split$split_info$test_indices
chd_train_data <- chd_split$train_data
chd_test_data <- chd_split$test_data

# Verify the data split
cat("\n=== CHD Cohort Data Split ===\n")
cat("CHD Training set size:", nrow(chd_train_data), "\n")
cat("CHD Test set size:", nrow(chd_test_data), "\n")
cat("CHD Event rate (outcome = 1) in training:", round(mean(chd_train_data$outcome, na.rm = TRUE) * 100, 2), "%\n")
cat("CHD Event rate (outcome = 1) in test:", round(mean(chd_test_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

```

##### LASSO Model

```{r chd-lasso-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Remove lagging keywords and variables starting with "sd"
chd_class_data <- chd_data %>%
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  select(-where(~all(is.na(.)))) %>%
  select(-ptid_e) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Split train/test before feature selection and imputation
chd_train <- chd_class_data[chd_train_indices, ]
chd_test  <- chd_class_data[chd_test_indices, ]

# Remove constant columns based only on training set
constant_cols <- names(chd_train)[sapply(chd_train, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  chd_train <- chd_train %>% select(-all_of(constant_cols))
  chd_test  <- chd_test  %>% select(-all_of(constant_cols))
}

# Impute missing values using training set medians
train_medians <- sapply(chd_train, function(x) if(is.numeric(x)) median(x, na.rm = TRUE) else NA)
chd_train <- chd_train %>% mutate(across(where(is.numeric), ~if_else(is.na(.), train_medians[cur_column()], .)))
chd_test  <- chd_test  %>% mutate(across(where(is.numeric), ~if_else(is.na(.), train_medians[cur_column()], .)))

# Final check for non-finite values
if (any(!is.finite(as.matrix(chd_train))) || any(!is.finite(as.matrix(chd_test)))) {
  stop("Data still contains NA/NaN/Inf values after cleaning. Please check the data source.")
}

cat("CHD Classification LASSO - Final train dimensions:", paste(dim(chd_train), collapse = " x "), "\n")
cat("CHD Classification LASSO - Final test dimensions:", paste(dim(chd_test), collapse = " x "), "\n")

```

```{r chd-lasso-train-test-split}

chd_train <- chd_class_data[chd_train_indices, ]
chd_test  <- chd_class_data[chd_test_indices, ]

# Create matrices for classification
chd_y_train <- chd_train$outcome
chd_x_train <- as.matrix(chd_train %>% select(-outcome))

chd_y_test <- chd_test$outcome
chd_x_test <- as.matrix(chd_test %>% select(-outcome))

# Debug: Check outcome variable
cat("Debug: Outcome variable checks:\n")
cat("  Outcome column exists in train:", "outcome" %in% colnames(chd_train), "\n")
cat("  Outcome column exists in test:", "outcome" %in% colnames(chd_test), "\n")
cat("  chd_y_train class:", class(chd_y_train), "\n")
cat("  chd_y_test class:", class(chd_y_test), "\n")
cat("  chd_y_train unique values:", unique(chd_y_train), "\n")
cat("  chd_y_test unique values:", unique(chd_y_test), "\n")
cat("  chd_y_train has NAs:", any(is.na(chd_y_train)), "\n")
cat("  chd_y_test has NAs:", any(is.na(chd_y_test)), "\n")

# Ensure outcome is numeric
if (!is.numeric(chd_y_train)) {
  warning("chd_y_train is not numeric, converting...")
  chd_y_train <- as.numeric(as.character(chd_y_train))
}
if (!is.numeric(chd_y_test)) {
  warning("chd_y_test is not numeric, converting...")
  chd_y_test <- as.numeric(as.character(chd_y_test))
}

# Verify the size of the new data frames
cat("CHD Training set size:", nrow(chd_train), "\n")
cat("CHD Test set size:", nrow(chd_test), "\n")
cat("CHD X_train dim:", paste(dim(chd_x_train), collapse = " x "), "\n")
cat("CHD X_test dim:", paste(dim(chd_x_test), collapse = " x "), "\n")

# Debug: Check data quality
cat("Debug: Data quality checks:\n")
cat("  Outcome balance - 0s:", sum(chd_y_train == 0), "1s:", sum(chd_y_train == 1), "\n")
cat("  Outcome proportion of 1s:", round(mean(chd_y_train), 4), "\n")
cat("  X_train has any variation:", any(apply(chd_x_train, 2, function(x) length(unique(x)) > 1)), "\n")
cat("  X_train column ranges:", paste(apply(chd_x_train, 2, function(x) round(max(x) - min(x), 2)), collapse = ", "), "\n")

# Check if outcome is too imbalanced
if (mean(chd_y_train) < 0.05 || mean(chd_y_train) > 0.95) {
  warning("Outcome is highly imbalanced - this may cause LASSO issues")
  cat("  Consider using class weights or different sampling strategies\n")
}

table(chd_train$outcome, useNA = "ifany")

chd_train_na <- chd_train %>%
  filter(is.na(outcome))

chd_train_na

```

```{r chd-classification-lasso-model-fit}

# Fit classification LASSO with cross-validation on the TRAINING data ONLY
# Note: Using type.measure = "auc" instead of "class" for imbalanced data
# AUC focuses on ranking ability rather than classification error, which is
# more robust when one class heavily dominates the dataset
set.seed(1997)
chd_class_lasso_cv <- cv.glmnet(
  x = chd_x_train,  # TRAINING matrix
  y = chd_y_train,  # TRAINING outcome vector
  family = "binomial",  # Binary classification
  alpha = 1,  # L1 penalty (LASSO)
  nfolds = 5,
  type.measure = "auc"  # AUC is more robust for imbalanced data
)

# Add error checking for LASSO CV
if (is.null(chd_class_lasso_cv)) {
  stop("Failed to fit CHD LASSO cross-validation model")
}
cat("✓ CHD LASSO cross-validation completed successfully\n")

# Note: Using AUC-based cross-validation instead of classification error
# This should help with imbalanced outcomes and prevent over-regularization

# Get optimal lambda from cross-validation
chd_class_optimal_lambda <- chd_class_lasso_cv$lambda.min

cat("CHD Classification LASSO - Optimal lambda:", round(chd_class_optimal_lambda, 6), "\n")

# Debug: Check lambda values and CV results
cat("Debug: Lambda range from CV:\n")
cat("  Lambda min:", round(chd_class_lasso_cv$lambda.min, 6), "\n")
cat("  Lambda 1SE:", round(chd_class_lasso_cv$lambda.1se, 6), "\n")
cat("  CV AUC at min lambda:", round(max(chd_class_lasso_cv$cvm), 4), "\n")
cat("  Number of non-zero coefficients at min lambda:", sum(coef(chd_class_lasso_cv, s = "lambda.min") != 0), "\n")
cat("  Number of non-zero coefficients at 1SE lambda:", sum(coef(chd_class_lasso_cv, s = "lambda.1se") != 0), "\n")

# Check if lambda is reasonable
if (chd_class_optimal_lambda > 1) {
  warning("Optimal lambda is very high (>1), this may cause all coefficients to be zero")
  cat("  This suggests the model may be over-regularized\n")

  # Try using lambda.1se instead, which is usually more conservative
  cat("  Trying lambda.1se as alternative...\n")
  chd_class_optimal_lambda <- chd_class_lasso_cv$lambda.1se
  cat("  New lambda (1SE):", round(chd_class_optimal_lambda, 6), "\n")

  # If still too high, try a smaller value
  if (chd_class_optimal_lambda > 0.5) {
    cat("  Lambda still too high, trying smaller value...\n")
    # Find lambda that gives reasonable number of non-zero coefficients
    lambda_seq <- chd_class_lasso_cv$lambda[chd_class_lasso_cv$lambda < 0.5]
    if (length(lambda_seq) > 0) {
      # Use the largest lambda that gives at least 5 non-zero coefficients
      for (lam in rev(lambda_seq)) {
        n_coefs <- sum(coef(chd_class_lasso_cv, s = lam) != 0)
        if (n_coefs >= 5) {
          chd_class_optimal_lambda <- lam
          cat("  Selected lambda:", round(lam, 6), "with", n_coefs, "non-zero coefficients\n")
          break
        }
      }
    }
  }
}

# Use the cross-validated glmnet fit directly (avoids mismatch when refitting)
cat("Using cv.glmnet fitted model at lambda.min...\n")
chd_class_lasso_model <- chd_class_lasso_cv$glmnet.fit

# Add error checking for fitted model
if (is.null(chd_class_lasso_model)) {
  stop("cv.glmnet did not return a fitted glmnet model")
}
cat("✓ Using cross-validated glmnet fit\n")


# Extract nonzero coefficients for CHD LASSO (handle S4 sparse matrix)
chd_class_coefs <- coef(chd_class_lasso_model, s = chd_class_optimal_lambda)
coefs_vec <- as.vector(chd_class_coefs)
features_vec <- rownames(chd_class_coefs)
chd_class_nonzero_coefs <- data.frame(
  feature = features_vec[coefs_vec != 0],
  coefficient = coefs_vec[coefs_vec != 0]
)
chd_class_nonzero_coefs <- chd_class_nonzero_coefs[chd_class_nonzero_coefs$feature != "(Intercept)", ]

# Debug: Check model coefficients
cat("Debug: Model coefficient analysis:\n")
cat("  Total coefficients:", length(chd_class_coefs), "\n")
cat("  Non-zero coefficients:", sum(chd_class_coefs != 0), "\n")
cat("  Intercept value:", round(chd_class_coefs[1], 6), "\n")
cat("  Non-zero coefficient values:", round(chd_class_coefs[chd_class_coefs != 0], 6), "\n")
cat("  Coefficient range:", round(range(chd_class_coefs), 6), "\n")

# Check if model is meaningful
if (sum(chd_class_coefs != 0) <= 1) {
  warning("Model has only intercept or very few non-zero coefficients")
  cat("  This suggests the model may be over-regularized or data issues exist\n")
}


```

###### Accuracy

```{r chd-lasso-model-prediction}

# Probability predictions on TEST data
chd_lasso_probabilities <- predict(
  chd_class_lasso_model,
  newx = chd_x_test,
  s = chd_class_optimal_lambda,   # or "lambda.min", pick one and keep consistently
  type = "response"
)

# Debug: Check prediction values
cat("Debug: Prediction analysis:\n")
cat("  Prediction length:", length(chd_lasso_probabilities), "\n")
cat("  Prediction range:", round(range(chd_lasso_probabilities), 6), "\n")
cat("  Prediction mean:", round(mean(chd_lasso_probabilities), 6), "\n")
cat("  Prediction unique values:", round(unique(chd_lasso_probabilities), 6), "\n")
cat("  Number of unique predictions:", length(unique(chd_lasso_probabilities)), "\n")

# Check if predictions are constant
if (length(unique(chd_lasso_probabilities)) == 1) {
  warning("All predictions are identical - this indicates a model problem")
  cat("  Constant prediction value:", unique(chd_lasso_probabilities), "\n")
  cat("  This usually means all coefficients are zero or model didn't fit properly\n")
}

# Clean and align predictions + actuals
valid_idx <- !is.na(chd_y_test)
chd_lasso_actuals <- chd_y_test[valid_idx]
chd_lasso_predictions <- as.numeric(chd_lasso_probabilities[valid_idx])

# Turn actual into a 0/1 factor for ROC
actual_factor <- factor(chd_lasso_actuals, levels = c(0, 1))


# ROC object for threshold determination
cat("Creating ROC object...\n")
roc_obj <- pROC::roc(response = actual_factor, predictor = chd_lasso_predictions)

# cutpointr expects a data.frame
cp_data <- data.frame(pred = chd_lasso_predictions, actual = chd_lasso_actuals)
# Optimize for sum of sensitivity and specificity (Youden), but you can change to F1, recall, etc.
cp <- cutpointr(cp_data, x = pred, class = actual, method = maximize_metric, metric = sum_sens_spec)
cutpointr_thresh <- cp$optimal_cutpoint
cat("cutpointr optimal threshold:", round(cutpointr_thresh, 3), "\n")

# Also keep recall-friendly threshold for comparison
event_rate <- mean(chd_lasso_actuals)
recall_thresh <- max(0.15, event_rate)
cat("Recall-friendly threshold:", round(recall_thresh, 3), "\n")

# Evaluate at both thresholds using the new function
metrics_cutpointr <- create_classification_metrics(
  probs = chd_lasso_predictions,
  actual = chd_lasso_actuals,
  cohort_name = "CHD",
  model_name = paste0("LASSO (cutpointr-optimal=", round(cutpointr_thresh, 2), ")"),
  threshold = cutpointr_thresh
)

metrics_recall <- create_classification_metrics(
  probs = chd_lasso_predictions,
  actual = chd_lasso_actuals,
  cohort_name = "CHD",
  model_name = paste0("LASSO (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
chd_lasso_cal_metrics <- calculate_calibration_metrics(
  predictions = chd_lasso_predictions,
  actual = chd_lasso_actuals
)

# Combine results and sort by Recall
all_chd_lasso_metrics <- rbind(metrics_cutpointr, metrics_recall) %>%
  mutate(
    Calibration_Slope = chd_lasso_cal_metrics$Calibration_Slope,
    Calibration_Intercept = chd_lasso_cal_metrics$Calibration_Intercept,
    Calibration_Brier = chd_lasso_cal_metrics$Calibration_Brier
  )
all_chd_lasso_metrics <- all_chd_lasso_metrics[order(-all_chd_lasso_metrics$Recall), ]
print(all_chd_lasso_metrics)
all_chd_lasso_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = chd_lasso_cal_metrics$Calibration_Slope,
    Calibration_Intercept = chd_lasso_cal_metrics$Calibration_Intercept,
    Calibration_Brier = chd_lasso_cal_metrics$Calibration_Brier
  )
all_chd_lasso_metrics <- all_chd_lasso_metrics[order(-all_chd_lasso_metrics$Recall), ]
print(all_chd_lasso_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```


###### Feature Importance

```{r chd-lasso-features, warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE}

# Display CHD LASSO coefficients
cat("CHD Classification LASSO Model - Number of selected features:", nrow(chd_class_nonzero_coefs), "\n")
  
  datatable(
    chd_class_nonzero_coefs,
    caption = "CHD Cohort - LASSO Classification Feature Coefficients",
    rownames = FALSE,
    options = list(
      pageLength = 15,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )

```

##### CatBoost Classification Model

```{r chd-catboost-model-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Prepare data for CatBoost classification
chd_catboost_df <- chd_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all predictors are numeric for CatBoost
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Remove constant columns
constant_cols <- names(chd_catboost_df)[sapply(chd_catboost_df, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  chd_catboost_df <- chd_catboost_df %>% select(-all_of(constant_cols))
}

# Impute any remaining missing values
chd_catboost_df <- chd_catboost_df %>%
  mutate(across(everything(), ~if_else(is.na(.), median(., na.rm = TRUE), .)))

cat("CHD Classification CatBoost - Final data dimensions:", paste(dim(chd_catboost_df), collapse = " x "), "\n")

```

```{r chd-catboost-data-prep}

# Split the data using the existing train/test indices
chd_catboost_train <- chd_catboost_df[chd_train_indices, ]
chd_catboost_test  <- chd_catboost_df[chd_test_indices, ]

# Prepare training features and labels
chd_catboost_y_train <- chd_catboost_train$outcome
chd_catboost_x_train <- chd_catboost_train %>% select(-outcome)

# Prepare test features and labels
chd_catboost_y_test <- chd_catboost_test$outcome
chd_catboost_x_test <- chd_catboost_test %>% select(-outcome)

# Verify the data preparation
cat("CHD CatBoost Classification - Training set size:", nrow(chd_catboost_x_train), "\n")
cat("CHD CatBoost Classification - Test set size:", nrow(chd_catboost_x_test), "\n")
cat("CHD CatBoost Classification - Training features:", ncol(chd_catboost_x_train), "\n")

```

```{r chd-catboost-model}

# Data Pools for classification
chd_catboost_train_pool <- catboost.load_pool(
  data = chd_catboost_x_train, 
  label = chd_catboost_y_train
)

chd_catboost_test_pool <- catboost.load_pool(
  data = chd_catboost_x_test, 
  label = chd_catboost_y_test
)

# Model parameters for classification
chd_catboost_params <- list(
  loss_function = 'Logloss',  # Binary classification loss
  eval_metric = 'Logloss',    # Classification metric
  iterations = 2000,
  depth = 4,
  verbose = 500,              # Print train and test metrics every 500 iterations
  random_seed = 1997
)

# Train the CatBoost classification model
set.seed(1997)
chd_catboost_model <- catboost.train(
  learn_pool = chd_catboost_train_pool,
  test_pool = chd_catboost_test_pool,
  params = chd_catboost_params
)

cat("CHD CatBoost Classification Model trained successfully\n")

```

###### Accuracy

```{r chd-catboost-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}


# Probability predictions on TEST data
chd_catboost_probabilities_raw <- catboost.predict(
  chd_catboost_model,
  chd_catboost_test_pool,
  prediction_type = "Probability"
)

# Clean and align actuals and predictions (remove NAs)
valid_idx <- !is.na(chd_catboost_y_test)
chd_catboost_actuals <- chd_catboost_y_test[valid_idx]
chd_catboost_predictions <- as.numeric(chd_catboost_probabilities_raw[valid_idx])
actual_factor <- factor(chd_catboost_actuals, levels = c(0, 1))

# ROC object for threshold determination
roc_obj <- pROC::roc(actual_factor, chd_catboost_predictions, quiet = TRUE)

# Determine thresholds (numeric scalars)
roc_best_thresh <- as.numeric(coords(roc_obj, "best", ret = "threshold", transpose = FALSE))
event_rate <- mean(chd_catboost_actuals)
recall_thresh <- max(0.15, event_rate)

cat("ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("Recall-friendly threshold:", round(recall_thresh, 3), "\n")

# Evaluate at both thresholds
metrics_roc <- create_classification_metrics(
  probs = chd_catboost_predictions,
  actual = chd_catboost_actuals,
  cohort_name = "CHD",
  model_name = paste0("CatBoost (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = chd_catboost_predictions,
  actual = chd_catboost_actuals,
  cohort_name = "CHD",
  model_name = paste0("CatBoost (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
chd_catboost_cal_metrics <- calculate_calibration_metrics(
  predictions = chd_catboost_predictions,
  actual = chd_catboost_actuals
)

# Combine & sort by Recall
all_chd_catboost_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = chd_catboost_cal_metrics$Calibration_Slope,
    Calibration_Intercept = chd_catboost_cal_metrics$Calibration_Intercept,
    Calibration_Brier = chd_catboost_cal_metrics$Calibration_Brier
  )
all_chd_catboost_metrics <- all_chd_catboost_metrics[order(-all_chd_catboost_metrics$Recall), ]
print(all_chd_catboost_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

##### CatBoost Random Forest Classification Model

```{r chd-randomforest-fit-model, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Prepare data for CatBoost Random Forest classification
chd_rf_data <- chd_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all categorical predictors are factors for CatBoost
  mutate(across(where(is.character), as.factor)) 

# Remove constant columns
constant_cols <- names(chd_rf_data)[sapply(chd_rf_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  chd_rf_data <- chd_rf_data %>% select(-all_of(constant_cols))
}


# Split data using existing train/test indices
chd_rf_train <- chd_rf_data[chd_train_indices, ]
chd_rf_test  <- chd_rf_data[chd_test_indices, ]

# Identify categorical features (columns with limited unique values)
categorical_features <- which(sapply(chd_rf_train, function(x) {
  length(unique(x)) <= 10 && length(unique(x)) > 1
}))

cat("CHD CatBoost Random Forest - Training set size:", nrow(chd_rf_train), "\n")
cat("CHD CatBoost Random Forest - Test set size:", nrow(chd_rf_test), "\n")
cat("CHD CatBoost Random Forest - Features:", ncol(chd_rf_train) - 1, "\n")
cat("CHD CatBoost Random Forest - Categorical features:", length(categorical_features), "\n")

# Prepare training and testing pools
chd_rf_train_pool <- catboost.load_pool(
  data = chd_rf_train[, -which(names(chd_rf_train) == "outcome")],
  label = chd_rf_train$outcome
)

chd_rf_test_pool <- catboost.load_pool(
  data = chd_rf_test[, -which(names(chd_rf_test) == "outcome")],
  label = chd_rf_test$outcome
)

# Random Forest-like parameters for CatBoost
chd_rf_params <- list(
  iterations = 500,               # number of trees
  depth = 8,                      # tree depth
  learning_rate = 1.0,            # no shrinkage, independent trees
  bootstrap_type = "Bernoulli",   # random row sampling
  subsample = 0.8,                # 80% rows per tree
  sampling_frequency = "PerTree", # resample once per tree
  rsm = 0.5,                      # use 50% of features at each split
  loss_function = "Logloss",      # classification
  eval_metric = "AUC",
  random_strength = 1.0,          # randomness in split score
  l2_leaf_reg = 3.0,             # mild regularization
  random_seed = 1997,
  verbose = 100,
  use_best_model = FALSE
)

# Train the CatBoost Random Forest model
set.seed(1997)
chd_rf_model <- catboost.train(chd_rf_train_pool, chd_rf_test_pool, chd_rf_params)

cat("CHD CatBoost Random Forest Classification Model trained successfully\n")

```

###### Accuracy Metrics

```{r chd-randomforest-accuracy, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}


# Predict probabilities on TEST data (CatBoost RF-mode)
chd_rf_probabilities_raw <- as.numeric(
  catboost.predict(
    chd_rf_model,
    chd_rf_test_pool,
    prediction_type = "Probability"
  )
)

# Clean & align actuals/predictions
actual_all <- chd_rf_test$outcome
valid_idx <- !is.na(actual_all)
chd_rf_actuals <- as.numeric(as.character(actual_all[valid_idx]))
chd_rf_predictions <- as.numeric(chd_rf_probabilities_raw[valid_idx])

# Thresholds via ROC (tie-safe)

roc_obj <- pROC::roc(factor(chd_rf_actuals, levels = c(0, 1)), chd_rf_predictions, quiet = TRUE)

best_vals <- pROC::coords(roc_obj, "best", ret = "threshold", transpose = FALSE)
roc_best_thresh <- median(as.numeric(unlist(best_vals)), na.rm = TRUE)

event_rate <- mean(chd_rf_actuals, na.rm = TRUE)
recall_thresh <- max(0.15, event_rate)

cat("CHD RF ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("CHD RF Recall-friendly threshold:", round(recall_thresh, 3), "\n")

# Evaluate at both thresholds using your metrics helper
metrics_roc <- create_classification_metrics(
  probs = chd_rf_predictions,
  actual = chd_rf_actuals,
  cohort_name = "CHD",
  model_name = paste0("CatBoost Random Forest (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = chd_rf_predictions,
  actual = chd_rf_actuals,
  cohort_name = "CHD",
  model_name = paste0("CatBoost Random Forest (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
chd_rf_cal_metrics <- calculate_calibration_metrics(
  predictions = chd_rf_predictions,
  actual = chd_rf_actuals
)

# Combine & sort by Recall
all_chd_rf_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = chd_rf_cal_metrics$Calibration_Slope,
    Calibration_Intercept = chd_rf_cal_metrics$Calibration_Intercept,
    Calibration_Brier = chd_rf_cal_metrics$Calibration_Brier
  )
all_chd_rf_metrics <- all_chd_rf_metrics[order(-all_chd_rf_metrics$Recall), ]
print(all_chd_rf_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

##### Traditional Random Forest Classification Model

```{r chd-traditional-rf-fit-model-data, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Prepare data for traditional Random Forest classification
chd_trad_rf_data <- chd_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all categorical predictors are factors for CatBoost
  mutate(across(where(is.character), as.factor)) %>% 
  # Ensure outcome is a factor for Random Forest
  mutate(outcome = as.factor(outcome))


# Remove constant columns
constant_cols <- names(chd_trad_rf_data)[sapply(chd_trad_rf_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  chd_trad_rf_data <- chd_trad_rf_data %>% select(-all_of(constant_cols))
}

# Split data using existing train/test indices
chd_trad_rf_train <- chd_trad_rf_data[chd_train_indices, ]
chd_trad_rf_test  <- chd_trad_rf_data[chd_test_indices, ]

cat("CHD Traditional Random Forest - Training set size:", nrow(chd_trad_rf_train), "\n")
cat("CHD Traditional Random Forest - Test set size:", nrow(chd_trad_rf_test), "\n")
cat("CHD Traditional Random Forest - Features:", ncol(chd_trad_rf_train) - 1, "\n")

# Train the traditional Random Forest model
set.seed(1997)
chd_trad_rf_model <- randomForest(
  outcome ~ .,
  data = chd_trad_rf_train,
  ntree = 1000,           # Number of trees
  mtry = sqrt(ncol(chd_trad_rf_train) - 1),  # Number of features to consider at each split
  importance = TRUE,      # Calculate feature importance
  proximity = FALSE,      # Don't calculate proximity matrix to save memory
  do.trace = 50,          # Print progress every 50 trees
  na.action=na.roughfix
)

cat("CHD Traditional Random Forest Classification Model trained successfully\n")

```

###### Accuracy Metrics

```{r chd-traditional-rf-accuracy, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}


#  Predict probabilities on TEST data
# For traditional randomForest models, type = "prob" gives class probabilities
chd_trad_rf_probabilities_raw <- predict(
  chd_trad_rf_model,
  chd_trad_rf_test,
  type = "prob"
)[, 2]  # probability for class "1"


# Clean and align actuals and predictions
valid_idx <- !is.na(chd_trad_rf_test$outcome)
chd_trad_rf_actuals <- as.numeric(as.character(chd_trad_rf_test$outcome[valid_idx]))
chd_trad_rf_predictions <- as.numeric(chd_trad_rf_probabilities_raw[valid_idx])
actual_factor <- factor(chd_trad_rf_actuals, levels = c(0, 1))


# ROC object for threshold determination
roc_obj <- pROC::roc(actual_factor, chd_trad_rf_predictions, quiet = TRUE)


# Determine thresholds (numeric scalars)
roc_best_thresh <- as.numeric(coords(roc_obj, "best", ret = "threshold", transpose = FALSE))


event_rate <- mean(chd_trad_rf_actuals, na.rm = TRUE)
recall_thresh <- max(0.15, event_rate, na.rm = TRUE)


cat("ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("Recall-friendly threshold:", round(recall_thresh, 3), "\n")


# Evaluate at both thresholds
metrics_roc <- create_classification_metrics(
  probs = chd_trad_rf_predictions,
  actual = chd_trad_rf_actuals,
  cohort_name = "CHD",
  model_name = paste0("Traditional RF (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = chd_trad_rf_predictions,
  actual = chd_trad_rf_actuals,
  cohort_name = "CHD",
  model_name = paste0("Traditional RF (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
chd_trad_rf_cal_metrics <- calculate_calibration_metrics(
  predictions = chd_trad_rf_predictions,
  actual = chd_trad_rf_actuals
)

# Combine & sort by Recall
all_chd_trad_rf_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = chd_trad_rf_cal_metrics$Calibration_Slope,
    Calibration_Intercept = chd_trad_rf_cal_metrics$Calibration_Intercept,
    Calibration_Brier = chd_trad_rf_cal_metrics$Calibration_Brier
  )
all_chd_trad_rf_metrics <- all_chd_trad_rf_metrics[order(-all_chd_trad_rf_metrics$Recall), ]
print(all_chd_trad_rf_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

#### Myocarditis/Cardiomyopathy Cohort Analysis

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

mc_data <- model_data %>% filter(primary_etiology %in% c("Cardiomyopathy", "Myocarditis"))


# Myo/Cardio Cohort summary
cat("Myocarditis/Cardiomyopathy Cohort Size:", nrow(mc_data), "patients\n")
cat("outcome Distribution:\n")
print(table(mc_data$outcome))
cat("Event Rate (outcome = 1):", round(mean(mc_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

# Myo/Cardio Cohort descriptive statistics
mc_summary <- mc_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), list(
    mean = ~ round(mean(.x, na.rm = TRUE), 2),
    median = ~ round(median(.x, na.rm = TRUE), 2),
    sd = ~ round(sd(.x, na.rm = TRUE), 2)
  ))) %>%
  pivot_longer(everything(), names_to = "stat", values_to = "value") %>%
  separate(stat, into = c("variable", "statistic"), sep = "_(?=[^_]*$)") %>%
  pivot_wider(names_from = statistic, values_from = value)

# Display Myo/Cardio summary table
datatable(
  mc_summary,
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

```{r}

# Create unified train/test split for Myo/Cardio cohort
mc_split <- create_unified_train_test_split(mc_data, "Myo/Cardio", seed = 1997)

# Extract split info
mc_train_indices <- mc_split$split_info$train_indices
mc_test_indices  <- mc_split$split_info$test_indices
mc_train_data    <- mc_split$train_data
mc_test_data     <- mc_split$test_data

```

##### LASSO Model

```{r myo-lasso-df, echo=FALSE, warning=FALSE, message=FALSE}

mc_class_data <- mc_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all predictors are numeric for glmnet
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Remove constant columns
constant_cols <- names(mc_class_data)[sapply(mc_class_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  mc_class_data <- mc_class_data %>% select(-all_of(constant_cols))
}

# Impute any remaining missing values
mc_class_data <- mc_class_data %>%
  mutate(across(everything(), ~if_else(is.na(.), median(., na.rm = TRUE), .)))

# Final check for non-finite values
if (any(!is.finite(as.matrix(mc_class_data)))) {
  stop("Data still contains NA/NaN/Inf values after cleaning. Please check the data source.")
}

cat("Myo/Cardio Classification LASSO - Final data dimensions:", paste(dim(mc_class_data), collapse = " x "), "\n")

```

```{r myo-lasso-train-test-split}

mc_train <- mc_class_data[mc_train_indices, ]
mc_test  <- mc_class_data[mc_test_indices, ]

# Make factor NAs explicit; do NOT coerce to integers
library(forcats); library(tidyr)
mc_train <- mc_train %>%
  mutate(
    across(where(is.character), ~ replace_na(., "Missing")),
    across(where(is.factor), ~ fct_na_value_to_level(., level = "Missing"))
  )
mc_test <- mc_test %>%
  mutate(
    across(where(is.character), ~ replace_na(., "Missing")),
    across(where(is.factor), ~ fct_na_value_to_level(., level = "Missing"))
  )

# Leak-free numeric imputation: compute TRAIN medians, apply to both
num_cols <- names(Filter(is.numeric, mc_train %>% dplyr::select(-outcome)))
if (length(num_cols)) {
  train_meds <- sapply(mc_train[num_cols], function(x) median(x, na.rm = TRUE))
  for (nm in names(train_meds)) {
    mc_train[[nm]][is.na(mc_train[[nm]])] <- train_meds[[nm]]
    mc_test[[nm]][is.na(mc_test[[nm]])]  <- train_meds[[nm]]
  }
}

# One-hot encode via model.matrix; align TEST to TRAIN columns
mm_train <- model.matrix(outcome ~ . - 1, data = mc_train)
mm_test_raw <- model.matrix(outcome ~ . - 1, data = mc_test)

mm_test <- matrix(0, nrow = nrow(mm_test_raw), ncol = ncol(mm_train),
                  dimnames = list(NULL, colnames(mm_train)))
common <- intersect(colnames(mm_test_raw), colnames(mm_train))
mm_test[, common] <- mm_test_raw[, common, drop = FALSE]

mc_y_train <- as.integer(mc_train$outcome)  # ensure 0/1
mc_y_test  <- as.integer(mc_test$outcome)

# Debug: Check outcome variable
cat("Debug: Outcome variable checks:\n")
cat("  Outcome column exists in train:", "outcome" %in% colnames(mc_train), "\n")
cat("  Outcome column exists in test:", "outcome" %in% colnames(mc_test), "\n")
cat("  mc_y_train class:", class(mc_y_train), "\n")
cat("  mc_y_test class:", class(mc_y_test), "\n")
cat("  mc_y_train unique values:", unique(mc_y_train), "\n")
cat("  mc_y_test unique values:", unique(mc_y_test), "\n")
cat("  mc_y_train has NAs:", any(is.na(mc_y_train)), "\n")
cat("  mc_y_test has NAs:", any(is.na(mc_y_test)), "\n")

cat("Myo/Cardio X_train dim:", paste(dim(mm_train), collapse = " x "), "\n")
cat("Myo/Cardio X_test dim:", paste(dim(mm_test), collapse = " x "), "\n")

# Debug: Check data quality
cat("Debug: Data quality checks:\n")
cat("  Outcome balance - 0s:", sum(mc_y_train == 0), "1s:", sum(mc_y_train == 1), "\n")
cat("  Outcome proportion of 1s:", round(mean(mc_y_train), 4), "\n")
cat("  X_train has any variation:", any(apply(mm_train, 2, function(x) length(unique(x)) > 1)), "\n")
cat("  X_train column ranges:", paste(apply(mm_train, 2, function(x) round(max(x) - min(x), 2)), collapse = ", "), "\n")

# Check if outcome is too imbalanced
if (mean(mc_y_train) < 0.05 || mean(mc_y_train) > 0.95) {
  warning("Outcome is highly imbalanced - this may cause LASSO issues")
  cat("  Consider using class weights or different sampling strategies\n")
}


```


```{r myo-lasso-model-fit}

# Positive class weight to counter severe imbalance
w_pos <- sum(mc_y_train == 0) / max(1, sum(mc_y_train == 1))
w <- ifelse(mc_y_train == 1, w_pos, 1)

set.seed(1997)
mc_class_lasso_cv <- cv.glmnet(
  x = mm_train,
  y = mc_y_train,
  family = "binomial",
  alpha = 0.5,            # elastic-net often better than pure L1 in rare events
  nfolds = 5,
  type.measure = "auc",   # choose lambda by AUC (robust to imbalance)
  weights = w,
  standardize = TRUE
)

# Add error checking for LASSO CV
if (is.null(mc_class_lasso_cv)) {
  stop("Failed to fit MC LASSO cross-validation model")
}
cat("✓ MC LASSO cross-validation completed successfully\n")

mc_class_optimal_lambda <- mc_class_lasso_cv$lambda.min
cat("Myo/Cardio Classification LASSO - Optimal lambda:", round(mc_class_optimal_lambda, 6), "\n")

# Debug: Check lambda values and CV results
cat("Debug: Lambda range from CV:\n")
cat("  Lambda min:", round(mc_class_lasso_cv$lambda.min, 6), "\n")
cat("  Lambda 1SE:", round(mc_class_lasso_cv$lambda.1se, 6), "\n")
cat("  CV AUC at min lambda:", round(max(mc_class_lasso_cv$cvm), 4), "\n")
cat("  Number of non-zero coefficients at min lambda:", sum(coef(mc_class_lasso_cv, s = "lambda.min") != 0), "\n")
cat("  Number of non-zero coefficients at 1SE lambda:", sum(coef(mc_class_lasso_cv, s = "lambda.1se") != 0), "\n")

# Check if lambda is reasonable
if (mc_class_optimal_lambda > 1) {
  warning("Optimal lambda is very high (>1), this may cause all coefficients to be zero")
  cat("  This suggests the model may be over-regularized\n")

  # Try using lambda.1se instead, which is usually more conservative
  cat("  Trying lambda.1se as alternative...\n")
  mc_class_optimal_lambda <- mc_class_lasso_cv$lambda.1se
  cat("  New lambda (1SE):", round(mc_class_optimal_lambda, 6), "\n")

  # If still too high, try a smaller value
  if (mc_class_optimal_lambda > 0.5) {
    cat("  Lambda still too high, trying smaller value...\n")
    # Find lambda that gives reasonable number of non-zero coefficients
    lambda_seq <- mc_class_lasso_cv$lambda[mc_class_lasso_cv$lambda < 0.5]
    if (length(lambda_seq) > 0) {
      # Use the largest lambda that gives at least 5 non-zero coefficients
      for (lam in rev(lambda_seq)) {
        n_coefs <- sum(coef(mc_class_lasso_cv, s = lam) != 0)
        if (n_coefs >= 5) {
          mc_class_optimal_lambda <- lam
          cat("  Selected lambda:", round(lam, 6), "with", n_coefs, "non-zero coefficients\n")
          break
        }
      }
    }
  }
}


# Use the cross-validated glmnet fit directly (avoids mismatch when refitting)
cat("Using cv.glmnet fitted model at lambda.min...\n")
mc_class_lasso_model <- mc_class_lasso_cv$glmnet.fit

# Add error checking for fitted model
if (is.null(mc_class_lasso_model)) {
  stop("cv.glmnet did not return a fitted glmnet model")
}
cat("✓ Using cross-validated glmnet fit\n")


```



```{r myo-lasso-prediction-probabilities}

# Probabilities on TEST
mc_lasso_probabilities_raw <- as.numeric(
  predict(mc_class_lasso_cv, newx = mm_test, s = "lambda.min", type = "response")
)

# Debug: Check prediction values
cat("Debug: Prediction analysis:\n")
cat("  Prediction length:", length(mc_lasso_probabilities_raw), "\n")
cat("  Prediction range:", round(range(mc_lasso_probabilities_raw), 6), "\n")
cat("  Prediction mean:", round(mean(mc_lasso_probabilities_raw), 6), "\n")
cat("  Prediction unique values:", round(unique(mc_lasso_probabilities_raw), 6), "\n")
cat("  Number of unique predictions:", length(unique(mc_lasso_probabilities_raw)), "\n")

# Check if predictions are constant
if (length(unique(mc_lasso_probabilities_raw)) == 1) {
  warning("All predictions are identical - this indicates a model problem")
  cat("  Constant prediction value:", unique(mc_lasso_probabilities_raw), "\n")
  cat("  This usually means all coefficients are zero or model didn't fit properly\n")
}

valid_idx <- !is.na(mc_y_test)
mc_lasso_actuals <- mc_y_test[valid_idx]
mc_lasso_predictions <- mc_lasso_probabilities_raw[valid_idx]
actual_factor <- factor(mc_lasso_actuals, levels = c(0, 1))

roc_obj <- pROC::roc(actual_factor, mc_lasso_predictions, quiet = TRUE)

# Safe threshold extraction (handles ties)
best_vals <- pROC::coords(roc_obj, "best", ret = "threshold", transpose = FALSE)
roc_best_thresh <- median(as.numeric(unlist(best_vals)), na.rm = TRUE)

event_rate <- mean(mc_lasso_actuals)
recall_thresh <- max(0.15, event_rate)

cat("MYO ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("MYO Recall-friendly threshold:", round(recall_thresh, 3), "\n")

```

###### Accuracy

```{r myo-lasso-accuracy-metrics}

metrics_roc <- create_classification_metrics(
  probs = mc_lasso_predictions,
  actual = mc_lasso_actuals,
  cohort_name = "MYO_CARDIO",
  model_name = paste0("LASSO (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = mc_lasso_predictions,
  actual = mc_lasso_actuals,
  cohort_name = "MYO_CARDIO",
  model_name = paste0("LASSO (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
mc_lasso_cal_metrics <- calculate_calibration_metrics(
  predictions = mc_lasso_predictions,
  actual = mc_lasso_actuals
)

# Combine results and sort by Recall
all_mc_lasso_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = mc_lasso_cal_metrics$Calibration_Slope,
    Calibration_Intercept = mc_lasso_cal_metrics$Calibration_Intercept,
    Calibration_Brier = mc_lasso_cal_metrics$Calibration_Brier
  )
all_mc_lasso_metrics <- all_mc_lasso_metrics[order(-all_mc_lasso_metrics$Recall), ]
print(all_mc_lasso_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```


###### Feature Importance

```{r}

# Non-zero coefficients at lambda.min (feature importances)
mc_class_coefs <- coef(mc_class_lasso_cv, s = "lambda.min")  # sparse matrix
mc_class_nonzero_coefs <- data.frame(
  feature = rownames(mc_class_coefs)[mc_class_coefs@i + 1],
  coefficient = mc_class_coefs@x
) |>
  dplyr::filter(feature != "(Intercept)") |>
  dplyr::arrange(dplyr::desc(abs(coefficient)))

# Debug: Check model coefficients
cat("Debug: Model coefficient analysis:\n")
cat("  Total coefficients:", length(mc_class_coefs), "\n")
cat("  Non-zero coefficients:", sum(mc_class_coefs != 0), "\n")
cat("  Intercept value:", round(mc_class_coefs[1], 6), "\n")
cat("  Non-zero coefficient values:", round(mc_class_coefs[mc_class_coefs != 0], 6), "\n")
cat("  Coefficient range:", round(range(mc_class_coefs), 6), "\n")

# Check if model is meaningful
if (sum(mc_class_coefs != 0) <= 1) {
  warning("Model has only intercept or very few non-zero coefficients")
  cat("  This suggests the model may be over-regularized or data issues exist\n")
}

cat("Myo/Cardio Classification LASSO - Number of selected features:", nrow(mc_class_nonzero_coefs), "\n")


```


```{r myo-lasso-features, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

cat("Myo/Cardio Classification LASSO Model - Number of selected features:", nrow(mc_class_nonzero_coefs), "\n")
  
datatable(
  mc_class_nonzero_coefs,
  caption = "Myo/Cardio Cohort - LASSO Classification Feature Coefficients",
  rownames = FALSE,
  options = list(
    pageLength = 15,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

##### CatBoost Classification Model

```{r myo-catboost-model-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Prepare data for CatBoost classification
mc_catboost_df <- mc_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all predictors are numeric for CatBoost
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Remove constant columns
constant_cols <- names(mc_catboost_df)[sapply(mc_catboost_df, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  mc_catboost_df <- mc_catboost_df %>% select(-all_of(constant_cols))
}

# Impute any remaining missing values
mc_catboost_df <- mc_catboost_df %>%
  mutate(across(everything(), ~if_else(is.na(.), median(., na.rm = TRUE), .)))

cat("Myo/Cardio Classification CatBoost - Final data dimensions:", paste(dim(mc_catboost_df), collapse = " x "), "\n")

```

```{r myo-catboost-data-prep}

# Split the data using the existing train/test indices
mc_catboost_train <- mc_catboost_df[mc_train_indices, ]
mc_catboost_test  <- mc_catboost_df[mc_test_indices, ]

# Prepare training features and labels
mc_catboost_y_train <- mc_catboost_train$outcome
mc_catboost_x_train <- mc_catboost_train %>% select(-outcome)

# Prepare test features and labels
mc_catboost_y_test <- mc_catboost_test$outcome
mc_catboost_x_test <- mc_catboost_test %>% select(-outcome)

# Verify the data preparation
cat("Myo/Cardio CatBoost Classification - Training set size:", nrow(mc_catboost_x_train), "\n")
cat("Myo/Cardio CatBoost Classification - Test set size:", nrow(mc_catboost_x_test), "\n")
cat("Myo/Cardio CatBoost Classification - Training features:", ncol(mc_catboost_x_train), "\n")

```

```{r myo-catboost-model-fit}

# Data Pools for classification
mc_catboost_train_pool <- catboost.load_pool(
  data = mc_catboost_x_train, 
  label = mc_catboost_y_train
)

mc_catboost_test_pool <- catboost.load_pool(
  data = mc_catboost_x_test, 
  label = mc_catboost_y_test
)

# Model parameters for classification
mc_catboost_params <- list(
  loss_function = 'Logloss',  # Binary classification loss
  eval_metric = 'Logloss',    # Classification metric
  iterations = 2000,
  depth = 4,
  verbose = 500,              # Print train and test metrics every 500 iterations
  random_seed = 1997
)

# Train the CatBoost classification model
set.seed(1997)
mc_catboost_model <- catboost.train(
  learn_pool = mc_catboost_train_pool,
  test_pool = mc_catboost_test_pool,
  params = mc_catboost_params
)

cat("Myo/Cardio CatBoost Classification Model trained successfully\n")

```

###### Accuracy

```{r myo-catboost-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Predict probabilities on TEST data (Myo/Cardio)
mc_catboost_probabilities_raw <- as.numeric(
  catboost.predict(
    mc_catboost_model,
    mc_catboost_test_pool,
    prediction_type = "Probability"
  )
)


# Clean & align (remove NAs in y_test)
valid_idx <- !is.na(mc_catboost_y_test)
mc_catboost_actuals <- mc_catboost_y_test[valid_idx]
mc_catboost_predictions <- mc_catboost_probabilities_raw[valid_idx]
actual_factor <- factor(mc_catboost_actuals, levels = c(0, 1))


# ROC object for threshold determination
roc_obj <- pROC::roc(actual_factor, mc_catboost_predictions, quiet = TRUE)


# Tie-safe thresholds
best_vals <- pROC::coords(roc_obj, "best", ret = "threshold", transpose = FALSE)
roc_best_thresh <- median(as.numeric(unlist(best_vals)), na.rm = TRUE)

event_rate <- mean(mc_catboost_actuals, na.rm = TRUE)
recall_thresh <- max(0.15, event_rate)

cat("MYO ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("MYO Recall-friendly threshold:", round(recall_thresh, 3), "\n")


# Evaluate at both thresholds
metrics_roc <- create_classification_metrics(
  probs = mc_catboost_predictions,
  actual = mc_catboost_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("CatBoost (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = mc_catboost_predictions,
  actual = mc_catboost_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("CatBoost (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
mc_catboost_cal_metrics <- calculate_calibration_metrics(
  predictions = mc_catboost_predictions,
  actual = mc_catboost_actuals
)

# Combine & sort by Recall
all_mc_catboost_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = mc_catboost_cal_metrics$Calibration_Slope,
    Calibration_Intercept = mc_catboost_cal_metrics$Calibration_Intercept,
    Calibration_Brier = mc_catboost_cal_metrics$Calibration_Brier
  )
all_mc_catboost_metrics <- all_mc_catboost_metrics[order(-all_mc_catboost_metrics$Recall), ]
print(all_mc_catboost_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

##### CatBoost Random Forest Classification Model

```{r myo-randomforest-fit-model, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Prepare data for CatBoost Random Forest classification
mc_rf_data <- mc_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all categorical predictors are factors for CatBoost
  mutate(across(where(is.character), as.factor)) 

# Remove constant columns
constant_cols <- names(mc_rf_data)[sapply(mc_rf_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  mc_rf_data <- mc_rf_data %>% select(-all_of(constant_cols))
}


# Split data using existing train/test indices
mc_rf_train <- mc_rf_data[mc_train_indices, ]
mc_rf_test  <- mc_rf_data[mc_test_indices, ]

# Identify categorical features (columns with limited unique values)
categorical_features <- which(sapply(mc_rf_train, function(x) {
  length(unique(x)) <= 10 && length(unique(x)) > 1
}))

cat("Myo/Cardio CatBoost Random Forest - Training set size:", nrow(mc_rf_train), "\n")
cat("Myo/Cardio CatBoost Random Forest - Test set size:", nrow(mc_rf_test), "\n")
cat("Myo/Cardio CatBoost Random Forest - Features:", ncol(mc_rf_train) - 1, "\n")
cat("Myo/Cardio CatBoost Random Forest - Categorical features:", length(categorical_features), "\n")

# Prepare training and testing pools
mc_rf_train_pool <- catboost.load_pool(
  data = mc_rf_train[, -which(names(mc_rf_train) == "outcome")],
  label = mc_rf_train$outcome
)

mc_rf_test_pool <- catboost.load_pool(
  data = mc_rf_test[, -which(names(mc_rf_test) == "outcome")],
  label = mc_rf_test$outcome
)

# Random Forest-like parameters for CatBoost
mc_rf_params <- list(
  iterations = 500,               # number of trees
  depth = 8,                      # tree depth
  learning_rate = 1.0,            # no shrinkage, independent trees
  bootstrap_type = "Bernoulli",   # random row sampling
  subsample = 0.8,                # 80% rows per tree
  sampling_frequency = "PerTree", # resample once per tree
  rsm = 0.5,                      # use 50% of features at each split
  loss_function = "Logloss",      # classification
  eval_metric = "AUC",
  random_strength = 1.0,          # randomness in split score
  l2_leaf_reg = 3.0,             # mild regularization
  random_seed = 1997,
  verbose = 100,
  use_best_model = FALSE
)

# Train the CatBoost Random Forest model
set.seed(1997)
mc_rf_model <- catboost.train(mc_rf_train_pool, mc_rf_test_pool, mc_rf_params)

cat("Myo/Cardio CatBoost Random Forest Classification Model trained successfully\n")

```

###### Accuracy Metrics

```{r myo-randomforest-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}


# Predict probabilities on TEST data (CatBoost RF-mode)
mc_rf_probabilities_raw <- as.numeric(
  catboost.predict(
    mc_rf_model,
    mc_rf_test_pool,
    prediction_type = "Probability"
  )
)

# Clean & align actuals/predictions
actual_all <- mc_rf_test$outcome
valid_idx <- !is.na(actual_all)
mc_rf_actuals <- as.numeric(as.character(actual_all[valid_idx]))
mc_rf_predictions <- as.numeric(mc_rf_probabilities_raw[valid_idx])

# Thresholds via ROC (tie-safe)

roc_obj <- pROC::roc(factor(mc_rf_actuals, levels = c(0, 1)), mc_rf_predictions, quiet = TRUE)

best_vals <- pROC::coords(roc_obj, "best", ret = "threshold", transpose = FALSE)
roc_best_thresh <- median(as.numeric(unlist(best_vals)), na.rm = TRUE)

event_rate <- mean(mc_rf_actuals, na.rm = TRUE)
recall_thresh <- max(0.15, event_rate)

cat("MYO RF ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("MYO RF Recall-friendly threshold:", round(recall_thresh, 3), "\n")

# Evaluate at both thresholds using your metrics helper
metrics_roc <- create_classification_metrics(
  probs = mc_rf_predictions,
  actual = mc_rf_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("CatBoost Random Forest (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = mc_rf_predictions,
  actual = mc_rf_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("CatBoost Random Forest (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
mc_rf_cal_metrics <- calculate_calibration_metrics(
  predictions = mc_rf_predictions,
  actual = mc_rf_actuals
)

# Combine & sort by Recall
all_mc_rf_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = mc_rf_cal_metrics$Calibration_Slope,
    Calibration_Intercept = mc_rf_cal_metrics$Calibration_Intercept,
    Calibration_Brier = mc_rf_cal_metrics$Calibration_Brier
  )
all_mc_rf_metrics <- all_mc_rf_metrics[order(-all_mc_rf_metrics$Recall), ]
print(all_mc_rf_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

##### Traditional Random Forest Classification Model

```{r myo-traditional-rf-fit-model, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Prepare data for traditional Random Forest classification
mc_trad_rf_data <- mc_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(classification_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # Remove identifier columns
  select(-ptid_e) %>%
  # Ensure all categorical predictors are factors for CatBoost
  mutate(across(where(is.character), as.factor)) %>% 
  # Ensure outcome is a factor for Random Forest
  mutate(outcome = as.factor(outcome))


# Remove constant columns
constant_cols <- names(mc_trad_rf_data)[sapply(mc_trad_rf_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  mc_trad_rf_data <- mc_trad_rf_data %>% select(-all_of(constant_cols))
}

# Split data using existing train/test indices
mc_trad_rf_train <- mc_trad_rf_data[mc_train_indices, ]
mc_trad_rf_test  <- mc_trad_rf_data[mc_test_indices, ]

cat("Myo/Cardio Traditional Random Forest - Training set size:", nrow(mc_trad_rf_train), "\n")
cat("Myo/Cardio Traditional Random Forest - Test set size:", nrow(mc_trad_rf_test), "\n")
cat("Myo/Cardio Traditional Random Forest - Features:", ncol(mc_trad_rf_train) - 1, "\n")

# Train the traditional Random Forest model
set.seed(1997)
mc_trad_rf_model <- randomForest(
  outcome ~ .,
  data = mc_trad_rf_train,
  ntree = 500,           # Number of trees
  mtry = sqrt(ncol(mc_trad_rf_train) - 1),  # Number of features to consider at each split
  importance = TRUE,      # Calculate feature importance
  proximity = FALSE,      # Don't calculate proximity matrix to save memory
  do.trace = 50,          # Print progress every 50 trees
  na.action=na.roughfix 
)

cat("Myo/Cardio Traditional Random Forest Classification Model trained successfully\n")

```

###### Accuracy Metrics

```{r myo-traditional-rf-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}


# Predict probabilities on TEST data (native randomForest)
rf_prob_mat <- predict(mc_trad_rf_model, mc_trad_rf_test, type = "prob")

# Pick the positive-class column robustly: prefer "1" if present, else use the last column
pos_col <- if ("1" %in% colnames(rf_prob_mat)) "1" else tail(colnames(rf_prob_mat), 1)
mc_trad_rf_probabilities_raw <- as.numeric(rf_prob_mat[, pos_col])


# Clean and align actuals and predictions
valid_idx <- !is.na(mc_trad_rf_test$outcome)
mc_trad_rf_actuals <- as.numeric(as.character(mc_trad_rf_test$outcome[valid_idx]))
mc_trad_rf_predictions <- mc_trad_rf_probabilities_raw[valid_idx]

# Thresholds via ROC (tie-safe)
roc_obj <- pROC::roc(factor(mc_trad_rf_actuals, levels = c(0, 1)), mc_trad_rf_predictions, quiet = TRUE)

best_vals <- pROC::coords(roc_obj, "best", ret = "threshold", transpose = FALSE)
roc_best_thresh <- median(as.numeric(unlist(best_vals)), na.rm = TRUE)


event_rate <- mean(mc_trad_rf_actuals, na.rm = TRUE)
# If actual_clean is already numeric 0/1, the as.numeric(as.character(.)) is a no-op
recall_thresh <- max(0.15, event_rate)

cat("MYO RF (native) ROC-optimal threshold:", round(roc_best_thresh, 3), "\n")
cat("MYO RF (native) Recall-friendly threshold:", round(recall_thresh, 3), "\n")

# Evaluate at both thresholds using your helper
metrics_roc <- create_classification_metrics(
  probs = mc_trad_rf_predictions,
  actual = mc_trad_rf_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("Traditional Random Forest (ROC-optimal=", round(roc_best_thresh, 2), ")"),
  threshold = roc_best_thresh
)

metrics_recall <- create_classification_metrics(
  probs = mc_trad_rf_predictions,
  actual = mc_trad_rf_actuals,
  cohort_name = "Myo/Cardio",
  model_name = paste0("Traditional Random Forest (Recall-friendly=", round(recall_thresh, 2), ")"),
  threshold = recall_thresh
)

# Calculate calibration metrics
mc_trad_rf_cal_metrics <- calculate_calibration_metrics(
  predictions = mc_trad_rf_predictions,
  actual = mc_trad_rf_actuals
)

# Combine & sort by Recall
all_mc_trad_rf_metrics <- rbind(metrics_roc, metrics_recall) %>%
  mutate(
    Calibration_Slope = mc_trad_rf_cal_metrics$Calibration_Slope,
    Calibration_Intercept = mc_trad_rf_cal_metrics$Calibration_Intercept,
    Calibration_Brier = mc_trad_rf_cal_metrics$Calibration_Brier
  )
all_mc_trad_rf_metrics <- all_mc_trad_rf_metrics[order(-all_mc_trad_rf_metrics$Recall), ]
print(all_mc_trad_rf_metrics)

# AUC (threshold-independent)
cat("Test AUC:", round(pROC::auc(roc_obj), 3), "\n")


```

### Model Performance Comparison by Cohort

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Initialize a list to hold all metrics data frames
all_metrics_list <- list()

# Add metrics if they exist, using the correct object names from the cohort analysis
if(exists("all_chd_lasso_metrics")) {
  all_metrics_list[["chd_lasso"]] <- all_chd_lasso_metrics
}
if(exists("all_chd_catboost_metrics")) {
  all_metrics_list[["chd_catboost"]] <- all_chd_catboost_metrics
}
if(exists("all_chd_rf_metrics")) {
  all_metrics_list[["chd_rf"]] <- all_chd_rf_metrics
}
if(exists("all_chd_trad_rf_metrics")) {
  all_metrics_list[["chd_trad_rf"]] <- all_chd_trad_rf_metrics
}
if(exists("all_mc_lasso_metrics")) {
  all_metrics_list[["mc_lasso"]] <- all_mc_lasso_metrics
}
if(exists("all_mc_catboost_metrics")) {
  all_metrics_list[["mc_catboost"]] <- all_mc_catboost_metrics
}
if(exists("all_mc_rf_metrics")) {
  all_metrics_list[["mc_rf"]] <- all_mc_rf_metrics
}
if(exists("all_mc_trad_rf_metrics")) {
  all_metrics_list[["mc_trad_rf"]] <- all_mc_trad_rf_metrics
}


if(length(all_metrics_list) > 0) {
  # Combine all data frames into one
  cohort_metrics_df <- bind_rows(all_metrics_list, .id = "source")
  
  # Display comparison table
  datatable(
    cohort_metrics_df %>% select(-source), # Hide the source column if desired
    caption = "Cohort Performance Comparison - Classification Models",
    rownames = FALSE,
    options = list(
      pageLength = 10,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )
} else {
  cat("No cohort metrics available for comparison\n")
}

```

### Model Calibration Analysis

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Create calibration plots for all models
calibration_plots <- list()

# CHD Cohort Calibration Plots

if(exists("chd_lasso_predictions") && exists("chd_y_test")) {
  chd_lasso_cal_plot <- create_calibration_plot(
    chd_lasso_predictions, 
    chd_y_test,
    "CHD", 
    "LASSO"
  )
  if(!is.null(chd_lasso_cal_plot)) {
    calibration_plots[["CHD_LASSO"]] <- chd_lasso_cal_plot
  }
}

if(exists("chd_catboost_predictions") && exists("chd_catboost_y_test")) {
  chd_catboost_cal_plot <- create_calibration_plot(
    chd_catboost_predictions, 
    chd_catboost_y_test,
    "CHD", 
    "CatBoost"
  )
  if(!is.null(chd_catboost_cal_plot)) {
    calibration_plots[["CHD_CatBoost"]] <- chd_catboost_cal_plot
  }
}

if(exists("chd_rf_predictions") && exists("chd_rf_test$outcome")) {
  chd_rf_cal_plot <- create_calibration_plot(
    chd_rf_predictions, 
    chd_rf_test$outcome,
    "CHD", 
    "CatBoost RF"
  )
  if(!is.null(chd_rf_cal_plot)) {
    calibration_plots[["CHD_CatBoost_RF"]] <- chd_rf_cal_plot
  }
}

if(exists("chd_trad_rf_predictions") && exists("chd_trad_rf_test$outcome")) {
  chd_trad_rf_cal_plot <- create_calibration_plot(
    chd_trad_rf_predictions,
    chd_trad_rf_test$outcome,
    "CHD",
    "Traditional RF"
  )
  if(!is.null(chd_trad_rf_cal_plot)) {
    calibration_plots[["CHD_Traditional_RF"]] <- chd_trad_rf_cal_plot
  }
}

# Myo/Cardio Cohort Calibration Plots
if(exists("mc_lasso_predictions") && exists("mc_lasso_actuals")) {
  mc_lasso_cal_plot <- create_calibration_plot(
    mc_lasso_predictions,
    mc_lasso_actuals,
    "Myo/Cardio",
    "LASSO"
  )
  if(!is.null(mc_lasso_cal_plot)) {
    calibration_plots[["Myo_LASSO"]] <- mc_lasso_cal_plot
  }
}

if(exists("mc_catboost_predictions") && exists("mc_catboost_y_test")) {
  mc_catboost_cal_plot <- create_calibration_plot(
    mc_catboost_predictions,
    mc_catboost_y_test,
    "Myo/Cardio",
    "CatBoost"
  )
  if(!is.null(mc_catboost_cal_plot)) {
    calibration_plots[["Myo_CatBoost"]] <- mc_catboost_cal_plot
  }
}

if(exists("mc_rf_predictions") && exists("mc_rf_test$outcome")) {
  mc_rf_cal_plot <- create_calibration_plot(
    mc_rf_predictions,
    mc_rf_test$outcome,
    "Myo/Cardio",
    "CatBoost RF"
  )
  if(!is.null(mc_rf_cal_plot)) {
    calibration_plots[["Myo_CatBoost_RF"]] <- mc_rf_cal_plot
  }
}

if(exists("mc_trad_rf_probabilities") && exists("mc_trad_rf_test$outcome")) {
  mc_trad_rf_cal_plot <- create_calibration_plot(
    mc_trad_rf_probabilities,
    mc_trad_rf_test$outcome,
    "Myo/Cardio",
    "Traditional RF"
  )
  if(!is.null(mc_trad_rf_cal_plot)) {
    calibration_plots[["Myo_Traditional_RF"]] <- mc_trad_rf_cal_plot
  }
}

# Display calibration plots
if(length(calibration_plots) > 0) {
  cat("## Calibration Analysis for All Models\n\n")
  cat("Calibration plots show how well predicted probabilities align with observed event rates.\n")
  cat("Perfect calibration: points fall on the diagonal line (slope = 1, intercept = 0).\n\n")
  
  # Display each calibration plot
  for(plot_name in names(calibration_plots)) {
    cat("###", plot_name, "\n\n")
    print(calibration_plots[[plot_name]])
    cat("\n\n")
  }
} else {
  cat("No calibration plots available - ensure all models have been trained and predictions generated.\n")
}

```

### Calibration Metrics Summary

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Collect all metrics into a single list
all_model_metrics <- list(
  all_chd_lasso_metrics,
  all_chd_catboost_metrics,
  all_chd_rf_metrics,
  all_chd_trad_rf_metrics,
  all_mc_lasso_metrics,
  all_mc_catboost_metrics,
  all_mc_rf_metrics,
  all_mc_trad_rf_metrics
)

# Create the summary tables
metrics_summary <- create_metrics_summary(all_model_metrics)

# Display the summary table
datatable(
  metrics_summary$summary,
  caption = "Model Performance Summary",
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)
```

### All Cohorts and All Models

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Comprehensive summary table with top 25 features for each model and cohort
comprehensive_summary <- list()

# Function to get top 25 features with normalized importance
get_top_features <- function(feature_df, cohort_name, model_name, metrics_list = NULL) {
  if (nrow(feature_df) > 0) {
    # Get top 25 features
    top_features <- feature_df %>%
      slice_head(n = 25) %>%
      mutate(
        Cohort = cohort_name,
        Model = model_name,
        Rank = row_number()
      )
    
    # Normalize importance to 0-1 scale for Sankey diagram
    if ("importance" %in% colnames(top_features)) {
      top_features <- top_features %>%
        mutate(
          Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
        )
    } else if ("coefficient" %in% colnames(top_features)) {
      # For LASSO coefficients, use absolute values and normalize
      top_features <- top_features %>%
        mutate(
          importance = abs(coefficient),
          Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
        )
    }
    
    # Add model performance metrics if available
    if (!is.null(metrics_list)) {
      top_features <- top_features %>%
        mutate(
          AUC = ifelse("AUC" %in% names(metrics_list), round(metrics_list$AUC, 3), NA),
          C_Index = ifelse("C_Index" %in% names(metrics_list), round(metrics_list$C_Index, 3), NA),
          Accuracy = ifelse("Accuracy" %in% names(metrics_list), round(metrics_list$Accuracy, 3), NA),
          F1 = ifelse("F1" %in% names(metrics_list), round(metrics_list$F1, 3), NA)
        )
    }
    
    return(top_features)
  }
  return(NULL)
}

# --- Collect CHD Feature Importance ---

# 1. CHD LASSO
if (exists("chd_class_nonzero_coefs")) {
  if (exists("all_chd_lasso_metrics")) {
    chd_lasso_features <- get_top_features(chd_class_nonzero_coefs, "CHD", "LASSO", all_chd_lasso_metrics)
    if (!is.null(chd_lasso_features)) comprehensive_summary <- append(comprehensive_summary, list(chd_lasso_features))
  } else {
    warning("all_chd_lasso_metrics not found - skipping CHD LASSO features")
  }
}

# 2. CHD CatBoost
if (exists("chd_catboost_model")) {
  if (exists("all_chd_catboost_metrics")) {
    chd_catboost_imp <- catboost.get_feature_importance(chd_catboost_model, type = "FeatureImportance")
    chd_catboost_imp_df <- data.frame(feature = rownames(chd_catboost_imp), importance = chd_catboost_imp[,1]) %>% arrange(desc(importance))
    chd_catboost_features <- get_top_features(chd_catboost_imp_df, "CHD", "CatBoost", all_chd_catboost_metrics)
    if (!is.null(chd_catboost_features)) comprehensive_summary <- append(comprehensive_summary, list(chd_catboost_features))
  } else {
    warning("all_chd_catboost_metrics not found - skipping CHD CatBoost features")
  }
}

# 3. CHD CatBoost RF
if (exists("chd_rf_model")) {
  if (exists("all_chd_rf_metrics")) {
    chd_rf_imp <- catboost.get_feature_importance(chd_rf_model, type = "FeatureImportance")
    chd_rf_imp_df <- data.frame(feature = rownames(chd_rf_imp), importance = chd_rf_imp[,1]) %>% arrange(desc(importance))
    chd_rf_features <- get_top_features(chd_rf_imp_df, "CHD", "CatBoost RF", all_chd_rf_metrics)
    if (!is.null(chd_rf_features)) comprehensive_summary <- append(comprehensive_summary, list(chd_rf_features))
  } else {
    warning("all_chd_rf_metrics not found - skipping CHD CatBoost RF features")
  }
}

# 4. CHD Traditional RF
if (exists("chd_trad_rf_model")) {
  if (exists("all_chd_trad_rf_metrics")) {
    chd_trad_rf_imp <- as.data.frame(importance(chd_trad_rf_model))
    chd_trad_rf_imp_df <- chd_trad_rf_imp %>% rownames_to_column("feature") %>% select(feature, importance = MeanDecreaseGini) %>% arrange(desc(importance))
    chd_trad_rf_features <- get_top_features(chd_trad_rf_imp_df, "CHD", "Traditional RF", all_chd_trad_rf_metrics)
    if (!is.null(chd_trad_rf_features)) comprehensive_summary <- append(comprehensive_summary, list(chd_trad_rf_features))
  } else {
    warning("all_chd_trad_rf_metrics not found - skipping CHD Traditional RF features")
  }
}


# --- Collect Myo/Cardio Feature Importance ---

# 1. Myo/Cardio LASSO
if (exists("mc_class_nonzero_coefs")) {
  if (exists("all_mc_lasso_metrics")) {
    mc_lasso_features <- get_top_features(mc_class_nonzero_coefs, "Myo/Cardio", "LASSO", all_mc_lasso_metrics)
    if (!is.null(mc_lasso_features)) comprehensive_summary <- append(comprehensive_summary, list(mc_lasso_features))
  } else {
    warning("all_mc_lasso_metrics not found - skipping Myo/Cardio LASSO features")
  }
}

# 2. Myo/Cardio CatBoost
if (exists("mc_catboost_model")) {
  if (exists("all_mc_catboost_metrics")) {
    mc_catboost_imp <- catboost.get_feature_importance(mc_catboost_model, type = "FeatureImportance")
    mc_catboost_imp_df <- data.frame(feature = rownames(mc_catboost_imp), importance = mc_catboost_imp[,1]) %>% arrange(desc(importance))
    mc_catboost_features <- get_top_features(mc_catboost_imp_df, "Myo/Cardio", "CatBoost", all_mc_catboost_metrics)
    if (!is.null(mc_catboost_features)) comprehensive_summary <- append(comprehensive_summary, list(mc_catboost_features))
  } else {
    warning("all_mc_catboost_metrics not found - skipping Myo/Cardio CatBoost features")
  }
}

# 3. Myo/Cardio CatBoost RF
if (exists("mc_rf_model")) {
  if (exists("all_mc_rf_metrics")) {
    mc_rf_imp <- catboost.get_feature_importance(mc_rf_model, type = "FeatureImportance")
    mc_rf_imp_df <- data.frame(feature = rownames(mc_rf_imp), importance = mc_rf_imp[,1]) %>% arrange(desc(importance))
    mc_rf_features <- get_top_features(mc_rf_imp_df, "Myo/Cardio", "CatBoost RF", all_mc_rf_metrics)
    if (!is.null(mc_rf_features)) comprehensive_summary <- append(comprehensive_summary, list(mc_rf_features))
  } else {
    warning("all_mc_rf_metrics not found - skipping Myo/Cardio CatBoost RF features")
  }
}

# 4. Myo/Cardio Traditional RF
if (exists("mc_trad_rf_model")) {
  if (exists("all_mc_trad_rf_metrics")) {
    mc_trad_rf_imp <- as.data.frame(importance(mc_trad_rf_model))
    mc_trad_rf_imp_df <- mc_trad_rf_imp %>% rownames_to_column("feature") %>% select(feature, importance = MeanDecreaseGini) %>% arrange(desc(importance))
    mc_trad_rf_features <- get_top_features(mc_trad_rf_imp_df, "Myo/Cardio", "Traditional RF", all_mc_trad_rf_metrics)
    if (!is.null(mc_trad_rf_features)) comprehensive_summary <- append(comprehensive_summary, list(mc_trad_rf_features))
  } else {
    warning("all_mc_trad_rf_metrics not found - skipping Myo/Cardio Traditional RF features")
  }
}


# Combine all feature importance data
if (length(comprehensive_summary) > 0) {
  all_features_df <- bind_rows(comprehensive_summary)
  cat("Successfully collected feature importance from", length(comprehensive_summary), "models\n")
} else {
  warning("No feature importance data collected - comprehensive_summary is empty")
  all_features_df <- data.frame()
}
  
```

```{r}

library(dplyr)
library(DT)

# Select and arrange columns for a clear feature-focused view
if (nrow(all_features_df) > 0) {
  feature_summary_df <- all_features_df %>%
    select(Rank, feature, Cohort, Model, importance, Normalized_Importance) %>%
    arrange(Rank, Model, Cohort)

  # Display the interactive table
  datatable(
    feature_summary_df,
    caption = "Feature Importances by Cohort and Model",
    rownames = FALSE,
    options = list(
      pageLength = 25,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )
} else {
  cat("No feature importance data available to display\n")
}

```

### Workflow Summary for Comparison

```{r workflow-summary, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Create comprehensive workflow summary
workflow_summary <- data.frame(
  Workflow = c("Cohort Event Classification"),
  CHD_Cohort_Size = nrow(chd_data),
  CHD_Control_Size = sum(chd_data$outcome == 0, na.rm = TRUE),
  CHD_Target_Size = sum(chd_data$outcome == 1, na.rm = TRUE),
  CHD_Event_Rate = round(mean(chd_data$outcome, na.rm = TRUE) * 100, 2),
  MC_Cohort_Size = nrow(mc_data),
  MC_Control_Size = sum(mc_data$outcome == 0, na.rm = TRUE),
  MC_Target_Size = sum(mc_data$outcome == 1, na.rm = TRUE),
  MC_Event_Rate = round(mean(mc_data$outcome, na.rm = TRUE) * 100, 2)
)

# Add model performance metrics
if (exists("all_chd_lasso_metrics")) {
  chd_lasso_best <- all_chd_lasso_metrics %>% slice(1)  # Best by Recall
  workflow_summary$CHD_LASSO_AUC <- round(chd_lasso_best$AUC, 3)
  workflow_summary$CHD_LASSO_Precision <- round(chd_lasso_best$Precision, 3)
  workflow_summary$CHD_LASSO_Recall <- round(chd_lasso_best$Recall, 3)
} else {
  workflow_summary$CHD_LASSO_AUC <- NA
  workflow_summary$CHD_LASSO_Precision <- NA
  workflow_summary$CHD_LASSO_Recall <- NA
}

if (exists("all_chd_catboost_metrics")) {
  chd_catboost_best <- all_chd_catboost_metrics %>% slice(1)
  workflow_summary$CHD_CatBoost_AUC <- round(chd_catboost_best$AUC, 3)
  workflow_summary$CHD_CatBoost_Precision <- round(chd_catboost_best$Precision, 3)
  workflow_summary$CHD_CatBoost_Recall <- round(chd_catboost_best$Recall, 3)
} else {
  workflow_summary$CHD_CatBoost_AUC <- NA
  workflow_summary$CHD_CatBoost_Precision <- NA
  workflow_summary$CHD_CatBoost_Recall <- NA
}

if (exists("all_chd_rf_metrics")) {
  chd_rf_best <- all_chd_rf_metrics %>% slice(1)
  workflow_summary$CHD_CatBoost_RF_AUC <- round(chd_rf_best$AUC, 3)
  workflow_summary$CHD_CatBoost_RF_Precision <- round(chd_rf_best$Precision, 3)
  workflow_summary$CHD_CatBoost_RF_Recall <- round(chd_rf_best$Recall, 3)
} else {
  workflow_summary$CHD_CatBoost_RF_AUC <- NA
  workflow_summary$CHD_CatBoost_RF_Precision <- NA
  workflow_summary$CHD_CatBoost_RF_Recall <- NA
}

if (exists("all_chd_trad_rf_metrics")) {
  chd_trad_rf_best <- all_chd_trad_rf_metrics %>% slice(1)
  workflow_summary$CHD_Traditional_RF_AUC <- round(chd_trad_rf_best$AUC, 3)
  workflow_summary$CHD_Traditional_RF_Precision <- round(chd_trad_rf_best$Precision, 3)
  workflow_summary$CHD_Traditional_RF_Recall <- round(chd_trad_rf_best$Recall, 3)
} else {
  workflow_summary$CHD_Traditional_RF_AUC <- NA
  workflow_summary$CHD_Traditional_RF_Precision <- NA
  workflow_summary$CHD_Traditional_RF_Recall <- NA
}

if (exists("all_mc_lasso_metrics")) {
  mc_lasso_best <- all_mc_lasso_metrics %>% slice(1)
  workflow_summary$MC_LASSO_AUC <- round(mc_lasso_best$AUC, 3)
  workflow_summary$MC_LASSO_Precision <- round(mc_lasso_best$Precision, 3)
  workflow_summary$MC_LASSO_Recall <- round(mc_lasso_best$Recall, 3)
} else {
  workflow_summary$MC_LASSO_AUC <- NA
  workflow_summary$MC_LASSO_Precision <- NA
  workflow_summary$MC_LASSO_Recall <- NA
}

if (exists("all_mc_catboost_metrics")) {
  mc_catboost_best <- all_mc_catboost_metrics %>% slice(1)
  workflow_summary$MC_CatBoost_AUC <- round(mc_catboost_best$AUC, 3)
  workflow_summary$MC_CatBoost_Precision <- round(mc_catboost_best$Precision, 3)
  workflow_summary$MC_CatBoost_Recall <- round(mc_catboost_best$Recall, 3)
} else {
  workflow_summary$MC_CatBoost_AUC <- NA
  workflow_summary$MC_CatBoost_Precision <- NA
  workflow_summary$MC_CatBoost_Recall <- NA
}

if (exists("all_mc_rf_metrics")) {
  mc_rf_best <- all_mc_rf_metrics %>% slice(1)
  workflow_summary$MC_CatBoost_RF_AUC <- round(mc_rf_best$AUC, 3)
  workflow_summary$MC_CatBoost_RF_Precision <- round(mc_rf_best$Precision, 3)
  workflow_summary$MC_CatBoost_RF_Recall <- round(mc_rf_best$Recall, 3)
} else {
  workflow_summary$MC_CatBoost_RF_AUC <- NA
  workflow_summary$MC_CatBoost_RF_Precision <- NA
  workflow_summary$MC_CatBoost_RF_Recall <- NA
}

if (exists("all_mc_trad_rf_metrics")) {
  mc_trad_rf_best <- all_mc_trad_rf_metrics %>% slice(1)
  workflow_summary$MC_Traditional_RF_AUC <- round(mc_trad_rf_best$AUC, 3)
  workflow_summary$MC_Traditional_RF_Precision <- round(mc_trad_rf_best$Precision, 3)
  workflow_summary$MC_Traditional_RF_Recall <- round(mc_trad_rf_best$Recall, 3)
} else {
  workflow_summary$MC_Traditional_RF_AUC <- NA
  workflow_summary$MC_Traditional_RF_Precision <- NA
  workflow_summary$MC_Traditional_RF_Recall <- NA
}

# Display the comprehensive summary
cat("=== COHORT EVENT CLASSIFICATION WORKFLOW SUMMARY ===\n\n")

# Cohort sizes
cat("CHD Cohort:\n")
cat("  Total Size:", workflow_summary$CHD_Cohort_Size, "patients\n")
cat("  Control (outcome=0):", workflow_summary$CHD_Control_Size, "patients\n")
cat("  Target (outcome=1):", workflow_summary$CHD_Target_Size, "patients\n")
cat("  Event Rate:", workflow_summary$CHD_Event_Rate, "%\n\n")

cat("Myo/Cardio Cohort:\n")
cat("  Total Size:", workflow_summary$MC_Cohort_Size, "patients\n")
cat("  Control (outcome=0):", workflow_summary$MC_Control_Size, "patients\n")
cat("  Target (outcome=1):", workflow_summary$MC_Target_Size, "patients\n")
cat("  Event Rate:", workflow_summary$MC_Event_Rate, "%\n\n")

# Model performance summary
cat("Model Performance Summary (Best by Recall):\n")
cat("  CHD LASSO - AUC:", workflow_summary$CHD_LASSO_AUC, "Precision:", workflow_summary$CHD_LASSO_Precision, "Recall:", workflow_summary$CHD_LASSO_Recall, "\n")
cat("  CHD CatBoost - AUC:", workflow_summary$CHD_CatBoost_AUC, "Precision:", workflow_summary$CHD_CatBoost_Precision, "Recall:", workflow_summary$CHD_CatBoost_Recall, "\n")
cat("  CHD CatBoost RF - AUC:", workflow_summary$CHD_CatBoost_RF_AUC, "Precision:", workflow_summary$CHD_CatBoost_RF_Precision, "Recall:", workflow_summary$CHD_CatBoost_RF_Recall, "\n")
cat("  CHD Traditional RF - AUC:", workflow_summary$CHD_Traditional_RF_AUC, "Precision:", workflow_summary$CHD_Traditional_RF_Precision, "Recall:", workflow_summary$CHD_Traditional_RF_Recall, "\n")
cat("  MC LASSO - AUC:", workflow_summary$MC_LASSO_AUC, "Precision:", workflow_summary$MC_LASSO_Precision, "Recall:", workflow_summary$MC_LASSO_Recall, "\n")
cat("  MC CatBoost - AUC:", workflow_summary$MC_CatBoost_AUC, "Precision:", workflow_summary$MC_CatBoost_Precision, "Recall:", workflow_summary$MC_CatBoost_Recall, "\n")
cat("  MC CatBoost RF - AUC:", workflow_summary$MC_CatBoost_RF_AUC, "Precision:", workflow_summary$MC_CatBoost_RF_Precision, "Recall:", workflow_summary$MC_CatBoost_RF_Recall, "\n")
cat("  MC Traditional RF - AUC:", workflow_summary$MC_Traditional_RF_AUC, "Precision:", workflow_summary$MC_Traditional_RF_Precision, "Recall:", workflow_summary$MC_Traditional_RF_Recall, "\n\n")

# Save summary for external comparison
write.csv(workflow_summary, "cohort_event_classification_summary.csv", row.names = FALSE)
cat("Summary saved to: cohort_event_classification_summary.csv\n")

# Display as table
datatable(
  workflow_summary,
  caption = "Cohort Event Classification Workflow Summary",
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

### Sankey Chart: Cohort to Features - Classification Model

```{r sankey-plot, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

library(dplyr)
library(plotly)

# Prepare data for the Sankey diagram
if (nrow(all_features_df) > 0) {
  sankey_data <- all_features_df %>%
    select(source = Cohort, target = feature, value = Normalized_Importance) %>%
    filter(!is.na(value) & value > 0)

  if(nrow(sankey_data) > 0) {
  
  # Create a unique list of all nodes (cohorts and features)
  all_nodes <- unique(c(sankey_data$source, sankey_data$target))
  
  # Create a links data frame with 0-based indices
  links <- sankey_data %>%
    mutate(
      source = match(source, all_nodes) - 1,
      target = match(target, all_nodes) - 1
    )

  # Create the Sankey plot
cohort_sankey_plot <- plot_ly(
    type = "sankey",
    orientation = "h",
    node = list(
      label = all_nodes,
      pad = 15,
      thickness = 20,
      line = list(color = "black", width = 0.5)
    ),
    link = list(
      source = links$source,
      target = links$target,
      value = links$value
    )
  ) %>%
    layout(
      title = "Feature Importance Flow: Cohorts to Features",
      font = list(size = 10)
    )
    
  } else {
    cat("No valid data available to generate Sankey diagram.\n")
  }
} else {
  cat("No feature importance data available to generate Sankey diagram.\n")
}

cohort_sankey_plot

```


```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}

if (exists("cohort_sankey_plot")) {
  htmlwidgets::saveWidget(
    cohort_sankey_plot,
    file = "sankey_cohort_classification_feature_importance.html",
    selfcontained = TRUE
  )
}


```

[Full Size Sankey Cohort Chart](https://jerome-dixon.io/uva/notebooks/sankey_cohort_classification_feature_importance.html?Expires=1760110551&Signature=nmIziaZRTh8j~RJdFCaJ7KpeFAuA0dzI4x7WNXxrTG7DA1VMtYabOpgBmEq4TudvUn15luBd1Cf023pShihttboykIYk6xU0~e4HRabsE3n9UG5v5q9I1apoatXWMItzotIGdwOLBceTuvzJjqLfN5EbVoEainXdMAc6E9mbpjDwIpfkcQiCk7SICLNOE49mjj1NHMqxC2YyV1BXaPV0-w86y5-RNAVhZgu4jn1U6uO83HbSvVciuI29WGIRbyomTzqD7UdKoDs0BO9MVINzPDV41pNyeRsUrb7vsz2aqK-dn75Ca4UawU11plKMxzVKD5f2ZgWHYZCUeA0dVTrmhA__&Key-Pair-Id=KH0ZA0Z2LKGN9)

