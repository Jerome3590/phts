# ‚úÖ Ready to Run: 1000-Split Replication Study

**Status:** Configuration complete and optimized for EC2 ‚úÖ  
**Date:** November 21, 2025  
**Hardware:** 32-core EC2 instance, 1TB RAM

---

## üöÄ Quick Start (Choose One)

### Option 1: R Script (Recommended)

```bash
# SSH into EC2
ssh -i your-key.pem ec2-user@your-ec2-ip

# Start tmux session
tmux new -s replication

# Run analysis
export N_WORKERS=30
cd /path/to/phts
time Rscript graft-loss/feature_importance/replicate_20_features_MC_CV.R 2>&1 | tee replication_1000.log

# Detach: Ctrl+B then D
# Reattach: tmux attach -t replication
```

### Option 2: Jupyter Notebook

```bash
# Start Jupyter on EC2
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser

# From local machine, create SSH tunnel
ssh -i your-key.pem -L 8888:localhost:8888 ec2-user@your-ec2-ip

# Open: http://localhost:8888
# Navigate to: graft_loss_feature_importance_20_MC_CV.ipynb
# Click: Run All
```

---

## ‚öôÔ∏è Configuration Summary

```r
n_predictors <- 20      # Top 20 features
n_trees_rsf <- 500      # RSF: 500 trees
n_trees_aorsf <- 100    # AORSF: 100 trees
horizon <- 1            # 1-year prediction
n_mc_splits <- 1000     # ‚úÖ 1000 splits (publication quality)
train_prop <- 0.75      # 75/25 train/test
n_workers <- 30         # ‚úÖ 30 parallel workers (EC2 optimized)
```

---

## üìä What Will Happen

### Timeline (Expected: 30-45 minutes)

```
00:00 - Analysis starts
      ‚îî‚îÄ Loading data and setting up 30 workers
      
00:03 - Original Period starts (2010-2019)
      ‚îú‚îÄ RSF: 1000 splits (5-8 min)
      ‚îú‚îÄ CatBoost: 1000 splits (3-5 min)
      ‚îî‚îÄ AORSF: 1000 splits (5-8 min)
      
00:16 - Full Period starts (2010-2024)
      ‚îú‚îÄ RSF: 1000 splits (6-10 min)
      ‚îú‚îÄ CatBoost: 1000 splits (4-6 min)
      ‚îî‚îÄ AORSF: 1000 splits (6-10 min)
      
00:32 - Full No COVID starts (2010-2024 excl 2020-2023)
      ‚îú‚îÄ RSF: 1000 splits (5-8 min)
      ‚îú‚îÄ CatBoost: 1000 splits (3-5 min)
      ‚îî‚îÄ AORSF: 1000 splits (5-8 min)
      
00:45 - ‚úÖ Complete!
      ‚îî‚îÄ 11 output files saved
```

### Output Files (11 total)

```
graft-loss/feature_importance/outputs/
‚îú‚îÄ‚îÄ original_rsf_top20.csv                    ‚Üê Top 20 features
‚îú‚îÄ‚îÄ original_catboost_top20.csv
‚îú‚îÄ‚îÄ original_aorsf_top20.csv
‚îú‚îÄ‚îÄ full_rsf_top20.csv
‚îú‚îÄ‚îÄ full_catboost_top20.csv
‚îú‚îÄ‚îÄ full_aorsf_top20.csv
‚îú‚îÄ‚îÄ full_no_covid_rsf_top20.csv
‚îú‚îÄ‚îÄ full_no_covid_catboost_top20.csv
‚îú‚îÄ‚îÄ full_no_covid_aorsf_top20.csv
‚îú‚îÄ‚îÄ cindex_comparison_mc_cv.csv               ‚Üê C-index summary
‚îî‚îÄ‚îÄ summary_statistics_mc_cv.csv              ‚Üê Detailed stats with CI
```

**Interpretation of the scaled feature chart:**

- The `scaled_feature_importance_bar_chart.png` reports a per-feature score computed by summing each feature's per-model, per-period normalized importance after scaling by the model's relative performance (see `importance_weights.R`). In short: importances are made non-negative, normalized to unit-sum per model-period, multiplied by a relative weight based on mean held-out C-index, and then summed across cohort √ó algorithm cells. Higher bars indicate features with broad, weighted support from better-performing models.

---

## üëÄ Monitor Progress

### In Separate SSH Session

```bash
# Watch output directory
watch -n 5 'ls -lh graft-loss/feature_importance/outputs/ | tail -15'

# Count completed files (should reach 11)
watch -n 5 'ls -1 graft-loss/feature_importance/outputs/*.csv | wc -l'

# Monitor CPU usage (should see ~30 cores active)
htop

# Check memory (should use <100GB of 1TB available)
free -h
```

### If Running with nohup

```bash
# Watch live output
tail -f replication_1000.log

# Search for completed periods
grep "Period complete" replication_1000.log

# Check for errors
grep -i "error\|warning" replication_1000.log
```

---

## üìà Expected Results

### C-Index Values (All Models, All Periods)

```
Model    | Original | Full     | Full No COVID
---------|----------|----------|--------------
RSF      | 0.74     | 0.78     | 0.75
CatBoost | 0.82     | 0.82     | 0.83
AORSF    | 0.76     | 0.80     | 0.77
```

**Characteristics:**
- ‚úÖ Values in 0.70-0.85 range (realistic)
- ‚úÖ 95% CI widths < 0.10 (narrow and precise)
- ‚úÖ CatBoost > AORSF > RSF (expected ranking)
- ‚úÖ Consistent across periods

### Top Features (Expected)

Common across all models:
1. Recipient age at transplant
2. Donor age
3. Cold ischemia time
4. PRA (Panel Reactive Antibody)
5. Previous transplants
6. Diagnosis group
7. Ventilator support
8. ECMO support
9. Waiting time
10. Blood type mismatch

---

## ‚úÖ Validation Checklist

After completion, verify:

```bash
# 1. All files created
ls -1 graft-loss/feature_importance/outputs/*.csv | wc -l
# Expected: 11

# 2. Check summary statistics
head -20 graft-loss/feature_importance/outputs/summary_statistics_mc_cv.csv

# 3. Verify C-index values in range
# Open summary_statistics_mc_cv.csv and check:
#   - All cindex_mean values between 0.70 and 0.85
#   - All CI widths < 0.10
#   - All n_successful_splits > 800
```

---

## üîß Troubleshooting

### Issue: Not all cores being used

**Check:**
```bash
htop  # Should see ~30 cores at high usage
```

**Fix:**
```bash
export N_WORKERS=30
# Then restart script
```

### Issue: Slow progress

**Check timing:**
```bash
# Time first period
time Rscript -e "
n_mc_splits <- 25
source('graft-loss/feature_importance/replicate_20_features_MC_CV.R')
"
# Should complete in 2-3 minutes
# If slower, check CPU/memory/disk
```

### Issue: Out of memory

**Check:**
```bash
free -h  # Should have >900GB available
```

**Fix (unlikely with 1TB):**
```r
# Reduce workers if needed
n_workers <- 20  # Instead of 30
```

### Issue: Interrupted/crashed

**Resume:**
Results are saved after each period. Check which files exist:

```bash
ls graft-loss/feature_importance/outputs/*.csv
```

If missing files for specific period, can resume by editing script to skip completed periods.

---

## üìö Documentation

- **README_original_vs_updated_study.md** - Original vs updated study comparison
- **README_mc_cv_parallel_ec2.md** - MC-CV + parallelization + EC2 overview

- **README_notebook_guide.md** - Notebook usage guide

---

## üéØ Success Indicators

You'll know it's working correctly when you see:

1. **Startup:**
   ```
   Auto-detected 32 cores, using 30 workers
   Setting up parallel processing with 30 workers...
   Expected speedup: 24x faster than single core
   ```

2. **Progress:**
   ```
   Processing Original period (2010-2019)...
   Running RSF with MC-CV (1000 splits)...
   Progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100%
   ```

3. **Results:**
   ```
   Original period RSF:
     C-index: 0.745 (95% CI: 0.715 - 0.775)
     Successful splits: 987/1000
   ```

4. **Completion:**
   ```
   All periods complete!
   Output files: 11
   Total runtime: 42 minutes
   ```

---

## üí° Pro Tips

1. **Use tmux/screen** - So analysis continues if SSH disconnects
2. **Monitor in separate session** - Keep an eye on progress
3. **Save the log** - Use `tee` to save output: `... | tee replication.log`
4. **Benchmark first** - Run with 25 splits to verify setup (2-3 min)
5. **Check outputs early** - After first period completes, spot-check results

---

## üöÄ Ready to Go!

Everything is configured and ready. Just:

1. SSH into your EC2 instance
2. Run one of the commands above
3. Wait ~30-45 minutes
4. Check the 11 output files

**Good luck with your replication study!** üéâ

---

**Questions?** Check the documentation files listed above or review the inline comments in the scripts.

