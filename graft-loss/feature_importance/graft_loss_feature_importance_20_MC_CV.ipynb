{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graft Loss Feature Importance with Monte Carlo Cross-Validation\n",
    "\n",
    "**Study Replication:** Publication-quality replication with Monte Carlo cross-validation (configurable 100â€“1000 splits)  \n",
    "**Updated:** November 21, 2025  \n",
    "**Hardware:** Optimized for 32-core EC2 instance (1TB RAM)  \n",
    "**Validation:** Proper evaluation on unseen test data\n",
    "\n",
    "## Key Changes from Original\n",
    "\n",
    "âœ… **Monte Carlo Cross-Validation** â€“ up to 1000 random 75/25 train/test splits (100-split runs used for faster iteration)  \n",
    "âœ… **Stratified Sampling** - Maintains event distribution  \n",
    "âœ… **Parallel Processing** - Fast execution with furrr/future (â‰ˆ30 workers)  \n",
    "âœ… **95% Confidence Intervals** - Narrow, precise estimates (tighter with more splits)  \n",
    "âœ… **Realistic C-Indexes** - Expected range 0.70-0.85\n",
    "\n",
    "## Methodology\n",
    "\n",
    "This notebook implements the feature selection methodology from the original bcjaeger/graft-loss study:\n",
    "\n",
    "1. Load data for pediatric heart transplant outcomes\n",
    "2. Define three time periods:\n",
    "   - **Original**: 2010-2019 (matches publication)\n",
    "   - **Full**: 2010-2024 (all available data)\n",
    "   - **Full No COVID**: 2010-2024 excluding 2020-2023\n",
    "3. For each period and method:\n",
    "   - Create 100â€“1000 stratified train/test splits (75/25)\n",
    "   - Train model on training set\n",
    "   - Evaluate on unseen test set\n",
    "   - Aggregate results across splits\n",
    "4. Calculate C-index with 95% CI\n",
    "5. Extract top 20 features\n",
    "\n",
    "## Expected Runtime (per full 3-period Ã— 3-method run)\n",
    "\n",
    "- **100 splits (current default):**\n",
    "  - Local (4 cores): ~2â€“4 hours (NOT RECOMMENDED)\n",
    "  - Workstation (16 cores): ~1â€“2 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~1â€“2 hours âœ… **RECOMMENDED FOR DEVELOPMENT**\n",
    "- **1000 splits (extended / publication-level):**\n",
    "  - Local (4 cores): 8â€“12+ hours (NOT RECOMMENDED)\n",
    "  - Workstation (16 cores): ~8â€“16 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~10â€“20 hours âœ… **RECOMMENDED FOR FINAL RESULTS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load required packages and configure parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/latex": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/markdown": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/plain": [
       "[1] \"R version 4.4.3 (2025-02-28)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here() starts at /home/pgx3874/graft-loss\n",
      "\n",
      "\n",
      "Attaching package: â€˜dplyrâ€™\n",
      "\n",
      "\n",
      "The following objects are masked from â€˜package:statsâ€™:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from â€˜package:baseâ€™:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: â€˜janitorâ€™\n",
      "\n",
      "\n",
      "The following objects are masked from â€˜package:statsâ€™:\n",
      "\n",
      "    chisq.test, fisher.test\n",
      "\n",
      "\n",
      "riskRegression version 2025.09.17\n",
      "\n",
      "Loading required package: future\n",
      "\n",
      "\n",
      "Attaching package: â€˜futureâ€™\n",
      "\n",
      "\n",
      "The following object is masked from â€˜package:survivalâ€™:\n",
      "\n",
      "    cluster\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All packages loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Check R version\n",
    "R.version.string\n",
    "\n",
    "# Load required packages\n",
    "library(here)\n",
    "library(dplyr)\n",
    "library(readr)\n",
    "library(survival)\n",
    "library(ranger)\n",
    "library(aorsf)\n",
    "library(catboost)\n",
    "library(tidyr)\n",
    "library(purrr)\n",
    "library(tibble)\n",
    "library(janitor)\n",
    "library(haven)\n",
    "library(riskRegression)\n",
    "library(prodlim)\n",
    "library(rsample)    # For MC-CV\n",
    "library(furrr)      # For parallel processing\n",
    "library(future)     # For parallel backend\n",
    "library(progressr)  # For progress bars\n",
    "\n",
    "cat(\"âœ“ All packages loaded successfully\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected 32 cores, using 30 workers\n",
      "Setting up parallel processing with 30 workers...\n",
      "Expected speedup: 24x faster than single core\n",
      "Set future.globals.maxSize to 20 GB\n",
      "âœ“ Output directory cleaned\n",
      "Output directory: /home/pgx3874/graft-loss/feature_importance/outputs \n",
      "MC-CV Configuration: 100 splits, 75/25 train/test split\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEBUG/TEST MODE - Quick testing before full 100-split run\n",
    "# ============================================================\n",
    "# Set DEBUG_MODE = TRUE for quick testing (5 splits, ~2-5 min)\n",
    "# Set DEBUG_MODE = FALSE for full analysis (100 splits, ~1-2 hours on EC2)\n",
    "\n",
    "DEBUG_MODE <- FALSE  # Change to TRUE for quick test\n",
    "\n",
    "if (DEBUG_MODE) {\n",
    "  cat(\"\\n\")\n",
    "  cat(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\")\n",
    "  cat(\"â•‘                    ðŸ” DEBUG MODE ENABLED                       â•‘\\n\")\n",
    "  cat(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"Quick test configuration:\\n\")\n",
    "  cat(\"  â€¢ MC-CV Splits: 5 (instead of 100)\\n\")\n",
    "  cat(\"  â€¢ Period: Original only (2010-2019)\\n\")\n",
    "  cat(\"  â€¢ Trees: Reduced (RSF: 100, AORSF: 50)\\n\")\n",
    "  cat(\"  â€¢ Expected time: 2-5 minutes\\n\")\n",
    "  cat(\"  â€¢ Purpose: Verify everything works before full run\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"To run full analysis, set DEBUG_MODE = FALSE\\n\")\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "n_predictors <- 20                                        # Top 20 features\n",
    "n_trees_rsf <- if (DEBUG_MODE) 100 else 500              # RSF trees (reduced in debug)\n",
    "n_trees_aorsf <- if (DEBUG_MODE) 50 else 100             # AORSF trees (reduced in debug)\n",
    "horizon <- 1                                              # 1-year prediction\n",
    "n_mc_splits <- if (DEBUG_MODE) 5 else 100                # MC-CV splits (5 for debug, 100 for full)\n",
    "train_prop <- 0.75                                        # 75% training, 25% testing\n",
    "\n",
    "# Set up parallel processing - EC2 Optimized (32 cores, 1TB RAM)\n",
    "# Use 30 out of 32 cores (leave 2 for system)\n",
    "n_workers <- as.integer(Sys.getenv(\"N_WORKERS\", \"0\"))\n",
    "if (n_workers < 1) {\n",
    "  total_cores <- parallel::detectCores()\n",
    "  n_workers <- max(1, total_cores - 2)\n",
    "  cat(sprintf(\"Auto-detected %d cores, using %d workers\\n\", total_cores, n_workers))\n",
    "}\n",
    "\n",
    "cat(sprintf(\"Setting up parallel processing with %d workers...\\n\", n_workers))\n",
    "cat(sprintf(\"Expected speedup: %dx faster than single core\\n\", round(n_workers * 0.8)))\n",
    "\n",
    "# Increase future.globals.maxSize for large MC-CV splits object\n",
    "# With 1TB RAM on EC2, we can handle large transfers\n",
    "options(future.globals.maxSize = 20 * 1024^3)  # 20 GB limit (plenty for 100 splits)\n",
    "cat(\"Set future.globals.maxSize to 20 GB\\n\")\n",
    "\n",
    "plan(multisession, workers = n_workers)\n",
    "\n",
    "# Create output directory (clean first to ensure fresh start)\n",
    "output_dir <- here(\"feature_importance\", \"outputs\")\n",
    "\n",
    "# Clean existing outputs directory to ensure fresh/clean results\n",
    "if (dir.exists(output_dir)) {\n",
    "  # Remove all files in outputs directory\n",
    "  output_files <- list.files(output_dir, full.names = TRUE, recursive = TRUE, include.dirs = FALSE)\n",
    "  if (length(output_files) > 0) {\n",
    "    cat(sprintf(\"Cleaning %d existing output files...\\n\", length(output_files)))\n",
    "    file.remove(output_files)\n",
    "  }\n",
    "  # Remove empty subdirectories\n",
    "  output_dirs <- list.dirs(output_dir, recursive = TRUE, full.names = TRUE)\n",
    "  output_dirs <- output_dirs[output_dirs != output_dir]  # Don't remove main directory\n",
    "  for (dir in rev(output_dirs)) {  # Reverse order to remove nested dirs first\n",
    "    if (length(list.files(dir)) == 0) {\n",
    "      unlink(dir, recursive = TRUE)\n",
    "    }\n",
    "  }\n",
    "  cat(\"âœ“ Output directory cleaned\\n\")\n",
    "}\n",
    "\n",
    "# Create fresh output directory\n",
    "dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "\n",
    "cat(\"Output directory:\", output_dir, \"\\n\")\n",
    "cat(sprintf(\"MC-CV Configuration: %d splits, %.0f/%.0f train/test split\\n\", \n",
    "            n_mc_splits, train_prop * 100, (1 - train_prop) * 100))\n",
    "flush.console()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Define functions for data preparation, C-index calculation, and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ prepare_modeling_data() defined\n"
     ]
    }
   ],
   "source": [
    "# Prepare modeling data with leakage prevention\n",
    "prepare_modeling_data <- function(data) {\n",
    "  # Find time and status columns\n",
    "  time_col <- intersect(c(\"time\", \"outcome_int_graft_loss\", \"int_graft_loss\"), names(data))[1]\n",
    "  status_col <- intersect(c(\"status\", \"outcome_graft_loss\", \"graft_loss\"), names(data))[1]\n",
    "  \n",
    "  if (is.na(time_col) || is.na(status_col)) {\n",
    "    stop(\"Cannot find time/status columns\")\n",
    "  }\n",
    "  \n",
    "  # Rename to standard names\n",
    "  if (time_col != \"time\") data <- data %>% rename(time = !!time_col)\n",
    "  if (status_col != \"status\") data <- data %>% rename(status = !!status_col)\n",
    "  \n",
    "  # Exclude leakage variables and identifier columns\n",
    "  exclude_exact <- c(\n",
    "    \"ID\", \"ptid_e\", \"int_dead\", \"int_death\", \"graft_loss\", \"txgloss\", \"death\", \"event\",\n",
    "    \"dpricaus\", \"deathspc\", \"concod\", \"age_death\", \"dlist\", \"txpl_year\",\n",
    "    \"rrace_b\", \"rrace_a\", \"rrace_ai\", \"rrace_pi\", \"rrace_o\", \"rrace_un\", \"race\",\n",
    "    \"patsupp\", \"pmorexam\", \"papooth\", \"pacuref\", \"pishltgr\",\n",
    "    \"pathero\", \"pcadrec\", \"pcadrem\", \"pdiffib\", \"cpathneg\",\n",
    "    \"dcardiac\", \"dneuro\", \"dreject\", \"dsecaccs\", \"dpriaccs\",\n",
    "    \"dconmbld\", \"dconmal\", \"dconcard\", \"dconneur\", \"dconrej\",\n",
    "    \"dmajbld\", \"dmalcanc\"\n",
    "  )\n",
    "  \n",
    "  exclude_prefixes <- c(\"dtx_\", \"cc_\", \"dcon\", \"dpri\", \"dsec\", \"dmaj\", \"sd\")\n",
    "  \n",
    "  exclude_by_prefix <- character(0)\n",
    "  for (prefix in exclude_prefixes) {\n",
    "    exclude_by_prefix <- c(exclude_by_prefix, \n",
    "                           names(data)[startsWith(names(data), prefix)])\n",
    "  }\n",
    "  \n",
    "  exclude_all <- unique(c(exclude_exact, exclude_by_prefix))\n",
    "  data <- data %>% select(-any_of(exclude_all))\n",
    "  \n",
    "  # Median imputation for numeric variables\n",
    "  numeric_vars <- names(data)[sapply(data, is.numeric) & names(data) != \"time\" & names(data) != \"status\"]\n",
    "  for (var in numeric_vars) {\n",
    "    if (any(is.na(data[[var]]))) {\n",
    "      median_val <- median(data[[var]], na.rm = TRUE)\n",
    "      data[[var]][is.na(data[[var]])] <- median_val\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Mode imputation for categorical variables\n",
    "  categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]\n",
    "  for (var in categorical_vars) {\n",
    "    if (any(is.na(data[[var]]))) {\n",
    "      mode_val <- names(sort(table(data[[var]]), decreasing = TRUE))[1]\n",
    "      data[[var]][is.na(data[[var]])] <- mode_val\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Remove constant columns\n",
    "  constant_cols <- names(data)[sapply(data, function(x) length(unique(na.omit(x))) <= 1)]\n",
    "  if (length(constant_cols) > 0) {\n",
    "    data <- data %>% select(-any_of(constant_cols))\n",
    "  }\n",
    "  \n",
    "  # Convert character to factor\n",
    "  data <- data %>% mutate(across(where(is.character), as.factor))\n",
    "  \n",
    "  return(data)\n",
    "}\n",
    "\n",
    "cat(\"âœ“ prepare_modeling_data() defined\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ calculate_cindex() and ranger_predictrisk() defined\n"
     ]
    }
   ],
   "source": [
    "# C-index calculation\n",
    "calculate_cindex <- function(time, status, risk_scores, horizon = NULL) {\n",
    "  valid_idx <- !is.na(time) & !is.na(status) & !is.na(risk_scores) &\n",
    "               is.finite(time) & is.finite(risk_scores) & time > 0\n",
    "  \n",
    "  time   <- as.numeric(time[valid_idx])\n",
    "  status <- as.numeric(status[valid_idx])\n",
    "  risk   <- as.numeric(risk_scores[valid_idx])\n",
    "  \n",
    "  n <- length(time)\n",
    "  events <- sum(status == 1)\n",
    "  \n",
    "  if (n < 10 || events < 1 || length(unique(risk)) == 1) {\n",
    "    return(list(cindex_td = NA_real_, cindex_ti = NA_real_))\n",
    "  }\n",
    "  \n",
    "  # Time-independent C-index (Harrell's)\n",
    "  num_conc_ti <- 0\n",
    "  num_disc_ti <- 0\n",
    "  num_ties_ti <- 0\n",
    "  \n",
    "  for (i in seq_len(n)) {\n",
    "    if (status[i] != 1) next\n",
    "    for (j in seq_len(n)) {\n",
    "      if (i == j) next\n",
    "      if (time[i] < time[j]) {\n",
    "        if (risk[i] > risk[j]) {\n",
    "          num_conc_ti <- num_conc_ti + 1\n",
    "        } else if (risk[i] < risk[j]) {\n",
    "          num_disc_ti <- num_disc_ti + 1\n",
    "        } else {\n",
    "          num_ties_ti <- num_ties_ti + 1\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  denom_ti <- num_conc_ti + num_disc_ti + num_ties_ti\n",
    "  cindex_ti <- if (denom_ti > 0) (num_conc_ti + 0.5 * num_ties_ti) / denom_ti else NA_real_\n",
    "  \n",
    "  # Time-dependent C-index\n",
    "  cindex_td <- tryCatch({\n",
    "    score_data <- data.frame(time = time, status = status)\n",
    "    pred_matrix <- matrix(risk, ncol = 1)\n",
    "    \n",
    "    evaluation <- riskRegression::Score(\n",
    "      object = list(Model = pred_matrix),\n",
    "      formula = Surv(time, status) ~ 1,\n",
    "      data = score_data,\n",
    "      times = if (!is.null(horizon)) horizon else median(time[status == 1]),\n",
    "      summary = \"risks\",\n",
    "      metrics = \"auc\",\n",
    "      se.fit = FALSE\n",
    "    )\n",
    "    \n",
    "    as.numeric(evaluation$AUC$score$AUC[1])\n",
    "  }, error = function(e) cindex_ti)\n",
    "  \n",
    "  return(list(cindex_td = cindex_td, cindex_ti = cindex_ti))\n",
    "}\n",
    "\n",
    "# ranger_predictrisk function\n",
    "ranger_predictrisk <- function(object, newdata, times) {\n",
    "  preds <- predict(object, data = newdata, type = \"response\")\n",
    "  if (is.null(preds$survival)) {\n",
    "    stop(\"ranger prediction did not return survival probabilities\")\n",
    "  }\n",
    "  \n",
    "  surv_matrix <- preds$survival\n",
    "  time_points <- preds$unique.death.times\n",
    "  closest_idx <- which.min(abs(time_points - times))\n",
    "  risk_scores <- 1 - surv_matrix[, closest_idx]\n",
    "  \n",
    "  return(as.numeric(risk_scores))\n",
    "}\n",
    "\n",
    "cat(\"âœ“ calculate_cindex() and ranger_predictrisk() defined\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Cross-Validation Function\n",
    "\n",
    "Main function that runs MC-CV for a single method and period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ run_mc_cv_method() defined\n"
     ]
    }
   ],
   "source": [
    "# Run MC-CV for a single method and time period\n",
    "run_mc_cv_method <- function(data, method, period_name, mc_splits) {\n",
    "  \n",
    "  cat(sprintf(\"\\n=== Running MC-CV for %s (%s) ===\\n\", method, period_name))\n",
    "  cat(sprintf(\"Splits: %d | Train: %.0f%% | Test: %.0f%%\\n\", \n",
    "              n_mc_splits, train_prop * 100, (1 - train_prop) * 100))\n",
    "  flush.console()\n",
    "  \n",
    "  split_ids <- seq_len(n_mc_splits)\n",
    "  \n",
    "  # Run splits in parallel with progress bar\n",
    "  with_progress({\n",
    "    p <- progressor(steps = n_mc_splits)\n",
    "    \n",
    "    results <- future_map(split_ids, function(split_id) {\n",
    "      p()  # Update progress\n",
    "      \n",
    "      # Get train/test data from split\n",
    "      split <- mc_splits$splits[[split_id]]\n",
    "      train_data <- rsample::analysis(split)\n",
    "      test_data <- rsample::assessment(split)\n",
    "      \n",
    "      # Train and evaluate\n",
    "      model <- NULL\n",
    "      predictions <- NULL\n",
    "      feature_importance <- NULL\n",
    "      \n",
    "      tryCatch({\n",
    "        if (method == \"RSF\") {\n",
    "          model <- ranger(\n",
    "            Surv(time, status) ~ .,\n",
    "            data = train_data,\n",
    "            num.trees = n_trees_rsf,\n",
    "            importance = \"permutation\",\n",
    "            min.node.size = 20,\n",
    "            splitrule = \"extratrees\",\n",
    "            num.random.splits = 10\n",
    "          )\n",
    "          \n",
    "          predictions <- ranger_predictrisk(model, test_data, horizon)\n",
    "          feature_importance <- model$variable.importance\n",
    "          \n",
    "        } else if (method == \"AORSF\") {\n",
    "          # Remove constant columns from training data (can occur after train/test split)\n",
    "          constant_cols <- names(train_data)[sapply(train_data, function(x) {\n",
    "            if (is.numeric(x)) {\n",
    "              length(unique(na.omit(x))) <= 1\n",
    "            } else {\n",
    "              length(unique(na.omit(x))) <= 1\n",
    "            }\n",
    "          })]\n",
    "          constant_cols <- setdiff(constant_cols, c(\"time\", \"status\"))\n",
    "          if (length(constant_cols) > 0) {\n",
    "            train_data <- train_data %>% select(-any_of(constant_cols))\n",
    "            test_data <- test_data %>% select(-any_of(constant_cols))\n",
    "          }\n",
    "          \n",
    "          model <- aorsf::orsf(\n",
    "            data = train_data,\n",
    "            formula = Surv(time, status) ~ .,\n",
    "            n_tree = n_trees_aorsf,\n",
    "            na_action = 'impute_meanmode'\n",
    "          )\n",
    "          \n",
    "          pred_obj <- predict(model, new_data = test_data, \n",
    "                              pred_type = 'risk', pred_horizon = horizon)\n",
    "          predictions <- if (is.matrix(pred_obj)) as.numeric(pred_obj[, 1]) else as.numeric(pred_obj)\n",
    "          feature_importance <- aorsf::orsf_vi_permute(model)\n",
    "          \n",
    "        } else if (method == \"CatBoost\") {\n",
    "          # Remove constant columns from training data (can occur after train/test split)\n",
    "          constant_cols <- names(train_data)[sapply(train_data, function(x) {\n",
    "            if (is.numeric(x)) {\n",
    "              length(unique(na.omit(x))) <= 1\n",
    "            } else {\n",
    "              length(unique(na.omit(x))) <= 1\n",
    "            }\n",
    "          })]\n",
    "          constant_cols <- setdiff(constant_cols, c(\"time\", \"status\"))\n",
    "          if (length(constant_cols) > 0) {\n",
    "            train_data <- train_data %>% select(-any_of(constant_cols))\n",
    "            test_data <- test_data %>% select(-any_of(constant_cols))\n",
    "          }\n",
    "          \n",
    "          # CatBoost Cox requires signed time labels:\n",
    "          # - Positive time for events (status = 1)\n",
    "          # - Negative time for censored (status = 0)\n",
    "          # Also handle non-positive times (set to epsilon)\n",
    "          eps <- .Machine$double.eps\n",
    "          train_time <- suppressWarnings(as.numeric(train_data$time))\n",
    "          test_time <- suppressWarnings(as.numeric(test_data$time))\n",
    "          train_time[!is.finite(train_time) | train_time <= 0] <- eps\n",
    "          test_time[!is.finite(test_time) | test_time <= 0] <- eps\n",
    "          \n",
    "          train_status <- as.integer(train_data$status)\n",
    "          test_status <- as.integer(test_data$status)\n",
    "          \n",
    "          # Create signed time labels for training\n",
    "          train_labels <- ifelse(train_status == 1L, train_time, -train_time)\n",
    "          \n",
    "          # Prepare feature data (exclude time and status)\n",
    "          train_features <- train_data %>% select(-time, -status)\n",
    "          test_features <- test_data %>% select(-time, -status)\n",
    "          \n",
    "          # CatBoost R automatically detects factors as categorical variables\n",
    "          # Just ensure factor levels are synchronized between train and test\n",
    "          for (col in names(train_features)) {\n",
    "            if (is.factor(train_features[[col]])) {\n",
    "              train_levels <- levels(train_features[[col]])\n",
    "              test_features[[col]] <- factor(test_features[[col]], levels = train_levels)\n",
    "            }\n",
    "          }\n",
    "          \n",
    "          train_pool <- catboost.load_pool(\n",
    "            data = train_features,\n",
    "            label = train_labels\n",
    "          )\n",
    "          \n",
    "          # Test pool doesn't need labels for prediction, but we'll create signed labels for consistency\n",
    "          test_labels <- ifelse(test_status == 1L, test_time, -test_time)\n",
    "          test_pool <- catboost.load_pool(\n",
    "            data = test_features,\n",
    "            label = test_labels\n",
    "          )\n",
    "          \n",
    "          # CatBoost configuration: single-threaded inside each R worker to avoid\n",
    "          # logger/thread-safety issues, and quiet logging in parallel runs.\n",
    "          params <- list(\n",
    "            loss_function  = 'Cox',\n",
    "            iterations     = 100,\n",
    "            learning_rate  = 0.1,\n",
    "            depth          = 6,\n",
    "            thread_count   = 1,\n",
    "            logging_level  = 'Silent',\n",
    "            verbose        = 0L\n",
    "          )\n",
    "          \n",
    "          model <- catboost.train(train_pool, params = params)\n",
    "          \n",
    "          # CatBoost Cox returns negative values for higher risk\n",
    "          # Invert sign so higher values = higher risk (consistent with other methods)\n",
    "          preds <- catboost.predict(model, test_pool)\n",
    "          predictions <- -1 * as.numeric(preds)\n",
    "          \n",
    "          # Get feature importance - CatBoost returns a matrix with rownames as feature names\n",
    "          # IMPORTANT: catboost.get_feature_importance() returns a matrix (not a named vector)\n",
    "          # - Values are in the first column: importance_matrix[, 1]\n",
    "          # - Feature names are in rownames: rownames(importance_matrix)\n",
    "          # Convert to named vector for consistency with RSF and AORSF (which return named vectors directly)\n",
    "          importance_matrix <- catboost.get_feature_importance(model)\n",
    "          feature_importance <- as.numeric(importance_matrix[, 1])\n",
    "          names(feature_importance) <- rownames(importance_matrix)\n",
    "        }\n",
    "        \n",
    "        # Calculate C-index on TEST data\n",
    "        cindex_result <- calculate_cindex(\n",
    "          time = test_data$time,\n",
    "          status = test_data$status,\n",
    "          risk_scores = predictions,\n",
    "          horizon = horizon\n",
    "        )\n",
    "        \n",
    "        return(list(\n",
    "          split_id = split_id,\n",
    "          cindex_td = cindex_result$cindex_td,\n",
    "          cindex_ti = cindex_result$cindex_ti,\n",
    "          feature_importance = feature_importance,\n",
    "          n_train = nrow(train_data),\n",
    "          n_test = nrow(test_data),\n",
    "          success = TRUE\n",
    "        ))\n",
    "        \n",
    "      }, error = function(e) {\n",
    "        return(list(\n",
    "          split_id = split_id,\n",
    "          cindex_td = NA_real_,\n",
    "          cindex_ti = NA_real_,\n",
    "          feature_importance = NULL,\n",
    "          n_train = nrow(train_data),\n",
    "          n_test = nrow(test_data),\n",
    "          success = FALSE,\n",
    "          error = e$message\n",
    "        ))\n",
    "      })\n",
    "    }, .options = furrr_options(\n",
    "      seed = TRUE,\n",
    "      packages = c(\n",
    "        \"dplyr\", \"purrr\", \"tibble\", \"rsample\",\n",
    "        \"ranger\", \"aorsf\", \"catboost\",\n",
    "        \"riskRegression\", \"prodlim\"\n",
    "      )\n",
    "    ))\n",
    "  })\n",
    "  \n",
    "  # Aggregate results\n",
    "  successful_splits <- Filter(function(x) x$success, results)\n",
    "  n_successful <- length(successful_splits)\n",
    "  \n",
    "  cat(sprintf(\"Successful splits: %d / %d\\n\", n_successful, n_mc_splits))\n",
    "  flush.console()\n",
    "  \n",
    "  # If all splits failed (e.g., CatBoost configuration/data issue),\n",
    "  # do not stop the whole analysis. Instead, log a warning and return\n",
    "  # an empty/NA summary so that other methods and periods can continue.\n",
    "  if (n_successful == 0) {\n",
    "    error_splits <- Filter(function(x) !is.null(x$error), results)\n",
    "    if (length(error_splits) > 0) {\n",
    "      first_error <- error_splits[[1]]$error\n",
    "      cat(sprintf(\"WARNING: example error for %s (%s): %s\\n\",\n",
    "                  method, period_name, first_error))\n",
    "    }\n",
    "    cat(sprintf(\"WARNING: All MC-CV splits failed for %s (%s). Skipping this method.\\n\",\n",
    "                method, period_name))\n",
    "    flush.console()\n",
    "    \n",
    "    return(list(\n",
    "      method = method,\n",
    "      period = period_name,\n",
    "      n_splits = n_mc_splits,\n",
    "      n_successful = 0,\n",
    "      cindex_td_mean = NA_real_,\n",
    "      cindex_td_sd = NA_real_,\n",
    "      cindex_td_ci_lower = NA_real_,\n",
    "      cindex_td_ci_upper = NA_real_,\n",
    "      cindex_ti_mean = NA_real_,\n",
    "      cindex_ti_sd = NA_real_,\n",
    "      cindex_ti_ci_lower = NA_real_,\n",
    "      cindex_ti_ci_upper = NA_real_,\n",
    "      top_features = numeric(0)\n",
    "    ))\n",
    "  }\n",
    "  \n",
    "  # Extract C-indexes\n",
    "  cindex_td_values <- sapply(successful_splits, function(x) x$cindex_td)\n",
    "  cindex_ti_values <- sapply(successful_splits, function(x) x$cindex_ti)\n",
    "  \n",
    "  cindex_td_values <- cindex_td_values[!is.na(cindex_td_values)]\n",
    "  cindex_ti_values <- cindex_ti_values[!is.na(cindex_ti_values)]\n",
    "  \n",
    "  # Aggregate feature importance\n",
    "  # All methods (RSF, AORSF, CatBoost) return named numeric vectors\n",
    "  all_feature_names <- unique(unlist(lapply(successful_splits, function(x) {\n",
    "    if (is.null(x$feature_importance)) return(NULL)\n",
    "    names(x$feature_importance)\n",
    "  })))\n",
    "  \n",
    "  aggregated_importance <- sapply(all_feature_names, function(feature) {\n",
    "    importances <- sapply(successful_splits, function(x) {\n",
    "      if (is.null(x$feature_importance)) return(NA_real_)\n",
    "      if (feature %in% names(x$feature_importance)) {\n",
    "        return(as.numeric(x$feature_importance[feature]))\n",
    "      }\n",
    "      return(NA_real_)\n",
    "    })\n",
    "    mean(importances, na.rm = TRUE)\n",
    "  })\n",
    "  \n",
    "  # Ensure aggregated_importance is a numeric vector\n",
    "  aggregated_importance <- as.numeric(aggregated_importance)\n",
    "  names(aggregated_importance) <- all_feature_names\n",
    "  \n",
    "  top_features <- sort(aggregated_importance, decreasing = TRUE)[1:min(n_predictors, length(aggregated_importance))]\n",
    "  \n",
    "  # Calculate statistics\n",
    "  results_summary <- list(\n",
    "    method = method,\n",
    "    period = period_name,\n",
    "    n_splits = n_mc_splits,\n",
    "    n_successful = n_successful,\n",
    "    cindex_td_mean = mean(cindex_td_values, na.rm = TRUE),\n",
    "    cindex_td_sd = sd(cindex_td_values, na.rm = TRUE),\n",
    "    cindex_td_ci_lower = quantile(cindex_td_values, 0.025, na.rm = TRUE),\n",
    "    cindex_td_ci_upper = quantile(cindex_td_values, 0.975, na.rm = TRUE),\n",
    "    cindex_ti_mean = mean(cindex_ti_values, na.rm = TRUE),\n",
    "    cindex_ti_sd = sd(cindex_ti_values, na.rm = TRUE),\n",
    "    cindex_ti_ci_lower = quantile(cindex_ti_values, 0.025, na.rm = TRUE),\n",
    "    cindex_ti_ci_upper = quantile(cindex_ti_values, 0.975, na.rm = TRUE),\n",
    "    top_features = top_features\n",
    "  )\n",
    "  \n",
    "  # Print summary\n",
    "  cat(sprintf(\"\\n--- Results for %s (%s) ---\\n\", method, period_name))\n",
    "  cat(sprintf(\"Time-Dependent C-Index: %.4f Â± %.4f (95%% CI: %.4f - %.4f)\\n\",\n",
    "              results_summary$cindex_td_mean,\n",
    "              results_summary$cindex_td_sd,\n",
    "              results_summary$cindex_td_ci_lower,\n",
    "              results_summary$cindex_td_ci_upper))\n",
    "  cat(sprintf(\"Time-Independent C-Index: %.4f Â± %.4f (95%% CI: %.4f - %.4f)\\n\",\n",
    "              results_summary$cindex_ti_mean,\n",
    "              results_summary$cindex_ti_sd,\n",
    "              results_summary$cindex_ti_ci_lower,\n",
    "              results_summary$cindex_ti_ci_upper))\n",
    "  # Display top 10 features sorted alphabetically for easier comparison\n",
    "  top10_features <- names(top_features)[1:min(10, length(top_features))]\n",
    "  top10_features_sorted <- sort(top10_features)\n",
    "  cat(sprintf(\"Top 10 features (alphabetical): %s\\n\", paste(top10_features_sorted, collapse = \", \")))\n",
    "  flush.console()\n",
    "  \n",
    "  return(results_summary)\n",
    "}\n",
    "\n",
    "cat(\"âœ“ run_mc_cv_method() defined\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/pgx3874/graft-loss/data/phts_txpl_ml.sas7bdat \n",
      "âœ“ Loaded data: 5835 rows, 478 columns\n"
     ]
    }
   ],
   "source": [
    "# Find and load SAS file\n",
    "sas_path_local <- here(\"data\", \"phts_txpl_ml.sas7bdat\")\n",
    "sas_path_external <- here(\"graft-loss-parallel-processing\", \"data\", \"phts_txpl_ml.sas7bdat\")\n",
    "sas_path_graft_loss <- here(\"graft-loss\", \"data\", \"phts_txpl_ml.sas7bdat\")\n",
    "\n",
    "sas_path <- NULL\n",
    "for (path in c(sas_path_local, sas_path_external, sas_path_graft_loss)) {\n",
    "  if (file.exists(path)) {\n",
    "    sas_path <- path\n",
    "    break\n",
    "  }\n",
    "}\n",
    "\n",
    "if (is.null(sas_path)) {\n",
    "  stop(\"Cannot find phts_txpl_ml.sas7bdat in any location\")\n",
    "}\n",
    "\n",
    "cat(\"Loading data from:\", sas_path, \"\\n\")\n",
    "\n",
    "# Load data\n",
    "phts_base <- haven::read_sas(sas_path) %>%\n",
    "  filter(TXPL_YEAR >= 2010) %>%\n",
    "  janitor::clean_names() %>%\n",
    "  rename(\n",
    "    outcome_int_graft_loss = int_graft_loss,\n",
    "    outcome_graft_loss = graft_loss\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    ID = 1:n(),\n",
    "    across(.cols = where(is.character), ~ ifelse(.x %in% c(\"\", \"unknown\", \"missing\"), NA_character_, .x)),\n",
    "    across(.cols = where(is.character), as.factor),\n",
    "    tx_mcsd = if ('txnomcsd' %in% names(.)) {\n",
    "      if_else(txnomcsd == 'yes', 0, 1)\n",
    "    } else if ('txmcsd' %in% names(.)) {\n",
    "      txmcsd\n",
    "    } else {\n",
    "      NA_real_\n",
    "    }\n",
    "  )\n",
    "\n",
    "cat(sprintf(\"âœ“ Loaded data: %d rows, %d columns\\n\", nrow(phts_base), ncol(phts_base)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove CPBYPASS and Add Dichotomous DONISCH\n",
    "\n",
    "**Variable Processing:**\n",
    "\n",
    "1. **CPBYPASS (Cardiopulmonary Bypass Time):**\n",
    "   - Calculate summary statistics (median, IQR, non-missing counts)\n",
    "   - **Remove from dataset** - CPBYPASS is excluded from all modeling\n",
    "\n",
    "2. **DONISCH (Donor Ischemic Time):**\n",
    "   - Convert from continuous variable (minutes) to **dichotomous variable**\n",
    "   - **New dichotomous DONISCH:**\n",
    "     - `donisch = 1` if donor ischemic time > 4 hours (>240 minutes)\n",
    "     - `donisch = 0` if donor ischemic time â‰¤ 4 hours (â‰¤240 minutes)\n",
    "   - Variable name remains `donisch` (now binary instead of continuous)\n",
    "   - This transformation is applied before defining time periods and running analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Variables: CPBYPASS and DONISCH ===\n",
      "\n",
      "=== CPBYPASS Statistics (Overall) ===\n",
      "Median: 168.00 minutes (2.80 hours)\n",
      "IQR: 127.00 - 222.00 minutes (2.12 - 3.70 hours)\n",
      "IQR Range: 95.00 minutes (1.58 hours)\n",
      "N (non-missing): 5633 (96.5%)\n",
      "========================================\n",
      "\n",
      "=== DONISCH Transformation ===\n",
      "Original DONISCH (minutes) statistics:\n",
      "  Median: 216.00 minutes (3.60 hours)\n",
      "  IQR: 181.00 - 253.00 minutes (3.02 - 4.22 hours)\n",
      "  N (non-missing): 5770 (98.9%)\n",
      "\n",
      "Dichotomous DONISCH (>4 hours = 1, â‰¤4 hours = 0):\n",
      "  Total N: 5770\n",
      "  >4 hours (>240 min): 1827 (31.7%)\n",
      "  â‰¤4 hours (â‰¤240 min): 3943 (68.3%)\n",
      "========================================\n",
      "\n",
      "âœ“ CPBYPASS removed from dataset\n",
      "âœ“ DONISCH present as dichotomous variable (values: 0, 1)\n",
      "\n",
      "âœ“ Variable processing complete!\n",
      "\n",
      "=== Redefining time periods with updated variables ===\n",
      "Period: original | N: 4036 | Events: 768 (19.03%)\n",
      "Period: full | N: 5835 | Events: 939 (16.09%)\n",
      "Period: full_no_covid | N: 4196 | Events: 774 (18.45%)\n",
      "\n",
      "âœ“ Time periods redefined with updated variables\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Remove CPBYPASS and Add Dichotomous DONISCH\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"\\n=== Processing Variables: CPBYPASS and DONISCH ===\\n\\n\")\n",
    "\n",
    "# Calculate CPBYPASS statistics before removing it\n",
    "if (\"cpbypass\" %in% names(phts_base)) {\n",
    "  cpbypass_data <- phts_base$cpbypass[!is.na(phts_base$cpbypass)]\n",
    "  if (length(cpbypass_data) > 0) {\n",
    "    cpbypass_median <- median(cpbypass_data)\n",
    "    cpbypass_q1 <- quantile(cpbypass_data, 0.25, na.rm = TRUE)\n",
    "    cpbypass_q3 <- quantile(cpbypass_data, 0.75, na.rm = TRUE)\n",
    "    cpbypass_iqr <- cpbypass_q3 - cpbypass_q1\n",
    "    \n",
    "    cat(\"=== CPBYPASS Statistics (Overall) ===\\n\")\n",
    "    cat(sprintf(\"Median: %.2f minutes (%.2f hours)\\n\", cpbypass_median, cpbypass_median / 60))\n",
    "    cat(sprintf(\"IQR: %.2f - %.2f minutes (%.2f - %.2f hours)\\n\", \n",
    "                cpbypass_q1, cpbypass_q3, cpbypass_q1 / 60, cpbypass_q3 / 60))\n",
    "    cat(sprintf(\"IQR Range: %.2f minutes (%.2f hours)\\n\", cpbypass_iqr, cpbypass_iqr / 60))\n",
    "    cat(sprintf(\"N (non-missing): %d (%.1f%%)\\n\", length(cpbypass_data), \n",
    "                100 * length(cpbypass_data) / nrow(phts_base)))\n",
    "    cat(\"========================================\\n\\n\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Create DONISCH dichotomous variable (>4 hours = 1, â‰¤4 hours = 0)\n",
    "# DONISCH is in minutes, so 4 hours = 240 minutes\n",
    "if (\"donisch\" %in% names(phts_base)) {\n",
    "  # Store original for summary statistics\n",
    "  donisch_original <- phts_base$donisch\n",
    "  \n",
    "  # Create dichotomous variable (>240 minutes = 1, â‰¤240 minutes = 0)\n",
    "  phts_base <- phts_base %>%\n",
    "    mutate(\n",
    "      donisch = if_else(\n",
    "        is.na(donisch_original), \n",
    "        NA_real_,\n",
    "        if_else(donisch_original > 240, 1, 0)  # >4 hours = 1, â‰¤4 hours = 0\n",
    "      )\n",
    "    )\n",
    "  \n",
    "  # Print DONISCH transformation summary\n",
    "  donisch_summary <- phts_base %>%\n",
    "    filter(!is.na(donisch)) %>%\n",
    "    summarise(\n",
    "      n_total = n(),\n",
    "      n_gt_4hr = sum(donisch == 1),\n",
    "      n_le_4hr = sum(donisch == 0),\n",
    "      pct_gt_4hr = 100 * mean(donisch == 1)\n",
    "    )\n",
    "  \n",
    "  # Also print original DONISCH statistics for reference\n",
    "  donisch_orig_stats <- donisch_original[!is.na(donisch_original)]\n",
    "  if (length(donisch_orig_stats) > 0) {\n",
    "    donisch_median <- median(donisch_orig_stats)\n",
    "    donisch_q1 <- quantile(donisch_orig_stats, 0.25, na.rm = TRUE)\n",
    "    donisch_q3 <- quantile(donisch_orig_stats, 0.75, na.rm = TRUE)\n",
    "    \n",
    "    cat(\"=== DONISCH Transformation ===\\n\")\n",
    "    cat(\"Original DONISCH (minutes) statistics:\\n\")\n",
    "    cat(sprintf(\"  Median: %.2f minutes (%.2f hours)\\n\", donisch_median, donisch_median / 60))\n",
    "    cat(sprintf(\"  IQR: %.2f - %.2f minutes (%.2f - %.2f hours)\\n\", \n",
    "                donisch_q1, donisch_q3, donisch_q1 / 60, donisch_q3 / 60))\n",
    "    cat(sprintf(\"  N (non-missing): %d (%.1f%%)\\n\", length(donisch_orig_stats),\n",
    "                100 * length(donisch_orig_stats) / nrow(phts_base)))\n",
    "    cat(\"\\nDichotomous DONISCH (>4 hours = 1, â‰¤4 hours = 0):\\n\")\n",
    "    cat(sprintf(\"  Total N: %d\\n\", donisch_summary$n_total))\n",
    "    cat(sprintf(\"  >4 hours (>240 min): %d (%.1f%%)\\n\", \n",
    "                donisch_summary$n_gt_4hr, donisch_summary$pct_gt_4hr))\n",
    "    cat(sprintf(\"  â‰¤4 hours (â‰¤240 min): %d (%.1f%%)\\n\", \n",
    "                donisch_summary$n_le_4hr, 100 - donisch_summary$pct_gt_4hr))\n",
    "    cat(\"========================================\\n\\n\")\n",
    "  }\n",
    "} else {\n",
    "  warning(\"DONISCH variable not found in dataset!\")\n",
    "}\n",
    "\n",
    "# Remove CPBYPASS from the base dataset\n",
    "if (\"cpbypass\" %in% names(phts_base)) {\n",
    "  phts_base <- phts_base %>% select(-cpbypass)\n",
    "  cat(\"âœ“ CPBYPASS removed from dataset\\n\")\n",
    "} else {\n",
    "  cat(\"âš  CPBYPASS not found in dataset (may have already been removed)\\n\")\n",
    "}\n",
    "\n",
    "# Verify DONISCH is present as dichotomous\n",
    "if (\"donisch\" %in% names(phts_base)) {\n",
    "  unique_vals <- unique(phts_base$donisch[!is.na(phts_base$donisch)])\n",
    "  cat(sprintf(\"âœ“ DONISCH present as dichotomous variable (values: %s)\\n\", \n",
    "              paste(sort(unique_vals), collapse = \", \")))\n",
    "} else {\n",
    "  warning(\"DONISCH not found in dataset after transformation!\")\n",
    "}\n",
    "\n",
    "cat(\"\\nâœ“ Variable processing complete!\\n\")\n",
    "\n",
    "# Redefine time periods with updated phts_base (CPBYPASS removed, DONISCH dichotomous)\n",
    "cat(\"\\n=== Redefining time periods with updated variables ===\\n\")\n",
    "periods <- list()\n",
    "periods$original <- phts_base %>% filter(txpl_year >= 2010 & txpl_year <= 2019)\n",
    "periods$full <- phts_base %>% filter(txpl_year >= 2010)\n",
    "periods$full_no_covid <- phts_base %>% filter(txpl_year >= 2010 & !(txpl_year >= 2020 & txpl_year <= 2023))\n",
    "\n",
    "# Print summary\n",
    "for (period_name in names(periods)) {\n",
    "  period_data <- periods[[period_name]]\n",
    "  n_events <- sum(period_data$outcome_graft_loss, na.rm = TRUE)\n",
    "  event_rate <- 100 * n_events / nrow(period_data)\n",
    "  \n",
    "  cat(sprintf(\"Period: %s | N: %d | Events: %d (%.2f%%)\\n\", \n",
    "              period_name, nrow(period_data), n_events, event_rate))\n",
    "}\n",
    "\n",
    "cat(\"\\nâœ“ Time periods redefined with updated variables\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Analysis\n",
    "\n",
    "Execute MC-CV for all methods and periods.\n",
    "\n",
    "- **100 splits (default full run):** Typically ~1â€“2 hours on a 32-core EC2 instance, longer on smaller machines.\n",
    "- **1000 splits (extended run):** Linearly more expensive; expect roughly 8â€“10Ã— the 100-split time.\n",
    "\n",
    "**Note:** You can run each period separately by uncommenting only one `period_name` at a time, or reduce the number of methods/periods to shorten runtime further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing Period: original\n",
      "========================================\n",
      "Period data: 4036 rows, 380 columns\n",
      "Events: 768 (19.03%)\n",
      "Creating 100 MC-CV splits (stratified)...\n",
      "\n",
      "=== Running MC-CV for RSF (original) ===\n",
      "Splits: 100 | Train: 75% | Test: 25%\n"
     ]
    }
   ],
   "source": [
    "# Select periods and methods to run\n",
    "# In DEBUG_MODE, only run original period for speed\n",
    "period_names <- if (DEBUG_MODE) {\n",
    "  c(\"original\")  # Debug: just one period (~2-5 min)\n",
    "} else {\n",
    "  c(\"original\", \"full\", \"full_no_covid\")  # Full: all periods (~30-45 min)\n",
    "}\n",
    "\n",
    "# Methods to run. Include CatBoost (with single-threaded, quiet config)\n",
    "# alongside RSF and AORSF.\n",
    "method_names <- c(\"RSF\", \"CatBoost\", \"AORSF\")\n",
    "\n",
    "# Store all results\n",
    "all_results <- list()\n",
    "\n",
    "# Run analysis\n",
    "for (period_name in period_names) {\n",
    "  cat(sprintf(\"\\n========================================\\n\"))\n",
    "  cat(sprintf(\"Processing Period: %s\\n\", period_name))\n",
    "  cat(sprintf(\"========================================\\n\"))\n",
    "  flush.console()\n",
    "  \n",
    "  # Prepare data\n",
    "  period_data <- prepare_modeling_data(periods[[period_name]])\n",
    "  \n",
    "  cat(sprintf(\"Period data: %d rows, %d columns\\n\", nrow(period_data), ncol(period_data)))\n",
    "  cat(sprintf(\"Events: %d (%.2f%%)\\n\", sum(period_data$status), \n",
    "              100 * sum(period_data$status) / nrow(period_data)))\n",
    "  \n",
    "  # Create MC-CV splits (stratified by outcome)\n",
    "  cat(sprintf(\"Creating %d MC-CV splits (stratified)...\\n\", n_mc_splits))\n",
    "  flush.console()\n",
    "  mc_splits <- mc_cv(\n",
    "    data = period_data,\n",
    "    prop = train_prop,\n",
    "    times = n_mc_splits,\n",
    "    strata = status\n",
    "  )\n",
    "  \n",
    "  # Run each method\n",
    "  period_results <- list()\n",
    "  \n",
    "  for (method in method_names) {\n",
    "    result <- run_mc_cv_method(period_data, method, period_name, mc_splits)\n",
    "    period_results[[method]] <- result\n",
    "    \n",
    "    # Save top features (sorted alphabetically for easier comparison)\n",
    "    top_features_df <- tibble(\n",
    "      feature = names(result$top_features),\n",
    "      importance = as.numeric(result$top_features),\n",
    "      cindex_td = result$cindex_td_mean,\n",
    "      cindex_ti = result$cindex_ti_mean\n",
    "    ) %>%\n",
    "      arrange(feature)  # Sort alphabetically for easier visual comparison\n",
    "    \n",
    "    output_file <- file.path(output_dir, sprintf(\"%s_%s_top20.csv\", \n",
    "                                                  period_name, tolower(method)))\n",
    "    write_csv(top_features_df, output_file)\n",
    "    cat(sprintf(\"âœ“ Saved: %s\\n\", basename(output_file)))\n",
    "  }\n",
    "  \n",
    "  all_results[[period_name]] <- period_results\n",
    "}\n",
    "\n",
    "cat(\"\\nâœ“ Analysis complete for all periods!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Summary Results\n",
    "\n",
    "Create summary tables with C-index comparisons and statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create C-index comparison table\n",
    "cindex_comparison <- map_df(period_names, function(period) {\n",
    "  map_df(method_names, function(method) {\n",
    "    result <- all_results[[period]][[method]]\n",
    "    tibble(\n",
    "      period = period,\n",
    "      method = method,\n",
    "      cindex_td_mean = result$cindex_td_mean,\n",
    "      cindex_td_sd = result$cindex_td_sd,\n",
    "      cindex_td_ci_lower = result$cindex_td_ci_lower,\n",
    "      cindex_td_ci_upper = result$cindex_td_ci_upper,\n",
    "      cindex_ti_mean = result$cindex_ti_mean,\n",
    "      cindex_ti_sd = result$cindex_ti_sd,\n",
    "      cindex_ti_ci_lower = result$cindex_ti_ci_lower,\n",
    "      cindex_ti_ci_upper = result$cindex_ti_ci_upper,\n",
    "      n_splits = result$n_successful\n",
    "    )\n",
    "  })\n",
    "})\n",
    "\n",
    "write_csv(cindex_comparison, file.path(output_dir, \"cindex_comparison_mc_cv.csv\"))\n",
    "cat(\"âœ“ Saved: cindex_comparison_mc_cv.csv\\n\")\n",
    "\n",
    "# Display the table\n",
    "print(cindex_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "summary_stats <- map_df(period_names, function(period) {\n",
    "  period_data <- periods[[period]]\n",
    "  tibble(\n",
    "    period = period,\n",
    "    n_patients = nrow(period_data),\n",
    "    n_events = sum(period_data$outcome_graft_loss, na.rm = TRUE),\n",
    "    event_rate = 100 * sum(period_data$outcome_graft_loss, na.rm = TRUE) / nrow(period_data)\n",
    "  )\n",
    "})\n",
    "\n",
    "write_csv(summary_stats, file.path(output_dir, \"summary_statistics_mc_cv.csv\"))\n",
    "cat(\"âœ“ Saved: summary_statistics_mc_cv.csv\\n\")\n",
    "\n",
    "# Display the table\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close parallel processing\n",
    "plan(sequential)\n",
    "\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Analysis Complete!\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "cat(sprintf(\"Output directory: %s\\n\", output_dir))\n",
    "cat(sprintf(\"MC-CV splits: %d\\n\", n_mc_splits))\n",
    "cat(sprintf(\"Train/Test ratio: %.0f/%.0f\\n\", train_prop * 100, (1 - train_prop) * 100))\n",
    "cat(\"\\nResults show C-indexes with 95% confidence intervals\\n\")\n",
    "cat(\"based on\", n_mc_splits, \"independent train/test splits.\\n\\n\")\n",
    "cat(\"âœ“ All files saved successfully!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization plots for feature importance and C-index analysis\n",
    "# Uses the updated create_visualizations.R script with improved normalization procedure\n",
    "# Creates: Feature importance heatmap, C-index heatmap, scaled bar chart, and C-index table\n",
    "\n",
    "library(here)\n",
    "\n",
    "# Source the visualization script from scripts/R/\n",
    "# Scripts are consolidated in scripts/R/ to match EC2 structure\n",
    "if (!file.exists(here(\"scripts\", \"R\", \"create_visualizations.R\"))) {\n",
    "  stop(\"Cannot find scripts/R/create_visualizations.R. Ensure scripts are in scripts/R/ directory.\")\n",
    "}\n",
    "source(here(\"scripts\", \"R\", \"create_visualizations.R\"))\n",
    "\n",
    "# Run visualizations\n",
    "# The function automatically detects outputs/ directory relative to current working directory\n",
    "# Notebook should be run from feature_importance/ directory, so outputs/ will be found there\n",
    "run_visualizations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sync Results and Code to S3\n",
    "\n",
    "Sync output files and code (notebook + R scripts) to S3 bucket. \n",
    "\n",
    "**Files synced:**\n",
    "- **Code:** `*.ipynb`, `*.R`, `README*.md`\n",
    "- **Output CSV files:** `outputs/*.csv` (11 files: 9 feature files + 2 summary files)\n",
    "- **Plot files:** `outputs/plots/*.png` (3 PNG files) and `outputs/plots/*.csv` (1 CSV table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing outputs and code to S3...\n",
      "Source: feature_importance/ directory\n",
      "Destination: s3://uva-private-data-lake/graft-loss/feature_importance/ \n",
      "AWS CLI: /usr/local/bin/aws \n",
      "\n",
      "Running: \"/usr/local/bin/aws\" s3 sync \"/home/pgx3874/graft-loss/feature_importance\" s3://uva-private-data-lake/graft-loss/feature_importance/ --include \"*.ipynb\" --include \"*.R\" --include \"README*.md\" --include \"outputs/**\" --exclude \"*checkpoint*\" --exclude \"*.tmp\" --exclude \"*.ipynb_checkpoints/*\" --exclude \"*.RData\" --exclude \"*.Rhistory\" --exclude \".Rproj.user/*\" --exclude \"catboost_info/*\" --exclude \"*.log\" --exclude \"*\" \n",
      "\n",
      "âœ“ Successfully synced outputs and code to S3\n",
      "  - Code files: *.ipynb, *.R, README*.md\n",
      "  - Output CSV files: outputs/*.csv (11 files)\n",
      "  - Plot files: outputs/plots/*.png and outputs/plots/*.csv (4 files)\n",
      "  - Total: All code, outputs, and visualizations synced\n"
     ]
    }
   ],
   "source": [
    "# Sync outputs and code to S3\n",
    "# On EC2, we're in the feature_importance directory\n",
    "s3_bucket <- \"s3://uva-private-data-lake/graft-loss/feature_importance/\"\n",
    "\n",
    "# Find AWS CLI (check common locations - EC2 typically has it in /usr/local/bin or /usr/bin)\n",
    "aws_cmd <- Sys.which(\"aws\")\n",
    "if (aws_cmd == \"\") {\n",
    "  # Try common EC2 installation paths\n",
    "  aws_paths <- c(\n",
    "    \"/usr/local/bin/aws\",\n",
    "    \"/usr/bin/aws\",\n",
    "    \"/home/ec2-user/.local/bin/aws\"\n",
    "  )\n",
    "  aws_cmd <- NULL\n",
    "  for (path in aws_paths) {\n",
    "    if (file.exists(path)) {\n",
    "      aws_cmd <- path\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  if (is.null(aws_cmd)) {\n",
    "    stop(\"AWS CLI not found. Please install AWS CLI or ensure it's in your PATH.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Syncing outputs and code to S3...\\n\")\n",
    "cat(\"Source: feature_importance/ directory\\n\")\n",
    "cat(\"Destination:\", s3_bucket, \"\\n\")\n",
    "cat(\"AWS CLI:\", aws_cmd, \"\\n\\n\")\n",
    "\n",
    "# Get current directory (should be feature_importance)\n",
    "current_dir <- getwd()\n",
    "if (!grepl(\"feature_importance\", current_dir)) {\n",
    "  warning(\"Current directory doesn't appear to be feature_importance. Double-check sync destination.\")\n",
    "}\n",
    "\n",
    "# Sync feature_importance directory (includes outputs/ and code files)\n",
    "# Explicitly include notebook, R scripts, README files, and outputs directory (including plots subdirectory)\n",
    "# Exclude temporary files, checkpoints, and unnecessary directories\n",
    "# Note: --delete flag removed for safety (won't delete files in S3 that don't exist locally)\n",
    "# Include patterns are processed before exclude patterns, then exclude everything else\n",
    "# The \"outputs/**\" pattern matches all files recursively in outputs/ including outputs/plots/\n",
    "sync_cmd <- sprintf(\n",
    "  '\"%s\" s3 sync \"%s\" %s --include \"*.ipynb\" --include \"*.R\" --include \"README*.md\" --include \"outputs/**\" --exclude \"*checkpoint*\" --exclude \"*.tmp\" --exclude \"*.ipynb_checkpoints/*\" --exclude \"*.RData\" --exclude \"*.Rhistory\" --exclude \".Rproj.user/*\" --exclude \"catboost_info/*\" --exclude \"*.log\" --exclude \"*\"',\n",
    "  aws_cmd,\n",
    "  current_dir,\n",
    "  s3_bucket\n",
    ")\n",
    "\n",
    "cat(\"Running:\", sync_cmd, \"\\n\\n\")\n",
    "result <- system(sync_cmd)\n",
    "\n",
    "if (result == 0) {\n",
    "  cat(\"âœ“ Successfully synced outputs and code to S3\\n\")\n",
    "  cat(\"  - Code files: *.ipynb, *.R, README*.md\\n\")\n",
    "  cat(\"  - Output CSV files: outputs/*.csv (11 files)\\n\")\n",
    "  cat(\"  - Plot files: outputs/plots/*.png and outputs/plots/*.csv (4 files)\\n\")\n",
    "  cat(\"  - Total: All code, outputs, and visualizations synced\\n\")\n",
    "} else {\n",
    "  warning(sprintf(\"S3 sync returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Shutdown EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shutdown EC2 instance after analysis completes\n",
    "# Set SHUTDOWN_EC2 = TRUE to enable, FALSE to disable\n",
    "SHUTDOWN_EC2 <- TRUE  # Change to TRUE to enable auto-shutdown\n",
    "\n",
    "if (SHUTDOWN_EC2) {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"Shutting down EC2 instance...\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  \n",
    "  # Get instance ID from EC2 metadata service\n",
    "  instance_id <- tryCatch({\n",
    "    system(\"curl -s http://169.254.169.254/latest/meta-data/instance-id\", intern = TRUE)\n",
    "  }, error = function(e) {\n",
    "    cat(\"Warning: Could not retrieve instance ID from metadata service.\\n\")\n",
    "    cat(\"If running on EC2, check that metadata service is accessible.\\n\")\n",
    "    return(NULL)\n",
    "  })\n",
    "  \n",
    "  if (!is.null(instance_id) && length(instance_id) > 0 && nchar(instance_id[1]) > 0) {\n",
    "    instance_id <- instance_id[1]\n",
    "    cat(sprintf(\"Instance ID: %s\\n\", instance_id))\n",
    "    \n",
    "    # Find AWS CLI\n",
    "    aws_cmd <- Sys.which(\"aws\")\n",
    "    if (aws_cmd == \"\") {\n",
    "      aws_paths <- c(\n",
    "        \"/usr/local/bin/aws\",\n",
    "        \"/usr/bin/aws\",\n",
    "        \"/home/ec2-user/.local/bin/aws\"\n",
    "      )\n",
    "      aws_cmd <- NULL\n",
    "      for (path in aws_paths) {\n",
    "        if (file.exists(path)) {\n",
    "          aws_cmd <- path\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if (!is.null(aws_cmd) && aws_cmd != \"\") {\n",
    "      # Stop the instance (use terminate-instances for permanent deletion)\n",
    "      shutdown_cmd <- sprintf(\n",
    "        '\"%s\" ec2 stop-instances --instance-ids %s',\n",
    "        aws_cmd,\n",
    "        instance_id\n",
    "      )\n",
    "      \n",
    "      cat(\"Running:\", shutdown_cmd, \"\\n\")\n",
    "      result <- system(shutdown_cmd)\n",
    "      \n",
    "      if (result == 0) {\n",
    "        cat(\"âœ“ EC2 instance stop command sent successfully\\n\")\n",
    "        cat(\"Instance will stop in a few moments.\\n\")\n",
    "        cat(\"Note: This is a STOP (not terminate), so you can restart it later.\\n\")\n",
    "      } else {\n",
    "        warning(sprintf(\"EC2 stop command returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "      }\n",
    "    } else {\n",
    "      cat(\"Warning: AWS CLI not found. Cannot shutdown instance.\\n\")\n",
    "      cat(\"Install AWS CLI or ensure it's in your PATH.\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"Warning: Could not determine instance ID. Skipping shutdown.\\n\")\n",
    "    cat(\"If you want to shutdown manually, use:\\n\")\n",
    "    cat(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"EC2 Auto-Shutdown: DISABLED\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  cat(\"To enable auto-shutdown, set SHUTDOWN_EC2 = TRUE in this cell.\\n\")\n",
    "  cat(\"Instance will continue running.\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "python",
   "pygments_lexer": "r",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
