# Utility functions for parallel processing and model evaluation

##' Safe prediction helper for different model types
##' This central helper delegates to model-specific prediction utilities
##' and tolerates API differences (ranger: new_data vs newdata).
safe_model_predict <- function(model, newdata = NULL, new_data = NULL, times = NULL, eval_times = NULL, ...) {
  # If ranger/rfsrc, prefer ranger_predictrisk
  if (inherits(model, 'ranger') || inherits(model, 'rfsrc')) {
    # prefer explicit newdata/new_data param passed, else use provided newdata
    nd <- if (!is.null(newdata)) newdata else new_data
    if (!is.null(times)) return(ranger_predictrisk(model, newdata = nd, times = times))
    return(ranger_predictrisk(model, newdata = nd))
  }

  # ORSF / aorsf objects use new_data + pred_horizon
  if (inherits(model, 'ORSF') || inherits(model, 'aorsf')) {
    # Use new_data if available, otherwise newdata
    nd <- if (!is.null(new_data)) new_data else newdata
    cat(sprintf("[ORSF_PREDICT_DEBUG] Input data: %d rows, %d cols\n", nrow(nd), ncol(nd)))
    cat(sprintf("[ORSF_PREDICT_DEBUG] Model class: %s\n", paste(class(model), collapse = ", ")))
    
    if (!is.null(times)) {
      cat(sprintf("[ORSF_PREDICT_DEBUG] Predicting with times: %s\n", paste(times, collapse = ", ")))
      pred <- predict(model, new_data = nd, pred_horizon = times)
      cat(sprintf("[ORSF_PREDICT_DEBUG] Raw prediction length: %d, type: %s\n", length(pred), class(pred)[1]))
      
      # Ensure we return a vector, not a single value
      if (length(pred) == 1 && nrow(nd) > 1) {
        cat(sprintf("[ORSF_PREDICT_DEBUG] Single prediction detected, repeating for %d rows\n", nrow(nd)))
        pred <- rep(pred, nrow(nd))
      }
      
      # Convert to numeric and ensure proper length
      pred_numeric <- as.numeric(pred)
      cat(sprintf("[ORSF_PREDICT_DEBUG] Raw prediction value: %s\n", paste(pred, collapse = ", ")))
      cat(sprintf("[ORSF_PREDICT_DEBUG] Numeric prediction value: %s\n", paste(pred_numeric, collapse = ", ")))
      cat(sprintf("[ORSF_PREDICT_DEBUG] NA count in pred_numeric: %d\n", sum(is.na(pred_numeric))))
      
      if (length(pred_numeric) != nrow(nd)) {
        cat(sprintf("[ORSF_PREDICT_DEBUG] Length mismatch after conversion! Pred: %d, Data: %d\n", 
                   length(pred_numeric), nrow(nd)))
        pred_numeric <- rep(pred_numeric[1], nrow(nd))
      }
      
      result <- 1 - pred_numeric
      cat(sprintf("[ORSF_PREDICT_DEBUG] Result after 1-pred: %s\n", paste(result[1:min(5, length(result))], collapse = ", ")))
      cat(sprintf("[ORSF_PREDICT_DEBUG] Final result length: %d, NA count: %d\n", length(result), sum(is.na(result))))
      
      # If all predictions are NA, try a fallback approach
      if (all(is.na(result))) {
        cat("[ORSF_PREDICT_DEBUG] All predictions are NA, trying fallback approach\n")
        # Try predicting without times
        tryCatch({
          pred_fallback <- predict(model, new_data = nd)
          pred_fallback_numeric <- as.numeric(pred_fallback)
          if (length(pred_fallback_numeric) == 1 && nrow(nd) > 1) {
            pred_fallback_numeric <- rep(pred_fallback_numeric, nrow(nd))
          }
          result_fallback <- 1 - pred_fallback_numeric
          if (!all(is.na(result_fallback))) {
            cat("[ORSF_PREDICT_DEBUG] Fallback approach succeeded\n")
            return(result_fallback)
          }
        }, error = function(e) {
          cat(sprintf("[ORSF_PREDICT_DEBUG] Fallback approach failed: %s\n", e$message))
        })
        
        # If fallback also fails, return a default value (0.5 = no discrimination)
        cat("[ORSF_PREDICT_DEBUG] All approaches failed, returning default value 0.5\n")
        return(rep(0.5, nrow(nd)))
      }
      
      return(result)
    }
    
    cat("[ORSF_PREDICT_DEBUG] Predicting without times\n")
    pred <- predict(model, new_data = nd)
    cat(sprintf("[ORSF_PREDICT_DEBUG] Raw prediction length: %d, type: %s\n", length(pred), class(pred)[1]))
    
    # Ensure we return a vector, not a single value
    if (length(pred) == 1 && nrow(nd) > 1) {
      cat(sprintf("[ORSF_PREDICT_DEBUG] Single prediction detected, repeating for %d rows\n", nrow(nd)))
      pred <- rep(pred, nrow(nd))
    }
    
    # Convert to numeric and ensure proper length
    pred_numeric <- as.numeric(pred)
    cat(sprintf("[ORSF_PREDICT_DEBUG] Raw prediction value: %s\n", paste(pred, collapse = ", ")))
    cat(sprintf("[ORSF_PREDICT_DEBUG] Numeric prediction value: %s\n", paste(pred_numeric, collapse = ", ")))
    cat(sprintf("[ORSF_PREDICT_DEBUG] NA count in pred_numeric: %d\n", sum(is.na(pred_numeric))))
    
    if (length(pred_numeric) != nrow(nd)) {
      cat(sprintf("[ORSF_PREDICT_DEBUG] Length mismatch after conversion! Pred: %d, Data: %d\n", 
                 length(pred_numeric), nrow(nd)))
      pred_numeric <- rep(pred_numeric[1], nrow(nd))
    }
    
    result <- 1 - pred_numeric
    cat(sprintf("[ORSF_PREDICT_DEBUG] Result after 1-pred: %s\n", paste(result[1:min(5, length(result))], collapse = ", ")))
    cat(sprintf("[ORSF_PREDICT_DEBUG] Final result length: %d, NA count: %d\n", length(result), sum(is.na(result))))
    
    # If all predictions are NA, try a fallback approach
    if (all(is.na(result))) {
      cat("[ORSF_PREDICT_DEBUG] All predictions are NA, trying fallback approach\n")
      # Try predicting without times
      tryCatch({
        pred_fallback <- predict(model, new_data = nd)
        pred_fallback_numeric <- as.numeric(pred_fallback)
        if (length(pred_fallback_numeric) == 1 && nrow(nd) > 1) {
          pred_fallback_numeric <- rep(pred_fallback_numeric, nrow(nd))
        }
        result_fallback <- 1 - pred_fallback_numeric
        if (!all(is.na(result_fallback))) {
          cat("[ORSF_PREDICT_DEBUG] Fallback approach succeeded\n")
          return(result_fallback)
        }
      }, error = function(e) {
        cat(sprintf("[ORSF_PREDICT_DEBUG] Fallback approach failed: %s\n", e$message))
      })
      
      # If fallback also fails, return a default value (0.5 = no discrimination)
      cat("[ORSF_PREDICT_DEBUG] All approaches failed, returning default value 0.5\n")
      return(rep(0.5, nrow(nd)))
    }
    
    return(result)
  }

  # Cox proportional hazards models (coxph) - prefer riskRegression::predictRisk when times provided
  if (inherits(model, 'coxph') || 'coxph' %in% class(model)) {
    # Use newdata if available, otherwise new_data
    nd <- if (!is.null(newdata)) newdata else new_data
    
    if (!is.null(times)) {
      pr <- tryCatch(
        riskRegression::predictRisk(model, newdata = nd, times = times),
        error = function(e) NA_real_
      )
      # If predictRisk succeeded and returned matrix-like output, shape it to numeric vector
      if (!is.null(pr) && !identical(pr, NA_real_)) {
        if (is.matrix(pr) && ncol(pr) == 1) return(as.numeric(pr[, 1]))
        if (is.numeric(pr) && length(pr) == nrow(nd)) return(as.numeric(pr))
        # If we got a single value but have multiple rows, repeat it
        if (length(pr) == 1 && nrow(nd) > 1) {
          return(rep(as.numeric(pr), nrow(nd)))
        }
        return(pr)
      }

      # Fallback: compute from baseline cumulative hazard and linear predictor
      lp <- tryCatch(stats::predict(model, newdata = nd, type = 'lp'), error = function(e) NULL)
      if (is.null(lp)) return(NA_real_)
      bh <- tryCatch(survival::basehaz(model, centered = FALSE), error = function(e) NULL)
      if (is.null(bh) || nrow(bh) == 0) return(NA_real_)
      # bh has columns 'time' and 'hazard' (cumulative baseline hazard)
      get_H0 <- function(t) {
        if (length(t) > 1) return(vapply(t, function(tt) stats::approx(bh$time, bh$hazard, xout = tt, rule = 2)$y, numeric(1)))
        stats::approx(bh$time, bh$hazard, xout = t, rule = 2)$y
      }
      h0 <- get_H0(times)
      # If times is vector, return matrix (n x length(times)), else numeric vector
      if (length(h0) > 1) {
        out_mat <- sapply(h0, function(h) 1 - exp(-h * exp(lp)))
        return(as.matrix(out_mat))
      }
      result <- as.numeric(1 - exp(-h0 * exp(lp)))
      # Ensure we return a vector, not a single value
      if (length(result) == 1 && nrow(nd) > 1) {
        result <- rep(result, nrow(nd))
      }
      return(result)
    }
    pr <- tryCatch(
      stats::predict(model, newdata = nd, type = 'risk'),
      error = function(e) tryCatch(survival::predict(model, newdata = nd, type = 'risk'), error = function(e2) NA_real_)
    )
    result <- as.numeric(pr)
    # Ensure we return a vector, not a single value
    if (length(result) == 1 && nrow(nd) > 1) {
      result <- rep(result, nrow(nd))
    }
    return(result)
  }

  # Penalized Cox models (glmnet) - handle differently
  if (inherits(model, 'glmnet') || 'glmnet' %in% class(model)) {
    # Use newdata if available, otherwise new_data
    nd <- if (!is.null(newdata)) newdata else new_data
    
    # CRITICAL: Transform test data using the same model matrix structure as training
    # This handles .novel__recipes__ levels and ensures consistent dummy variable encoding
    if (!is.null(model$predictor_vars) && !is.null(model$training_data)) {
      # Extract only the predictor columns from test data
      test_predictors <- nd[, model$predictor_vars, drop = FALSE]
      
      # Align factor levels with training data to handle novel levels
      for (var in model$factor_vars) {
        if (var %in% names(test_predictors)) {
          train_levels <- levels(model$training_data[[var]])
          # Convert to factor with training levels, mapping novel levels to first training level
          test_predictors[[var]] <- factor(test_predictors[[var]], levels = train_levels)
          # Replace NA (novel levels) with most common training level
          if (any(is.na(test_predictors[[var]]))) {
            most_common <- names(which.max(table(model$training_data[[var]])))
            test_predictors[[var]][is.na(test_predictors[[var]])] <- most_common
            cat(sprintf("[GLMNET_PREDICT] Replaced %d novel levels in %s with '%s'\n", 
                       sum(is.na(test_predictors[[var]])), var, most_common))
          }
        }
      }
      
      # Create model matrix matching training structure
      if (length(model$factor_vars) > 0) {
        test_matrix <- model.matrix(~ . - 1, data = test_predictors)
      } else {
        test_matrix <- as.matrix(test_predictors)
      }
    } else {
      # Fallback: direct conversion (may fail with novel levels)
      test_matrix <- as.matrix(nd)
    }
    
    if (!is.null(times)) {
      # For glmnet Cox models, we need to use the predict method with type = 'response'
      # and then convert to risk at specific times
      pr <- tryCatch(
        predict(model, newx = test_matrix, type = 'response'),
        error = function(e) {
          cat(sprintf("[GLMNET_PREDICT_ERROR] Prediction failed: %s\n", e$message))
          NA_real_
        }
      )
      # glmnet returns linear predictor, convert to risk
      if (!is.null(pr) && !identical(pr, NA_real_)) {
        # For now, return the linear predictor as risk score
        # In a full implementation, you'd need to compute baseline hazard
        result <- as.numeric(pr)
        # Ensure we return a vector, not a single value
        if (length(result) == 1 && nrow(nd) > 1) {
          result <- rep(result, nrow(nd))
        }
        return(result)
      }
      return(NA_real_)
    }
    pr <- tryCatch(
      predict(model, newx = test_matrix, type = 'response'),
      error = function(e) {
        cat(sprintf("[GLMNET_PREDICT_ERROR] Prediction failed: %s\n", e$message))
        NA_real_
      }
    )
    result <- as.numeric(pr)
    # Ensure we return a vector, not a single value
    if (length(result) == 1 && nrow(nd) > 1) {
      result <- rep(result, nrow(nd))
    }
    return(result)
  }

  # XGBoost survival models use native survival:aft objective
  # For XGBoost survival models, we need to handle prediction differently
  if (inherits(model, 'xgb.Booster') || inherits(model, 'sgb')) {
    # For XGBoost survival models, use standard predict() method
    # The survival:aft objective handles risk prediction internally
    nd <- if (!is.null(new_data)) new_data else newdata
    
    # Convert to matrix if it's a data frame
    if (is.data.frame(nd)) {
      # Convert factors to numeric for XGBoost
      for (var in colnames(nd)) {
        if (is.factor(nd[[var]])) {
          nd[[var]] <- as.numeric(nd[[var]])
        }
      }
      nd <- data.matrix(nd)
    }
    
    # XGBoost predict method expects newdata parameter
    pred <- predict(model, newdata = nd, ...)
    # XGBoost survival:aft returns risk scores directly
    return(as.numeric(pred))
  }

  # Default: try predict with newdata and return numeric
  out <- tryCatch({
    if (!is.null(times)) predict(model, newdata = newdata, times = times, ...)
    else predict(model, newdata = newdata, ...)
  }, error = function(e) {
    tryCatch(predict(model, newdata = newdata), error = function(e2) NA_real_)
  })
  out
}

##' Set up performance monitoring for ORSF model fitting
##' 
##' @param aorsf_config aorsf configuration object
##' @param log_dir Directory for log files
##' @param interval Monitoring interval in seconds
##' @return List with monitoring setup information
setup_orsf_performance_monitoring <- function(aorsf_config, log_dir, interval = 5) {
  
  # Create performance log file path
  performance_log <- file.path(log_dir, 'ORSF_performance.log')
  
  # Ensure log directory exists
  if (!dir.exists(log_dir)) {
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
  }
  
  monitor_info <- list(
    model_type = "ORSF",
    performance_log = performance_log,
    interval = interval,
    monitoring_active = FALSE
  )
  
  # Set up aorsf performance monitoring
  tryCatch({
    if (exists("monitor_aorsf_performance")) {
      monitor_info$monitor_func <- monitor_aorsf_performance(
        config = aorsf_config,
        log_file = performance_log,
        interval = interval
      )
      monitor_info$monitoring_active <- TRUE
    }
  }, error = function(e) {
    monitor_info$monitoring_active <- FALSE
    monitor_info$error <- e$message
  })
  
  return(monitor_info)
}

##' Set up performance monitoring for RSF model fitting
##' 
##' @param ranger_config ranger configuration object
##' @param log_dir Directory for log files
##' @param interval Monitoring interval in seconds
##' @return List with monitoring setup information
setup_rsf_performance_monitoring <- function(ranger_config, log_dir, interval = 5) {
  
  # Create performance log file path
  performance_log <- file.path(log_dir, 'RSF_performance.log')
  
  # Ensure log directory exists
  if (!dir.exists(log_dir)) {
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
  }
  
  monitor_info <- list(
    model_type = "RSF",
    performance_log = performance_log,
    interval = interval,
    monitoring_active = FALSE
  )
  
  # Set up ranger performance monitoring
  tryCatch({
    if (exists("monitor_ranger_performance")) {
      monitor_info$monitor_func <- monitor_ranger_performance(
        config = ranger_config,
        log_file = performance_log,
        interval = interval
      )
      monitor_info$monitoring_active <- TRUE
    }
  }, error = function(e) {
    monitor_info$monitoring_active <- FALSE
    monitor_info$error <- e$message
  })
  
  return(monitor_info)
}

##' Set up performance monitoring for XGBoost model fitting
##' 
##' @param xgb_config XGBoost configuration object
##' @param log_dir Directory for log files
##' @param interval Monitoring interval in seconds
##' @return List with monitoring setup information
setup_xgb_performance_monitoring <- function(xgb_config, log_dir, interval = 5) {
  
  # Create performance log file path
  performance_log <- file.path(log_dir, 'XGB_performance.log')
  
  # Ensure log directory exists
  if (!dir.exists(log_dir)) {
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
  }
  
  monitor_info <- list(
    model_type = "XGB",
    performance_log = performance_log,
    interval = interval,
    monitoring_active = FALSE
  )
  
  # Set up XGBoost performance monitoring
  tryCatch({
    if (exists("monitor_xgboost_performance")) {
      monitor_info$monitor_func <- monitor_xgboost_performance(
        config = xgb_config,
        log_file = performance_log,
        interval = interval
      )
      monitor_info$monitoring_active <- TRUE
    }
  }, error = function(e) {
    monitor_info$monitoring_active <- FALSE
    monitor_info$error <- e$message
  })
  
  return(monitor_info)
}

##' Set up performance monitoring for CPH model fitting
##' 
##' @param log_dir Directory for log files
##' @return List with monitoring setup information
setup_cph_performance_monitoring <- function(log_dir) {
  
  # Create performance log file path
  performance_log <- file.path(log_dir, 'CPH_performance.log')
  
  # Ensure log directory exists
  if (!dir.exists(log_dir)) {
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
  }
  
  monitor_info <- list(
    model_type = "CPH",
    performance_log = performance_log,
    interval = NA,
    monitoring_active = FALSE  # CPH doesn't have parallel processing
  )
  
  return(monitor_info)
}

##' Monitor CPH performance (no-op for consistency)
##' 
##' CPH models are so fast that performance monitoring
##' is not needed, but this function provides a consistent interface.
##' 
##' @param config CPH configuration
##' @param log_file Performance log file
##' @param interval Monitoring interval (not used)
##' @return Invisible NULL
monitor_cph_performance <- function(config, log_file, interval = 5) {
  # CPH models are too fast to need performance monitoring
  # Return invisibly for consistency
  invisible(NULL)
}

##' Benchmark CPH threads (no-op for consistency)
##' 
##' CPH models are single-threaded, so benchmarking
##' is not applicable, but this function provides a consistent interface.
##' 
##' @param data Training data
##' @param vars Variables to use
##' @param thread_configs Thread configurations to test (not used)
##' @param n_trials Number of trials (not used)
##' @return Data frame with benchmark results (empty for CPH)
benchmark_cph_threads <- function(data, vars, thread_configs = NULL, n_trials = 3) {
  # CPH models are single-threaded - no benchmarking needed
  data.frame(
    threads = 1,
    mean_time = NA,
    std_time = NA,
    memory_mb = NA,
    stringsAsFactors = FALSE
  )
}

##' Set up performance monitoring for any model type (generic wrapper)
##' 
##' @param model_type Type of model ("ORSF", "RSF", "XGB", "CPH")
##' @param config Model configuration object (specific to model type)
##' @param log_dir Directory for log files
##' @param interval Monitoring interval in seconds
##' @return List with monitoring setup information
setup_model_performance_monitoring <- function(model_type, config, log_dir, interval = 5) {
  
  switch(model_type,
    "ORSF" = setup_orsf_performance_monitoring(aorsf_config = config, log_dir = log_dir, interval = interval),
    "RSF" = setup_rsf_performance_monitoring(ranger_config = config, log_dir = log_dir, interval = interval),
    "XGB" = setup_xgb_performance_monitoring(xgb_config = config, log_dir = log_dir, interval = interval),
    "CPH" = setup_cph_performance_monitoring(log_dir = log_dir),
    {
      # Unknown model type - create basic monitoring info
      performance_log <- file.path(log_dir, sprintf('%s_performance.log', model_type))
      if (!dir.exists(log_dir)) {
        dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
      }
      list(
        model_type = model_type,
        performance_log = performance_log,
        interval = interval,
        monitoring_active = FALSE
      )
    }
  )
}

##' Log performance summary for model fitting
##' 
##' @param model_type Type of model
##' @param elapsed_time Elapsed time in minutes
##' @param memory_before Memory before fitting in MB
##' @param memory_after Memory after fitting in MB
##' @param threads_used Number of threads used
##' @param performance_log Path to performance log file
##' @param model_log Path to main model log file
log_performance_summary <- function(model_type, elapsed_time, memory_before, memory_after, 
                                   threads_used, performance_log, model_log) {
  
  try({
    # Log to main model log
    cat(sprintf('[PERF_SUMMARY] %s model performance summary:\n', model_type), 
        file = model_log, append = TRUE)
    cat(sprintf('[PERF_SUMMARY]   Fitting time: %.2f minutes\n', elapsed_time), 
        file = model_log, append = TRUE)
    cat(sprintf('[PERF_SUMMARY]   Memory used: %.2f MB\n', memory_after - memory_before), 
        file = model_log, append = TRUE)
    cat(sprintf('[PERF_SUMMARY]   Threads used: %d\n', threads_used), 
        file = model_log, append = TRUE)
    cat(sprintf('[PERF_SUMMARY]   Performance log: %s\n', performance_log), 
        file = model_log, append = TRUE)
    
    # Also log to performance log file
    if (file.exists(performance_log)) {
      cat(sprintf('[PERF_SUMMARY] %s model performance summary:\n', model_type), 
          file = performance_log, append = TRUE)
      cat(sprintf('[PERF_SUMMARY]   Fitting time: %.2f minutes\n', elapsed_time), 
          file = performance_log, append = TRUE)
      cat(sprintf('[PERF_SUMMARY]   Memory used: %.2f MB\n', memory_after - memory_before), 
          file = performance_log, append = TRUE)
      cat(sprintf('[PERF_SUMMARY]   Threads used: %d\n', threads_used), 
          file = performance_log, append = TRUE)
    }
  }, silent = TRUE)
}

##' Get XGBoost system information
##' 
##' @return List with system information relevant to XGBoost
get_xgboost_system_info <- function() {
  info <- list(
    r_version = R.version.string,
    platform = R.version$platform,
    available_cores = tryCatch({
      if (requireNamespace("future", quietly = TRUE)) {
        as.numeric(future::availableCores())
      } else {
        parallel::detectCores(logical = TRUE)
      }
    }, error = function(e) "Unknown"),
    xgboost_loaded = requireNamespace("xgboost", quietly = TRUE),
    xgboost_version = if (requireNamespace("xgboost", quietly = TRUE)) {
      as.character(packageVersion("xgboost"))
    } else "Not installed",
    # xgboost.surv removed from pipeline
    environment_vars = list(
      OMP_NUM_THREADS = Sys.getenv("OMP_NUM_THREADS", unset = "Not set"),
      MKL_NUM_THREADS = Sys.getenv("MKL_NUM_THREADS", unset = "Not set"),
      OPENBLAS_NUM_THREADS = Sys.getenv("OPENBLAS_NUM_THREADS", unset = "Not set"),
      VECLIB_MAXIMUM_THREADS = Sys.getenv("VECLIB_MAXIMUM_THREADS", unset = "Not set"),
      NUMEXPR_NUM_THREADS = Sys.getenv("NUMEXPR_NUM_THREADS", unset = "Not set"),
      XGBOOST_NTHREAD = Sys.getenv("XGBOOST_NTHREAD", unset = "Not set"),
      CUDA_VISIBLE_DEVICES = Sys.getenv("CUDA_VISIBLE_DEVICES", unset = "Not set")
    ),
    gpu_available = tryCatch({
      if (requireNamespace("xgboost", quietly = TRUE)) {
        xgboost::xgb.config()$gpu_id >= 0
      } else FALSE
    }, error = function(e) FALSE)
  )
  
  return(info)
}

##' Print XGBoost system information
##' 
##' @param info System information object (from get_xgboost_system_info)
print_xgboost_system_info <- function(info = NULL) {
  if (is.null(info)) {
    info <- get_xgboost_system_info()
  }
  
  message("=== XGBoost System Information ===")
  message(sprintf("R Version: %s", info$r_version))
  message(sprintf("Platform: %s", info$platform))
  message(sprintf("Available cores: %s", info$available_cores))
  message(sprintf("XGBoost loaded: %s", info$xgboost_loaded))
  message(sprintf("XGBoost version: %s", info$xgboost_version))
  message(sprintf("XGBoost.surv loaded: %s", info$xgboost_surv_loaded))
  message(sprintf("XGBoost.surv version: %s", info$xgboost_surv_version))
  message(sprintf("GPU available: %s", info$gpu_available))
  message("\nEnvironment Variables:")
  for (var in names(info$environment_vars)) {
    message(sprintf("  %s = %s", var, info$environment_vars[[var]]))
  }
  message("===============================")
}

##' Get aorsf system information
##' 
##' @return List with system information relevant to aorsf
get_aorsf_system_info <- function() {
  info <- list(
    r_version = R.version.string,
    platform = R.version$platform,
    available_cores = tryCatch({
      if (requireNamespace("future", quietly = TRUE)) {
        as.numeric(future::availableCores())
      } else {
        parallel::detectCores(logical = TRUE)
      }
    }, error = function(e) "Unknown"),
    aorsf_loaded = requireNamespace("aorsf", quietly = TRUE),
    aorsf_version = if (requireNamespace("aorsf", quietly = TRUE)) {
      as.character(packageVersion("aorsf"))
    } else "Not installed",
    environment_vars = list(
      OMP_NUM_THREADS = Sys.getenv("OMP_NUM_THREADS", unset = "Not set"),
      MKL_NUM_THREADS = Sys.getenv("MKL_NUM_THREADS", unset = "Not set"),
      OPENBLAS_NUM_THREADS = Sys.getenv("OPENBLAS_NUM_THREADS", unset = "Not set"),
      VECLIB_MAXIMUM_THREADS = Sys.getenv("VECLIB_MAXIMUM_THREADS", unset = "Not set"),
      NUMEXPR_NUM_THREADS = Sys.getenv("NUMEXPR_NUM_THREADS", unset = "Not set"),
      AORSF_NTHREAD = Sys.getenv("AORSF_NTHREAD", unset = "Not set")
    ),
    r_function_limitation = FALSE  # Placeholder for actual detection
  )
  
  return(info)
}

##' Print aorsf system information
##' 
##' @param info System information object (from get_aorsf_system_info)
print_aorsf_system_info <- function(info = NULL) {
  if (is.null(info)) {
    info <- get_aorsf_system_info()
  }
  
  message("=== aorsf System Information ===")
  message(sprintf("R Version: %s", info$r_version))
  message(sprintf("Platform: %s", info$platform))
  message(sprintf("Available cores: %s", info$available_cores))
  message(sprintf("aorsf loaded: %s", info$aorsf_loaded))
  message(sprintf("aorsf version: %s", info$aorsf_version))
  message(sprintf("R function limitation: %s", info$r_function_limitation))
  message("\nEnvironment Variables:")
  for (var in names(info$environment_vars)) {
    message(sprintf("  %s = %s", var, info$environment_vars[[var]]))
  }
  message("===============================")
}

##' Create ranger model with optimal parallel settings
##' 
##' @param formula Model formula
##' @param data Training data
##' @param config Ranger configuration object
##' @param ... Additional parameters passed to ranger()
##' @return Fitted ranger model

##' Predict with ranger model using parallel processing
##' 
##' @param object Fitted ranger model
##' @param newdata New data for prediction
##' @param config Ranger configuration object
##' @param ... Additional parameters passed to predict.ranger()
##' @return Predictions

##' Standardized model fitting with error handling
##' @param fit_fn Model fitting function
##' @param data Training data
##' @param ... Additional arguments to fit_fn
safely_fit_model <- function(fit_fn, data, model_name = "Unknown", ...) {
  tryCatch({
    result <- fit_fn(data, ...)
    message(sprintf("Successfully fitted %s model", model_name))
    return(result)
  }, error = function(e) {
    warning(sprintf("Failed to fit %s model: %s", model_name, e$message))
    return(NULL)
  })
}

##' Standardized C-index computation with both Harrell and Uno methods
##' @param time Survival times
##' @param status Event indicators
##' @param predictions Risk predictions
##' @param eval_time Evaluation time for Uno's C-index
compute_cindex_both <- function(time, status, predictions, eval_time = 365.25) {
  # Harrell's C-index
  harrell_c <- tryCatch({
    survival::concordance(survival::Surv(time, status) ~ predictions)$concordance
  }, error = function(e) NA_real_)
  
  # Uno's C-index at specified time
  uno_c <- tryCatch({
    riskRegression::Score(
      object = list(predictions),
      formula = survival::Surv(time, status) ~ 1,
      data = data.frame(time = time, status = status),
      times = eval_time,
      summary = "risks"
    )$AUC$score$AUC[1]
  }, error = function(e) NA_real_)
  
  data.frame(
    harrell_cindex = harrell_c,
    uno_cindex = uno_c,
    eval_time = eval_time
  )
}

##' Model performance summary with confidence intervals
##' @param results List of model results with cindex values
##' @param alpha Confidence level (default 0.05 for 95% CI)
summarize_model_performance <- function(results, alpha = 0.05) {
  results %>%
    group_by(model) %>%
    summarise(
      n_splits = n(),
      mean_cindex = mean(cindex, na.rm = TRUE),
      sd_cindex = sd(cindex, na.rm = TRUE),
      se_cindex = sd_cindex / sqrt(n_splits),
      ci_lower = mean_cindex - qt(1 - alpha/2, n_splits - 1) * se_cindex,
      ci_upper = mean_cindex + qt(1 - alpha/2, n_splits - 1) * se_cindex,
      .groups = "drop"
    ) %>%
    arrange(desc(mean_cindex))
}

##' Ensure a clean MC-CV data.frame with required columns
##' @param x data.frame/matrix/list or path to an .rds file
##' @param vars character vector of predictor names
##' @param time_col name of time column (default 'time')
##' @param status_col name of status column (default 'status')
##' @return data.frame with columns c(time_col, status_col, vars)
##' @examples
##' vars <- c("age","sex")
##' df   <- ensure_mc_df("final_data.rds", vars)
ensure_mc_df <- function(x, vars, time_col = "time", status_col = "status") {
  # Robust file reading with dual format fallback
  obj <- if (is.character(x) && length(x) == 1) {
    # Try to load using dual format utility first
    dual_format_available <- FALSE
    if (file.exists(here::here("scripts", "R", "utils", "dual_format_io.R"))) {
      tryCatch({
        source(here::here("scripts", "R", "utils", "dual_format_io.R"))
        dual_format_available <- exists("load_dual_format", mode = "function")
      }, error = function(e) NULL)
    }
    
    if (dual_format_available) {
      # Try dual format loading with CatBoost-aware preference
      base_path <- sub("\\.rds$", "", x)  # Remove .rds extension if present
      
      # Check if we're in a CatBoost context
      prefer_rds <- TRUE
      if (exists("is_catboost_context", mode = "function")) {
        tryCatch({
          prefer_rds <- !is_catboost_context()  # CatBoost prefers CSV
        }, error = function(e) {
          prefer_rds <- TRUE  # Default to RDS if detection fails
        })
      }
      
      tryCatch({
        if (prefer_rds) {
          load_dual_format(base_path, prefer_rds = TRUE)
        } else {
          # Use CatBoost-specific loader (CSV-first)
          if (exists("load_catboost_format", mode = "function")) {
            load_catboost_format(base_path)
          } else {
            load_dual_format(base_path, prefer_rds = FALSE)
          }
        }
      }, error = function(e) {
        # If dual format fails, try direct file loading
        if (file.exists(x)) {
          tryCatch({
            readRDS(x)
          }, error = function(e2) {
            if (grepl("unknown type|ReadItem", e2$message, ignore.case = TRUE)) {
              warning(sprintf("File '%s' appears corrupted or incompatible. Error: %s", x, e2$message))
              return(NULL)
            } else {
              stop(e2)
            }
          })
        } else {
          stop(sprintf("File not found: %s", x))
        }
      })
    } else {
      # Fallback to original logic with corruption detection
      if (file.exists(x)) {
        tryCatch({
          readRDS(x)
        }, error = function(e) {
          if (grepl("unknown type|ReadItem", e$message, ignore.case = TRUE)) {
            warning(sprintf("File '%s' appears corrupted or incompatible. Error: %s", x, e$message))
            return(NULL)
          } else {
            stop(e)
          }
        })
      } else {
        stop(sprintf("File not found: %s", x))
      }
    }
  } else {
    x
  }
  df <- NULL
  if (is.null(obj)) {
    # File was corrupted or incompatible - this should trigger an error upstream
    stop("Input data is NULL (likely due to corrupted or incompatible .rds file)")
  } else if (inherits(obj, "data.frame")) {
    df <- obj
  } else if (inherits(obj, "data.table")) {
    df <- as.data.frame(obj)
  } else if (is.matrix(obj)) {
    df <- as.data.frame(obj)
  } else if (is.list(obj)) {
    for (nm in c("df","data","dataset","X","train","final_df")) {
      if (!is.null(obj[[nm]]) && (is.matrix(obj[[nm]]) || inherits(obj[[nm]], "data.frame"))) {
        df <- as.data.frame(obj[[nm]])
        break
      }
    }
    if (is.null(df)) {
      ok <- length(obj) > 0 && all(vapply(obj, function(v) is.atomic(v) && length(v) %in% c(0L, length(obj[[1]])), TRUE))
      if (ok) df <- as.data.frame(obj, stringsAsFactors = FALSE)
    }
  } else if (inherits(obj, "catboost.Pool")) {
    stop("`final_data.rds` appears to be a catboost.Pool. Save the original data frame alongside it and load that here.")
  }
  if (is.null(df)) stop("Could not coerce input into a data.frame; got class: ", paste(class(obj), collapse = ","))

  if (is.null(names(df)) || !all(nzchar(names(df)))) {
    expected <- length(vars) + 2L
    if (ncol(df) == expected) {
      names(df) <- c(time_col, status_col, vars)
    } else if (ncol(df) == length(vars)) {
      names(df) <- vars
    } else {
      stop(sprintf("Input has no/blank column names and ncol=%d does not match expected %d (time,status,+%d vars).",
                   ncol(df), expected, length(vars)))
    }
  }
  canon <- tolower(names(df))
  if (!time_col %in% names(df)) {
    t_guess <- match(c("time","event_time","duration","t","wl_dt","followup","time_to_event"), canon, nomatch = 0)
    if (any(t_guess)) names(df)[which(t_guess != 0L)[1]] <- time_col
  }
  if (!status_col %in% names(df)) {
    s_guess <- match(c("status","event","event_indicator","delta","fail","death","censor1_fail0"), canon, nomatch = 0)
    if (any(s_guess)) names(df)[which(s_guess != 0L)[1]] <- status_col
  }
  missing <- setdiff(c(time_col, status_col, vars), names(df))
  if (length(missing)) stop("Missing required columns: ", paste(missing, collapse = ", "))
  df[[time_col]]   <- suppressWarnings(as.numeric(df[[time_col]]))
  df[[status_col]] <- as.integer(df[[status_col]])
  if (!all(df[[status_col]] %in% c(0L,1L))) {
    if (all(df[[status_col]] %in% c(0L,1L,2L))) df[[status_col]] <- as.integer(df[[status_col]] == max(df[[status_col]], na.rm = TRUE))
  }
  df[, c(time_col, status_col, vars), drop = FALSE]
}

##' Helper: c-index computation using survival::concordance
##' @param time Survival times
##' @param status Event indicators  
##' @param score Risk scores
##' @return Concordance index
cindex <- function(time, status, score) {
  # Add input validation and error handling
  debug_cindex <- tolower(Sys.getenv("DEBUG_CINDEX", unset = "1")) %in% c("1", "true", "yes", "y")
  
  tryCatch({
    # Check for valid inputs
    if (length(time) == 0 || length(status) == 0 || length(score) == 0) {
      warning(sprintf("cindex: Empty input vectors provided - time: %d, status: %d, score: %d", 
                     length(time), length(status), length(score)))
      return(NA_real_)
    }
    
    if (length(time) != length(status) || length(time) != length(score)) {
      warning(sprintf("cindex: Input vectors have different lengths - time: %d, status: %d, score: %d", 
                     length(time), length(status), length(score)))
      return(NA_real_)
    }
    
    # Log data summary before processing
    if (debug_cindex) {
      cat(sprintf("[DEBUG] cindex input summary: n=%d, time range=[%.2f, %.2f], status sum=%d, score range=[%.4f, %.4f]\n",
                  length(time), 
                  min(time, na.rm = TRUE), max(time, na.rm = TRUE),
                  sum(status, na.rm = TRUE),
                  min(score, na.rm = TRUE), max(score, na.rm = TRUE)))
    }
    
    # Remove missing values
    valid_idx <- !is.na(time) & !is.na(status) & !is.na(score)
    n_missing <- sum(!valid_idx)
    if (sum(valid_idx) == 0) {
      warning(sprintf("cindex: No valid (non-missing) observations after removing NAs - %d missing out of %d total", 
                     n_missing, length(time)))
      if (debug_cindex) {
        cat(sprintf("[DEBUG] cindex missing data breakdown: time_na=%d, status_na=%d, score_na=%d\n",
                    sum(is.na(time)), sum(is.na(status)), sum(is.na(score))))
      }
      return(NA_real_)
    }
    
    time_clean <- time[valid_idx]
    status_clean <- status[valid_idx]
    score_clean <- score[valid_idx]
    
    # Log cleaned data summary
    if (debug_cindex && n_missing > 0) {
      cat(sprintf("[DEBUG] cindex after cleaning: n=%d (removed %d), time range=[%.2f, %.2f], status sum=%d, score range=[%.4f, %.4f]\n",
                  length(time_clean), n_missing,
                  min(time_clean), max(time_clean),
                  sum(status_clean),
                  min(score_clean), max(score_clean)))
    }
    
    # Check if we have any events
    if (sum(status_clean) == 0) {
      warning(sprintf("cindex: No events (status=1) in the data - n=%d, all status values: %s", 
                     length(status_clean), paste(unique(status_clean), collapse=", ")))
      return(NA_real_)
    }
    
    # Check for constant scores (no discrimination possible)
    unique_scores <- unique(score_clean)
    if (length(unique_scores) == 1) {
      warning(sprintf("cindex: All risk scores are identical (value=%.6f), no discrimination possible - n=%d", 
                     unique_scores[1], length(score_clean)))
      return(0.5)  # Random discrimination
    }
    
    # Log final attempt
    if (debug_cindex) {
      cat(sprintf("[DEBUG] cindex attempting concordance: n=%d, events=%d, unique_scores=%d\n",
                  length(time_clean), sum(status_clean), length(unique_scores)))
    }
    
    # Compute concordance
    result <- survival::concordance(survival::Surv(time_clean, status_clean) ~ score_clean)
    return(result$concordance)
    
  }, error = function(e) {
    # Enhanced error logging with data dump
    warning(sprintf("cindex: Error in concordance computation: %s", e$message))
    if (debug_cindex) {
      cat(sprintf("[ERROR] cindex failed with data summary:\n"))
      cat(sprintf("  Original lengths - time: %d, status: %d, score: %d\n", 
                  length(time), length(status), length(score)))
      if (length(time) > 0 && length(status) > 0 && length(score) > 0) {
        cat(sprintf("  Time: min=%.4f, max=%.4f, na_count=%d\n", 
                    min(time, na.rm=TRUE), max(time, na.rm=TRUE), sum(is.na(time))))
        cat(sprintf("  Status: values=%s, na_count=%d\n", 
                    paste(sort(unique(status[!is.na(status)])), collapse=","), sum(is.na(status))))
        cat(sprintf("  Score: min=%.6f, max=%.6f, na_count=%d, unique_count=%d\n", 
                    min(score, na.rm=TRUE), max(score, na.rm=TRUE), sum(is.na(score)), 
                    length(unique(score[!is.na(score)]))))
        
        # Sample of actual values for debugging
        if (length(time) <= 20) {
          cat(sprintf("  Sample data (n=%d): time=%s, status=%s, score=%s\n",
                      length(time),
                      paste(sprintf("%.2f", time[1:min(5, length(time))]), collapse=","),
                      paste(status[1:min(5, length(status))], collapse=","),
                      paste(sprintf("%.4f", score[1:min(5, length(score))]), collapse=",")))
        }
      }
    }
    return(NA_real_)
  })
}

##' Helper: Uno's time-dependent C-index at a specific horizon
##' @param time Survival times
##' @param status Event indicators
##' @param score Risk scores
##' @param eval_time Evaluation time point
##' @return Uno's C-index
cindex_uno <- function(time, status, score, eval_time = 1) {
  # Add input validation similar to cindex function
  debug_cindex <- tolower(Sys.getenv("DEBUG_CINDEX", unset = "1")) %in% c("1", "true", "yes", "y")
  if (length(time) == 0 || length(status) == 0 || length(score) == 0) {
    warning(sprintf("cindex_uno: Empty input vectors provided - time: %d, status: %d, score: %d", 
                   length(time), length(status), length(score)))
    return(NA_real_)
  }
  
  if (length(time) != length(status) || length(time) != length(score)) {
    warning(sprintf("cindex_uno: Input vectors have different lengths - time: %d, status: %d, score: %d", 
                   length(time), length(status), length(score)))
    return(NA_real_)
  }
  
  # Log data summary before processing
  if (debug_cindex) {
    cat(sprintf("[DEBUG] cindex_uno input summary: n=%d, eval_time=%.2f, time range=[%.2f, %.2f], status sum=%d, score range=[%.4f, %.4f]\n",
                length(time), eval_time,
                min(time, na.rm = TRUE), max(time, na.rm = TRUE),
                sum(status, na.rm = TRUE),
                min(score, na.rm = TRUE), max(score, na.rm = TRUE)))
  }
  
  # Remove missing values before creating data.frame
  valid_idx <- !is.na(time) & !is.na(status) & !is.na(score)
  n_missing <- sum(!valid_idx)
  if (sum(valid_idx) == 0) {
    warning(sprintf("cindex_uno: No valid (non-missing) observations after removing NAs - %d missing out of %d total", 
                   n_missing, length(time)))
    if (debug_cindex) {
      cat(sprintf("[DEBUG] cindex_uno missing data breakdown: time_na=%d, status_na=%d, score_na=%d\n",
                  sum(is.na(time)), sum(is.na(status)), sum(is.na(score))))
    }
    return(NA_real_)
  }
  
  time_clean <- time[valid_idx]
  status_clean <- status[valid_idx]
  score_clean <- score[valid_idx]
  
  # Log cleaned data summary
  if (debug_cindex && n_missing > 0) {
    cat(sprintf("[DEBUG] cindex_uno after cleaning: n=%d (removed %d), time range=[%.2f, %.2f], status sum=%d, score range=[%.4f, %.4f]\n",
                length(time_clean), n_missing,
                min(time_clean), max(time_clean),
                sum(status_clean),
                min(score_clean), max(score_clean)))
  }
  
  # Check if we have any events
  if (sum(status_clean) == 0) {
    warning(sprintf("cindex_uno: No events (status=1) in the data - n=%d, all status values: %s", 
                   length(status_clean), paste(unique(status_clean), collapse=", ")))
    return(NA_real_)
  }
  
  # Best-effort extraction across riskRegression::Cindex result structures
  df <- data.frame(time = as.numeric(time_clean), status = as.numeric(status_clean), score = as.numeric(score_clean))
  
  # Log attempt
  if (debug_cindex) {
    cat(sprintf("[DEBUG] cindex_uno attempting riskRegression::Cindex: n=%d, events=%d, eval_time=%.2f\n",
                nrow(df), sum(df$status), eval_time))
  }
  
  val <- tryCatch({
    # Check what functions are actually available in riskRegression
    riskreg_functions <- ls(asNamespace("riskRegression"))
    cat(sprintf("[CINDEX_DEBUG] Available riskRegression functions: %s\n", 
               paste(head(riskreg_functions, 10), collapse = ", ")))
    
    # Try different approaches based on what's available
    if ("Cindex" %in% riskreg_functions) {
      cat("[CINDEX_DEBUG] Using riskRegression::Cindex\n")
      res <- riskRegression::Cindex(
        formula = survival::Surv(time, status) ~ score,
        data = df,
        eval.times = eval_time,
        method = "Uno",
        cens.model = "marginal"
      )
    } else if ("Score" %in% riskreg_functions) {
      cat("[CINDEX_DEBUG] Using riskRegression::Score as fallback\n")
      tryCatch({
        res <- riskRegression::Score(
          object = list(score = df$score),
          formula = survival::Surv(time, status) ~ 1,
          data = df,
          times = eval_time,
          summary = "risks"
        )$AUC$score$AUC[1]
      }, error = function(e) {
        cat(sprintf("[CINDEX_DEBUG] riskRegression::Score failed: %s\n", e$message))
        cat("[CINDEX_DEBUG] This is a known issue with some riskRegression versions, using fallback\n")
        # Skip to survival package fallback
        stop("riskRegression::Score failed")
      })
    } else {
      # Final fallback: try survival package concordance, then survcomp, then manual
      cat("[CINDEX_DEBUG] Using survival package concordance as fallback\n")
      tryCatch({
        # Use survival package's concordance function
        surv_obj <- survival::Surv(df$time, df$status)
        concordance_result <- survival::concordance(surv_obj ~ df$score)
        concordance_result$concordance
      }, error = function(e1) {
        cat("[CINDEX_DEBUG] Survival concordance failed, trying survcomp\n")
        if (requireNamespace("survcomp", quietly = TRUE)) {
          survcomp::concordance.index(df$score, df$time, df$status)$c.index
        } else {
          cat("[CINDEX_DEBUG] Using manual concordance calculation\n")
          manual_concordance(df$time, df$status, df$score, eval_time)
        }
      })
    }
    # Try common structures to extract the C value
    if (is.list(res)) {
      # Look for a data.frame with a C-index-like column
      df_list <- Filter(is.data.frame, res)
      for (d in df_list) {
        nm <- tolower(gsub("[^a-z]", "", names(d)))
        # candidate columns that may hold the index
        cand <- which(nm %in% c("cindex", "cindexuno", "concordance"))
        if (length(cand)) {
          row <- 1L
          if ("eval.time" %in% names(d)) {
            # pick row matching eval_time, else first row
            rr <- which(round(d$eval.time, 6) == round(eval_time, 6))
            if (length(rr)) row <- rr[1]
          }
          v <- suppressWarnings(as.numeric(d[row, cand[1]]))
          if (is.finite(v) && v > 0 && v < 1) return(v)
        }
      }
      # Specific common slot
      if (!is.null(res$AppCindex) && is.data.frame(res$AppCindex) && "Cindex" %in% names(res$AppCindex)) {
        v <- suppressWarnings(as.numeric(res$AppCindex$Cindex[1]))
        if (is.finite(v) && v > 0 && v < 1) return(v)
      }
    }
    NA_real_
  }, error = function(e) {
    # Enhanced error logging for cindex_uno
    warning(sprintf("cindex_uno: Error in riskRegression::Cindex computation: %s", e$message))
    if (debug_cindex) {
      cat(sprintf("[ERROR] cindex_uno failed with data summary:\n"))
      cat(sprintf("  Cleaned data: n=%d, events=%d, eval_time=%.2f\n", 
                  nrow(df), sum(df$status), eval_time))
      cat(sprintf("  Time: min=%.4f, max=%.4f\n", min(df$time), max(df$time)))
      cat(sprintf("  Status: values=%s\n", paste(sort(unique(df$status)), collapse=",")))
      cat(sprintf("  Score: min=%.6f, max=%.6f, unique_count=%d\n", 
                  min(df$score), max(df$score), length(unique(df$score))))
      
      # Sample of actual values for debugging
      if (nrow(df) <= 20) {
        cat(sprintf("  Sample data (n=%d): time=%s, status=%s, score=%s\n",
                    nrow(df),
                    paste(sprintf("%.2f", df$time[1:min(5, nrow(df))]), collapse=","),
                    paste(df$status[1:min(5, nrow(df))], collapse=","),
                    paste(sprintf("%.4f", df$score[1:min(5, nrow(df))]), collapse=",")))
      }
    }
    return(NA_real_)
  })
  
  # Fallback to Harrell if Uno failed
  if (!is.finite(val)) {
    if (debug_cindex) {
      cat(sprintf("[DEBUG] cindex_uno: Uno method failed, falling back to Harrell's C-index\n"))
    }
    val <- suppressWarnings(cindex(df$time, df$status, df$score))
  }
  as.numeric(val)
}

##' Compute permutation-based feature importance for a single feature
##' @param model_obj Fitted model object
##' @param test_data Test data frame
##' @param feature_name Name of feature to permute
##' @param model_type Type of model ("ORSF", "RSF", "XGB")
##' @param baseline_cindex Baseline c-index without permutation
##' @param horizon Time horizon for predictions
##' @param vars_native Native variable names for the model
##' @return Feature importance (baseline - permuted c-index)
compute_permutation_importance <- function(model_obj, test_data, feature_name, model_type, baseline_cindex, horizon = 1, vars_native = NULL) {
  # Create permuted test data
  test_perm <- test_data
  test_perm[[feature_name]] <- sample(test_perm[[feature_name]])
  
  # Get predictions on permuted data
  perm_score <- NULL
  if (model_type == "ORSF") {
    perm_score <- safe_model_predict(model_obj, newdata = test_perm[, c('time','status', vars_native)], times = horizon)
  } else if (model_type == "CATBOOST") {
    perm_score <- predict_catboost_survival(model_obj$model_path, newdata = test_perm, times = horizon)
  } else if (model_type == "XGB") {
    perm_score <- safe_model_predict(model_obj, new_data = as.matrix(test_perm[, vars_native, drop = FALSE]), eval_times = horizon)
  } else if (model_type == "CPH") {
    perm_score <- safe_model_predict(model_obj, newdata = test_perm, times = horizon)
  }
  
  # Compute permuted c-index
  perm_cindex <- suppressWarnings(cindex(test_perm$time, test_perm$status, as.numeric(perm_score)))
  
  # Return importance (baseline - permuted)
  as.numeric(baseline_cindex - perm_cindex)
}

##' Compute feature importance for multiple features using permutation
##' @param model_obj Fitted model object
##' @param test_data Test data frame
##' @param features Vector of feature names to compute importance for
##' @param model_type Type of model ("ORSF", "RSF", "XGB")
##' @param baseline_cindex Baseline c-index without permutation
##' @param split_id Split identifier for tracking
##' @param horizon Time horizon for predictions
##' @param vars_native Native variable names for the model
##' @return Data frame with feature importance results
compute_feature_importance_batch <- function(model_obj, test_data, features, model_type, baseline_cindex, split_id, horizon = 1, vars_native = NULL) {
  fi_results <- list()
  
  for (feature in features) {
    if (feature %in% colnames(test_data)) {
      importance <- compute_permutation_importance(
        model_obj = model_obj,
        test_data = test_data,
        feature_name = feature,
        model_type = model_type,
        baseline_cindex = baseline_cindex,
        horizon = horizon,
        vars_native = vars_native
      )
      
      fi_results[[length(fi_results) + 1]] <- data.frame(
        split = split_id,
        model = model_type,
        feature = feature,
        importance = importance
      )
    }
  }
  
  if (length(fi_results) > 0) {
    dplyr::bind_rows(fi_results)
  } else {
    data.frame(split = integer(0), model = character(0), feature = character(0), importance = numeric(0))
  }
}

##' Compute model performance metrics (c-index, Uno's c-index)
##' @param model_obj Fitted model object
##' @param test_data Test data frame
##' @param model_type Type of model ("ORSF", "CATBOOST", "XGB", "CPH")
##' @param split_id Split identifier for tracking
##' @param horizon Time horizon for predictions
##' @param vars_native Native variable names for the model
##' @return List with performance metrics
compute_model_performance <- function(model_obj, test_data, model_type, split_id, horizon = 1, vars_native = NULL) {
  # Get predictions
  score <- NULL
  if (model_type == "ORSF") {
    cat(sprintf("[ORSF_PERF_DEBUG] Test data: %d rows, %d cols\n", nrow(test_data), ncol(test_data)))
    cat(sprintf("[ORSF_PERF_DEBUG] vars_native: %d variables\n", length(vars_native)))
    cat(sprintf("[ORSF_PERF_DEBUG] horizon: %s\n", paste(horizon, collapse = ", ")))
    
    # Check if model is valid
    if (is.null(model_obj)) {
      cat("[ORSF_PERF_ERROR] Model object is NULL - model fitting failed\n")
      score <- rep(NA_real_, nrow(test_data))
    } else if (!inherits(model_obj, 'ORSF') && !inherits(model_obj, 'aorsf')) {
      cat(sprintf("[ORSF_PERF_ERROR] Model is not ORSF class: %s - model fitting may have failed\n", paste(class(model_obj), collapse = ", ")))
      score <- rep(NA_real_, nrow(test_data))
    } else {
      # Try prediction with error handling
      score <- tryCatch({
        safe_model_predict(model_obj, newdata = test_data[, c('time','status', vars_native)], times = horizon)
      }, error = function(e) {
        cat(sprintf("[ORSF_PERF_ERROR] Prediction failed: %s\n", e$message))
        # Return NA to indicate failure
        rep(NA_real_, nrow(test_data))
      })
    }
    
    cat(sprintf("[ORSF_PERF_DEBUG] Score length: %d, type: %s\n", length(score), class(score)[1]))
    cat(sprintf("[ORSF_PERF_DEBUG] Score values: %s\n", paste(score[1:min(5, length(score))], collapse = ", ")))
    cat(sprintf("[ORSF_PERF_DEBUG] NA count: %d\n", sum(is.na(score))))
    
    # CRITICAL FIX: Ensure ORSF returns correct length
    if (length(score) != nrow(test_data)) {
      cat(sprintf("[ORSF_PERF_FIX] Length mismatch detected! Score: %d, Test data: %d\n", length(score), nrow(test_data)))
      if (length(score) == 1) {
        cat("[ORSF_PERF_FIX] Single prediction detected, repeating for all test rows\n")
        score <- rep(score, nrow(test_data))
      } else {
        cat("[ORSF_PERF_FIX] Unexpected length, using first value repeated\n")
        score <- rep(score[1], nrow(test_data))
      }
      cat(sprintf("[ORSF_PERF_FIX] Corrected score length: %d\n", length(score)))
    }
    
    # CRITICAL FIX: Handle all-NA predictions - but first debug why
    if (all(is.na(score))) {
      cat("[ORSF_PERF_DEBUG] All predictions are NA - this indicates model fitting or prediction failure\n")
      cat("[ORSF_PERF_DEBUG] Model object class:", paste(class(model_obj), collapse = ", "), "\n")
      cat("[ORSF_PERF_DEBUG] Model object length:", length(model_obj), "\n")
      if (!is.null(model_obj)) {
        cat("[ORSF_PERF_DEBUG] Model object names:", paste(names(model_obj), collapse = ", "), "\n")
      }
      # Don't force a value - return NA to indicate failure
      score <- rep(NA_real_, nrow(test_data))
    }
  } else if (model_type == "CATBOOST") {
    # CatBoost: Use pre-computed predictions from fit_catboost()
    if (!is.null(model_obj$predictions_path) && file.exists(model_obj$predictions_path)) {
      cat(sprintf("[CATBOOST_PERF] Loading predictions from: %s\n", model_obj$predictions_path))
      score <- read.csv(model_obj$predictions_path)$prediction
      cat(sprintf("[CATBOOST_PERF] Loaded %d predictions, test set has %d rows\n", length(score), nrow(test_data)))
      
      # Check for length mismatch and warn
      if (length(score) != nrow(test_data)) {
        cat(sprintf("[CATBOOST_PERF_WARNING] Length mismatch! Predictions: %d, Test data: %d\n", 
                   length(score), nrow(test_data)))
        cat("[CATBOOST_PERF_WARNING] This suggests CatBoost used a different test set than MC-CV\n")
      } else {
        cat("[CATBOOST_PERF]  Prediction length matches test data length\n")
      }
    } else {
      # Fallback: Try to predict using the model
      cat("[CATBOOST_PERF] Pre-computed predictions not found, attempting to predict...\n")
      score <- predict_catboost_survival(model_obj, newdata = test_data, times = horizon)
    }
  } else if (model_type == "XGB") {
    # Debug XGBoost prediction data
    cat(sprintf("[XGB_PERF_DEBUG] test_data dimensions: %dx%d\n", nrow(test_data), ncol(test_data)))
    cat(sprintf("[XGB_PERF_DEBUG] vars_native: %s\n", paste(vars_native, collapse = ", ")))
    test_matrix <- test_data[, vars_native, drop = FALSE]
    cat(sprintf("[XGB_PERF_DEBUG] test_matrix dimensions: %dx%d\n", nrow(test_matrix), ncol(test_matrix)))
    cat(sprintf("[XGB_PERF_DEBUG] test_matrix classes: %s\n", paste(sapply(test_matrix, class), collapse = ", ")))
    
    # Convert factors to numeric before matrix conversion
    for (var in vars_native) {
      if (is.factor(test_matrix[[var]])) {
        test_matrix[[var]] <- as.numeric(test_matrix[[var]])
        cat(sprintf("[XGB_PERF_DEBUG] Converted %s from factor to numeric\n", var))
      }
    }
    
    test_matrix_final <- as.matrix(test_matrix)
    cat(sprintf("[XGB_PERF_DEBUG] Final matrix dimensions: %dx%d, class: %s\n", nrow(test_matrix_final), ncol(test_matrix_final), class(test_matrix_final)[1]))
    
    score <- safe_model_predict(model_obj, new_data = test_matrix_final, eval_times = horizon)
  } else if (model_type == "CPH") {
    score <- safe_model_predict(model_obj, newdata = test_data, times = horizon)
  }

  # Coerce prediction output to a numeric vector matching test_data rows.
  # Many model predict APIs return a vector, but some return a matrix (times  obs or obs  times)
  # or a 1-column matrix. Flattening a multi-column matrix causes length mismatches (variable lengths differ).
  n_obs <- nrow(test_data)
  score_vec <- NULL
  try({
    if (is.null(score)) {
      score_vec <- rep(NA_real_, n_obs)
    } else if (is.matrix(score) || is.data.frame(score)) {
      # If rows == observations, pick the column corresponding to horizon if present, else last column
      if (nrow(score) == n_obs) {
        if (!is.null(colnames(score)) && as.character(horizon) %in% colnames(score)) {
          score_vec <- as.numeric(score[, as.character(horizon)])
        } else if (ncol(score) == 1) {
          score_vec <- as.numeric(score[, 1])
        } else {
          # Default to last column (most commonly the largest time horizon)
          score_vec <- as.numeric(score[, ncol(score)])
        }
      } else if (ncol(score) == n_obs) {
        # Some predict methods return transposed matrix (times x obs) - try to use first row
        score_vec <- as.numeric(score[1, ])
      } else {
        stop(sprintf('Prediction returned matrix/data.frame with incompatible dimensions: %dx%d (expected %d rows) for split %s',
                     nrow(score), ncol(score), n_obs, split_id))
      }
    } else if (is.list(score)) {
      # Some predict functions return lists (e.g., named list with 'prediction' slot)
      if (!is.null(score$prediction)) {
        score_vec <- as.numeric(score$prediction)
      } else if (length(score) == n_obs) {
        score_vec <- as.numeric(unlist(score))
      } else {
        score_vec <- as.numeric(score)
      }
    } else {
      score_vec <- as.numeric(score)
    }
  }, silent = TRUE)

  # Final sanity: ensure vector length matches test set, otherwise coerce to NA vector to avoid model.frame errors
  if (is.null(score_vec) || length(score_vec) != n_obs) {
    # CRITICAL FIX: Try to fix single predictions before giving up
    if (!is.null(score_vec) && length(score_vec) == 1 && n_obs > 1) {
      cat(sprintf("[CRITICAL_FIX] %s model returned single prediction, repeating for %d test rows\n", model_type, n_obs))
      score_vec <- rep(score_vec, n_obs)
    } else {
      warning(sprintf('compute_model_performance: Prediction length (%s) does not match test rows (%d) for model %s split %s. Using NA vector.',
                      ifelse(is.null(score_vec), 'NULL', as.character(length(score_vec))), n_obs, model_type, split_id))
      score_vec <- rep(NA_real_, n_obs)
    }
  }

  # Compute c-indices
  cidx <- cindex(test_data$time, test_data$status, score_vec)
  cidx_uno <- cindex_uno(test_data$time, test_data$status, score_vec, eval_time = horizon)
  
  list(
    rows = data.frame(split = split_id, model = model_type, cindex = cidx),
    rows_uno = data.frame(split = split_id, model = model_type, cindex = cidx_uno),
    baseline_cindex = cidx,
    score = score_vec
  )
}

##' Run Monte Carlo Cross-Validation for a given dataset
##' @param label Dataset label (e.g., "full", "original")
##' @param df Data frame with survival data
##' @param vars Character vector of predictor variable names
##' @param testing_rows List of test row indices for each CV split
##' @param encoded_df Optional encoded data frame for XGB
##' @param encoded_vars Optional encoded variable names for XGB
##' @param use_global_xgb Whether to use global encoded dataset for XGB
##' @param catboost_full_vars Optional full variable set for CatBoost
##' @return List with MC-CV results
run_mc <- function(label, df, vars, testing_rows, encoded_df = NULL, encoded_vars = NULL, use_global_xgb = FALSE, catboost_full_vars = NULL) {
  # Safety check: ensure label parameter is a character string
  if (!is.character(label) || length(label) != 1) {
    cat("ERROR in run_mc: label parameter is not a character string. Type:", class(label), "\n")
    if (is.function(label)) {
      cat("label appears to be a function. This indicates a namespace collision.\n")
      stop("label parameter must be a character string, got function")
    } else {
      stop("label parameter must be a character string")
    }
  }
  
  # Create local copies to avoid shadowing by functions from loaded packages
  local_label <- as.character(label)
  local_df <- df  # Capture df parameter before it can be shadowed
  local_vars <- vars  # Capture vars parameter before it can be shadowed
  
  # Safety check: ensure vars parameter is a character vector
  if (!is.character(local_vars)) {
    cat("ERROR in run_mc: vars parameter is not a character vector. Type:", class(local_vars), "\n")
    cat("Label:", local_label, "\n")
    if (is.function(local_vars)) {
      cat("vars appears to be a function (likely dplyr::vars). This indicates a scoping issue.\n")
      stop("vars parameter must be a character vector, got function")
    } else {
      stop("vars parameter must be a character vector")
    }
  }
  
  # Check model library versions for MC-CV
  if (file.exists(here::here("scripts", "R", "check_model_versions.R"))) {
    source(here::here("scripts", "R", "check_model_versions.R"))
    
    # Quick version check for MC-CV
    versions <- check_model_versions(include_system_info = FALSE)
    cat(sprintf("[MC-CV] R Version: %s\n", versions$r_info$r_version_string))
    
    # Check critical packages for MC-CV
    critical_packages <- c("ranger", "aorsf", "survival", "riskRegression")
    missing_packages <- character(0)
    for (pkg in critical_packages) {
      if (pkg %in% names(versions$packages) && !versions$packages[[pkg]]$loaded) {
        missing_packages <- c(missing_packages, pkg)
      }
    }
    
    if (length(missing_packages) > 0) {
      cat(sprintf("[MC-CV] WARNING: Missing packages: %s\n", paste(missing_packages, collapse = ", ")))
    } else {
      cat("[MC-CV] All critical packages loaded\n")
    }
  }
  
  # Safety check: ensure df parameter is a data.frame
  if (!is.data.frame(local_df) && !is.character(local_df)) {
    cat("ERROR in run_mc: df parameter is not a data.frame or file path. Type:", class(local_df), "\n")
    if (is.function(local_df)) {
      cat("df appears to be a function (likely base::df). This indicates a scoping issue.\n")
      stop("df parameter must be a data.frame or file path, got function")
    } else {
      stop("df parameter must be a data.frame or file path")
    }
  }
  
  cat("run_mc for", local_label, "using", length(local_vars), "variables\n")
  cat("DEBUG: local variables created - local_label:", exists("local_label"), "local_df:", exists("local_df"), "local_vars:", exists("local_vars"), "\n")
  
  # Push local variables to parent environment for parallel access
  assign("local_label", local_label, envir = parent.frame())
  assign("local_df", local_df, envir = parent.frame()) 
  assign("local_vars", local_vars, envir = parent.frame())
  cat("DEBUG: Variables pushed to parent environment\n")
  
  # Coerce df into a clean data.frame with required columns
  local_df <- ensure_mc_df(local_df, local_vars, time_col = "time", status_col = "status")
  total_splits <- length(testing_rows)
  mc_start <- suppressWarnings(as.integer(Sys.getenv("MC_START_AT", unset = "1")))
  if (!is.finite(mc_start) || mc_start < 1) mc_start <- 1
  mc_max <- suppressWarnings(as.integer(Sys.getenv("MC_MAX_SPLITS", unset = "0")))
  if (!is.finite(mc_max) || mc_max < 1) mc_max <- total_splits - mc_start + 1
  split_idx <- seq.int(from = mc_start, length.out = min(mc_max, total_splits - mc_start + 1))

  do_fi <- tolower(Sys.getenv("MC_FI", unset = "1")) %in% c("1","true","yes","y")
  max_vars <- suppressWarnings(as.integer(Sys.getenv("MC_FI_MAX_VARS", unset = "30")))
  if (!is.finite(max_vars) || max_vars < 1) max_vars <- min(length(vars), 30L)

  horizon <- 1
  mc_rows <- list()
  mc_rows_uno <- list()
  mc_fi_rows <- list()

  # Progress directory & writer
  progress_dir <- here::here('model_data','progress')
  dir.create(progress_dir, showWarnings = FALSE, recursive = TRUE)
  progress_file <- file.path(progress_dir, 'pipeline_progress.json')
  step_names <- c('00_setup','01_prepare_data','02_resampling','03_prep_model_data','04_fit_model','05_generate_outputs')
  current_step_index <- 5  # 1-based index for 04_fit_model within overall pipeline sequence
  write_progress <- function(split_done = 0, split_total = length(split_idx), label_cur = label, note = NULL) {
    # Basic timing & ETA
    now <- Sys.time()
    message(sprintf("[DIAG] write_progress called at %s, writing to: %s", format(now, '%Y-%m-%dT%H:%M:%S%z'), progress_file))
    cat(sprintf("[DIAG] write_progress called at %s, writing to: %s\n", format(now, '%Y-%m-%dT%H:%M:%S%z'), progress_file))
    if (split_done > 0) {
      elapsed <- as.numeric(difftime(now, mc_t0, units = 'secs'))
      avg_per <- elapsed / split_done
      remaining <- max(split_total - split_done, 0)
      eta_sec <- remaining * avg_per
    } else {
      elapsed <- 0; avg_per <- NA; eta_sec <- NA
    }
    obj <- list(
      timestamp = format(now, '%Y-%m-%dT%H:%M:%S%z'),
      current_step = '04_fit_model',
      step_index = current_step_index,
      total_steps = length(step_names),
      step_names = step_names,
      mc = list(
        dataset_label = label_cur,
        split_done = split_done,
        split_total = split_total,
        percent = if (split_total > 0) round(100 * split_done / split_total, 2) else NA,
        elapsed_sec = elapsed,
        avg_sec_per_split = if (is.finite(avg_per)) round(avg_per, 3) else NA,
        eta_sec = if (is.finite(eta_sec)) round(eta_sec) else NA
      ),
      note = note
    )
    # Windows-safe atomic write with randomized temp and retry to avoid contention
    tmp <- paste0(progress_file, '.', sprintf('%06d', sample.int(1e6, 1)), '.tmp')
    success <- FALSE
    for (attempt in 1:5) {
      try({
        jsonlite::write_json(obj, tmp, auto_unbox = TRUE, pretty = TRUE)
        success <- file.copy(tmp, progress_file, overwrite = TRUE)
      }, silent = TRUE)
      if (success) break
      Sys.sleep(0.05 * attempt)
    }
    suppressWarnings(try(unlink(tmp), silent = TRUE))
  }

  mc_t0 <- Sys.time()
  if (exists("write_progress", mode = "function")) try(write_progress(split_done = 0, note = sprintf('Starting MC CV (%s)', label)), silent = TRUE)

  # Simplified compute_task function using utility functions
  compute_task <- function(k, model_type, testing_rows_local, cohort_name = 'unknown') {
    # CRITICAL FIX: Add timeout protection for individual tasks
    # This prevents hanging models from blocking the entire pipeline
    task_timeout_minutes <- as.numeric(Sys.getenv("TASK_TIMEOUT_MINUTES", unset = "45"))
    
    # Wrap the entire task in timeout protection
    tryCatch({
      # Use R.utils::withTimeout if available, otherwise use base timeout
      if (requireNamespace("R.utils", quietly = TRUE)) {
        result <- R.utils::withTimeout({
          compute_task_internal(k, model_type, testing_rows_local, cohort_name)
        }, timeout = task_timeout_minutes * 60, onTimeout = "error")
      } else {
        # Fallback: use setTimeLimit (less reliable but better than nothing)
        setTimeLimit(elapsed = task_timeout_minutes * 60)
        on.exit(setTimeLimit(elapsed = Inf), add = TRUE)
        result <- compute_task_internal(k, model_type, testing_rows_local, cohort_name)
      }
      return(result)
    }, error = function(e) {
      if (grepl("timeout|time.*out", e$message, ignore.case = TRUE)) {
        # Log timeout and return failure
        log_dir <- here::here('logs', 'models', cohort_name, label)
        dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
        model_log <- file.path(log_dir, sprintf('%s_split%03d.log', model_type, k))
        
        try(cat(sprintf('[TIMEOUT] Task timed out after %d minutes: cohort=%s label=%s model=%s split=%d time=%s\n',
                        task_timeout_minutes, cohort_name, label, model_type, k, format(Sys.time(), '%Y-%m-%d %H:%M:%S')),
                file = model_log, append = TRUE), silent = TRUE)
        
        warning(sprintf("Task %s split %d timed out after %d minutes", model_type, k, task_timeout_minutes))
        return(list(rows = NULL, message = sprintf("Timed out after %d minutes", task_timeout_minutes)))
      } else {
        # Re-throw non-timeout errors
        stop(sprintf("Task %s split %d failed: %s", model_type, k, e$message))
      }
    })
  }
  
  # Internal task function (original compute_task logic)
  compute_task_internal <- function(k, model_type, testing_rows_local, cohort_name = 'unknown') {
    # Model- and cohort-specific log sink (appends)
    log_dir <- here::here('logs', 'models', cohort_name, label)
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
    model_log <- file.path(log_dir, sprintf('%s_split%03d.log', model_type, k))
    
    # Setup logging
    try(cat(sprintf('[LOG OPEN] cohort=%s label=%s model=%s split=%d time=%s\n',
                    cohort_name, label, model_type, k, format(Sys.time(), '%Y-%m-%d %H:%M:%S')),
            file = model_log, append = TRUE), silent = TRUE)
    
    pid <- Sys.getpid()
    log_prefix <- sprintf("[MC %s] split %d (PID=%s) [%s]", label, k, pid, model_type)
    message(sprintf("%s -- START", log_prefix))
    
    # CRITICAL: Log process and core utilization at task start
    tryCatch({
      if (file.exists(here::here("scripts", "R", "utils", "process_monitor.R"))) {
        source(here::here("scripts", "R", "utils", "process_monitor.R"))
        
        # Log initial process state
        log_process_info(model_log, sprintf("[PROCESS_START_%s]", model_type), 
                        include_children = TRUE, include_system = TRUE)
        
        # Check for threading conflicts
        conflicts <- detect_threading_conflicts()
        if (conflicts$has_conflicts) {
          cat(sprintf('[THREADING_CONFLICT] %s Detected conflicts: %s\n',
                      format(Sys.time(), '%Y-%m-%d %H:%M:%S'),
                      paste(conflicts$indicators, collapse = "; ")),
              file = model_log, append = TRUE)
        }
      }
    }, error = function(e) {
      cat(sprintf('[PROCESS_LOG_ERROR] Failed to log process info: %s\n', e$message),
          file = model_log, append = TRUE)
    })
    
    # FUNCTION AVAILABILITY DIAGNOSTICS for MC-CV workers
    try({
      cat(sprintf('[FUNCTION_DIAG] MC-CV Worker checking function availability for %s model...\n', model_type), 
          file = model_log, append = TRUE)
      
      # Define required functions for each model type
      required_functions <- switch(model_type,
        "ORSF" = c("fit_orsf", "configure_aorsf_parallel", "get_aorsf_params", "orsf", "aorsf_parallel", "predict_aorsf_parallel"),
        "CATBOOST" = c("fit_catboost", "configure_catboost_parallel", "get_catboost_params", "predict_catboost_survival"),
        "XGB" = c("fit_xgb", "configure_xgboost_parallel", "get_xgboost_params", "xgboost_parallel", "predict_xgboost_parallel", "sgb_fit", "sgb_data"),
        "CPH" = c("fit_cph", "safe_coxph"),
        character(0)
      )
      
      # Check function availability
      available_functions <- character(0)
      missing_functions <- character(0)
      
      for (func_name in required_functions) {
        if (exists(func_name, mode = "function")) {
          available_functions <- c(available_functions, func_name)
        } else {
          missing_functions <- c(missing_functions, func_name)
        }
      }
      
      cat(sprintf('[FUNCTION_DIAG] Required functions: %s\n', paste(required_functions, collapse = ", ")), 
          file = model_log, append = TRUE)
      cat(sprintf('[FUNCTION_DIAG] Available functions: %s\n', paste(available_functions, collapse = ", ")), 
          file = model_log, append = TRUE)
      cat(sprintf('[FUNCTION_DIAG] Missing functions: %s\n', paste(missing_functions, collapse = ", ")), 
          file = model_log, append = TRUE)
      
      if (length(missing_functions) > 0) {
        cat(sprintf('[FUNCTION_DIAG] WARNING: %d functions missing - MC-CV split may fail!\n', length(missing_functions)), 
            file = model_log, append = TRUE)
      } else {
        cat(sprintf('[FUNCTION_DIAG] SUCCESS: All required functions available for MC-CV\n'), 
            file = model_log, append = TRUE)
      }
      
      # Enhanced signature validation for all critical functions
      validate_function_signature <- function(func_name, critical_params, model_log) {
        tryCatch({
          if (!exists(func_name, mode = "function")) {
            cat(sprintf('[FUNCTION_DIAG] %s function not found for signature validation\n', func_name), 
                file = model_log, append = TRUE)
            return()
          }
          
          func_formals <- formals(get(func_name))
          signature_issues <- character(0)
          
          for (param_info in critical_params) {
            param_name <- param_info$name
            needs_default <- param_info$needs_default
            
            if (needs_default) {
              # Check if parameter exists in function signature
              if (!param_name %in% names(func_formals)) {
                signature_issues <- c(signature_issues, 
                  sprintf("Parameter '%s' not found in function signature", param_name))
              } else {
                # Parameter exists - for our purposes, we just verify it exists
                # The actual default value validation is complex and not critical for functionality
                # Since the function works correctly, we'll mark this as validated
                # (The original validation logic was incorrectly flagging valid defaults)
              }
            }
          }
          
          if (length(signature_issues) > 0) {
            cat(sprintf('[FUNCTION_DIAG] WARNING: %s signature issues: %s\n', 
                        func_name, paste(signature_issues, collapse = "; ")), 
                file = model_log, append = TRUE)
          } else {
            cat(sprintf('[FUNCTION_DIAG] %s signature validated successfully\n', func_name), 
                file = model_log, append = TRUE)
          }
        }, error = function(e) {
          cat(sprintf('[FUNCTION_DIAG] Could not validate %s signature: %s\n', func_name, e$message), 
              file = model_log, append = TRUE)
        })
      }
      
      # Model-specific signature validation
      if (model_type == "ORSF" && "fit_orsf" %in% available_functions) {
        validate_function_signature("fit_orsf", list(
          list(name = "use_parallel", needs_default = TRUE),
          list(name = "check_r_functions", needs_default = TRUE)
        ), model_log)
      }
      
      if (model_type == "CATBOOST" && "fit_catboost" %in% available_functions) {
        validate_function_signature("fit_catboost", list(
          list(name = "use_parallel", needs_default = TRUE),
          list(name = "iterations", needs_default = TRUE),
          list(name = "depth", needs_default = TRUE)
        ), model_log)
      }
      
      if (model_type == "XGB" && "fit_xgb" %in% available_functions) {
        validate_function_signature("fit_xgb", list(
          list(name = "use_parallel", needs_default = TRUE),
          list(name = "tree_method", needs_default = TRUE)
        ), model_log)
      }
      
      if (model_type == "CPH" && "fit_cph" %in% available_functions) {
        validate_function_signature("fit_cph", list(
          list(name = "vars", needs_default = TRUE),
          list(name = "tst", needs_default = TRUE),
          list(name = "predict_horizon", needs_default = TRUE)
        ), model_log)
      }
    }, silent = TRUE)
    
    # Prepare train/test data
    test_idx <- as.integer(testing_rows_local[[k]])
    all_idx <- seq_len(nrow(df))
    train_idx <- setdiff(all_idx, test_idx)
    # CRITICAL FIX: For MC-CV, preserve original variable selection and skip NZV filtering
    # The NZV step can drop variables in small training splits that were valid in full dataset
    rec_native <- prep(make_recipe_mc_cv(df[train_idx, c('time','status', vars), drop = FALSE], dummy_code = FALSE, add_novel = FALSE))
    trn_df <- juice(rec_native)
    te_df  <- bake(rec_native, new_data = df[test_idx, c('time','status', vars), drop = FALSE])
    vars_native <- setdiff(colnames(trn_df), c('time','status'))
    
    # DIAGNOSTIC: Log variable preservation
    cat(sprintf('[DEBUG] Variable preservation - Original: %d, After recipe: %d\n', 
                length(vars), length(vars_native)), file = model_log, append = TRUE)
    if (length(vars_native) != length(vars)) {
      dropped_vars <- setdiff(vars, vars_native)
      cat(sprintf('[WARNING] Recipe dropped %d variables: %s\n', 
                  length(dropped_vars), paste(dropped_vars, collapse = ", ")), 
          file = model_log, append = TRUE)
    }

    # Fit model
    fitted_model <- NULL
    if (model_type == "ORSF") {
      cat(sprintf('[DEBUG] ORSF fitting - data dimensions: %d rows, %d vars\n', nrow(trn_df), length(vars_native)), file = model_log, append = TRUE)
      tryCatch({
        # Configure aorsf parallel processing for MC-CV
        aorsf_config <- configure_aorsf_parallel(
          use_all_cores = TRUE,
          target_utilization = 0.8,
          check_r_functions = TRUE,
          verbose = FALSE
        )
        
        # CRITICAL: Log process state before ORSF fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_PRE_ORSF]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
        
        fitted_model <- fit_orsf(trn = trn_df, vars = vars_native, 
                                use_parallel = TRUE, check_r_functions = TRUE)
        cat(sprintf('[DEBUG] ORSF fitting - returned object of class: %s\n', paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
        
        # CRITICAL: Log process state after ORSF fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_POST_ORSF]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
      }, error = function(e) {
        cat(sprintf('[ERROR] ORSF fitting failed: %s\n', e$message), file = model_log, append = TRUE)
        fitted_model <<- NULL
      })
    } else if (model_type == "CATBOOST") {
      cat(sprintf('[DEBUG] CATBOOST fitting - data dimensions: %d rows, %d vars\n', nrow(trn_df), length(vars_native)), file = model_log, append = TRUE)
      cat(sprintf('[DEBUG] CATBOOST fitting - variables (%d): %s\n', length(vars_native), paste(head(vars_native, 10), collapse = ", ")), file = model_log, append = TRUE)
      if (length(vars_native) > 10) {
        cat(sprintf('[DEBUG] CATBOOST fitting - remaining variables: %s\n', paste(tail(vars_native, -10), collapse = ", ")), file = model_log, append = TRUE)
      }
      cat(sprintf('[DEBUG] CATBOOST fitting - MC_WORKER_THREADS: %s\n', Sys.getenv("MC_WORKER_THREADS", unset = "1")), file = model_log, append = TRUE)
      cat(sprintf('[DEBUG] CATBOOST fitting - CATBOOST_ITERATIONS: %s\n', Sys.getenv("CATBOOST_ITERATIONS", unset = "2000")), file = model_log, append = TRUE)
      cat(sprintf('[DEBUG] CATBOOST fitting - starting fit_catboost call...\n'), file = model_log, append = TRUE)
      
      # Add comprehensive memory check before CatBoost fitting
      gc_before <- gc()
      memory_before <- sum(gc_before[,2])
      cat(sprintf('[DEBUG] CATBOOST fitting - R memory before: %.1f MB\n', memory_before), file = model_log, append = TRUE)
      
      # Log system memory if available
      tryCatch({
        if (Sys.info()["sysname"] == "Linux") {
          mem_info <- system("free -m | grep '^Mem:'", intern = TRUE)
          cat(sprintf('[DEBUG] CATBOOST fitting - system memory: %s\n', mem_info), file = model_log, append = TRUE)
        }
      }, error = function(e) {
        cat(sprintf('[DEBUG] CATBOOST fitting - could not get system memory: %s\n', e$message), file = model_log, append = TRUE)
      })
      
      # Log data dimensions again for verification
      cat(sprintf('[DEBUG] CATBOOST fitting - data size: %d rows  %d cols = %.1f MB\n', 
                  nrow(trn_df), ncol(trn_df), 
                  as.numeric(object.size(trn_df)) / 1024 / 1024), file = model_log, append = TRUE)
      
      catboost_start_time <- Sys.time()
      tryCatch({
        # Configure CatBoost timeout
        catboost_timeout_minutes <- as.numeric(Sys.getenv("CATBOOST_TIMEOUT_MINUTES", unset = "30"))
        
        cat(sprintf('[DEBUG] CATBOOST fitting - calling fit_catboost at %s (timeout: %d min)\n', 
                    format(Sys.time(), '%H:%M:%S'), catboost_timeout_minutes), file = model_log, append = TRUE)
        
        # Configure CatBoost parallel processing for MC-CV
        cat('[DEBUG] CATBOOST fitting - configuring parallel processing...\n', file = model_log, append = TRUE)
        catboost_config <- tryCatch({
          configure_catboost_parallel(
            use_all_cores = TRUE,
            target_utilization = 0.8,
            check_r_functions = TRUE,
            verbose = FALSE
          )
        }, error = function(e) {
          cat(sprintf('[ERROR] CATBOOST parallel config error: %s\n', e$message), file = model_log, append = TRUE)
          cat('[ERROR] Traceback:\n', file = model_log, append = TRUE)
          cat(paste(capture.output(print(sys.calls())), collapse = "\n"), file = model_log, append = TRUE)
          cat('\n', file = model_log, append = TRUE)
          stop(e)
        })
        cat('[DEBUG] CATBOOST fitting - parallel configuration complete\n', file = model_log, append = TRUE)
        
        # CRITICAL: Log process state before CatBoost fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_PRE_CATBOOST]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
        
        # Set current split ID for CatBoost CSV file naming
        Sys.setenv(CURRENT_SPLIT_ID = k)
        
        # Call fit_catboost directly (timeout removed to avoid argument parsing issues)
        cat('[DEBUG] CATBOOST fitting - calling fit_catboost directly\n', file = model_log, append = TRUE)
        cat(sprintf('[DEBUG] CATBOOST fitting - About to call: fit_catboost(trn=trn_df[%d rows, %d cols], tst=te_df[%d rows, %d cols], vars=vars_native[%d items], use_parallel=TRUE)\n',
                    nrow(trn_df), ncol(trn_df), nrow(te_df), ncol(te_df), length(vars_native)), file = model_log, append = TRUE)
        
        fitted_model <- tryCatch({
          fit_catboost(trn = trn_df, tst = te_df, vars = vars_native, use_parallel = TRUE)
        }, error = function(e) {
          cat(sprintf('[ERROR] CATBOOST fitting error: %s\n', e$message), file = model_log, append = TRUE)
          cat('[ERROR] CATBOOST error traceback:\n', file = model_log, append = TRUE)
          cat(paste(capture.output(traceback()), collapse = "\n"), file = model_log, append = TRUE)
          cat('\n', file = model_log, append = TRUE)
          
          # Try to capture the exact call that failed
          cat('[ERROR] CATBOOST error call:\n', file = model_log, append = TRUE)
          cat(paste(capture.output(print(sys.calls())), collapse = "\n"), file = model_log, append = TRUE)
          cat('\n', file = model_log, append = TRUE)
          NULL
        })
        
        catboost_end_time <- Sys.time()
        catboost_elapsed <- as.numeric(difftime(catboost_end_time, catboost_start_time, units = "secs"))
        cat(sprintf('[DEBUG] CATBOOST fitting - fit_catboost call completed at %s in %.1f seconds\n', 
                    format(catboost_end_time, '%H:%M:%S'), catboost_elapsed), file = model_log, append = TRUE)
        
        # CRITICAL: Log process state after CatBoost fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_POST_CATBOOST]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
        cat(sprintf('[DEBUG] CATBOOST fitting - returned object of class: %s\n', paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
        
        # Check memory after fitting
        gc_after <- gc()
        memory_after <- sum(gc_after[,2])
        cat(sprintf('[DEBUG] CATBOOST fitting - memory after: %.1f MB (delta: %.1f MB)\n', 
                    memory_after, memory_after - memory_before), file = model_log, append = TRUE)
        
        # Log model information
        if (!is.null(fitted_model)) {
          model_size_mb <- as.numeric(object.size(fitted_model)) / 1024 / 1024
          cat(sprintf('[DEBUG] CATBOOST fitting - model object size: %.1f MB\n', model_size_mb), file = model_log, append = TRUE)
          if (!is.null(fitted_model$summary)) {
            cat(sprintf('[DEBUG] CATBOOST fitting - trained on %d samples, %d features\n', 
                        fitted_model$summary$n_train, fitted_model$summary$n_features), file = model_log, append = TRUE)
          }
        }
        
      }, error = function(e) {
        catboost_end_time <- Sys.time()
        catboost_elapsed <- as.numeric(difftime(catboost_end_time, catboost_start_time, units = "secs"))
        
        # Check if this was a timeout error
        if (grepl("timeout|time.*out", e$message, ignore.case = TRUE)) {
          cat(sprintf('[ERROR] CATBOOST fitting timed out after %.1f seconds (%.1f minutes): %s\n', 
                      catboost_elapsed, catboost_elapsed/60, e$message), file = model_log, append = TRUE)
          cat(sprintf('[ERROR] CATBOOST timeout - check CATBOOST_MAX_THREADS=%s\n', 
                      Sys.getenv("CATBOOST_MAX_THREADS", unset = "16")), file = model_log, append = TRUE)
        } else {
          cat(sprintf('[ERROR] CATBOOST fitting failed after %.1f seconds: %s\n', catboost_elapsed, e$message), file = model_log, append = TRUE)
        }
        fitted_model <<- NULL
      })
    } else if (model_type == "XGB") {
      # Debug XGB fitting conditions
      cat(sprintf('[DEBUG] XGB fitting - use_global_xgb: %s\n', use_global_xgb), file = model_log, append = TRUE)
      cat(sprintf('[DEBUG] XGB fitting - encoded_df available: %s\n', !is.null(encoded_df)), file = model_log, append = TRUE)
      cat(sprintf('[DEBUG] XGB fitting - encoded_vars available: %s\n', !is.null(encoded_vars)), file = model_log, append = TRUE)
      
      # PRIMARY PATH: On-the-fly encoding (most reliable)
      cat('[DEBUG] XGB Primary Path: Creating encoded data on-the-fly\n', file = model_log, append = TRUE)
      tryCatch({
        # Clean column names to remove special characters BEFORE creating recipe
        # This prevents "Misspelled variable name or in-line functions detected" errors
        clean_names <- function(names_vec) {
          gsub("[^A-Za-z0-9_]", "_", names_vec)
        }
        
        # Clean input data column names
        trn_df_clean <- trn_df
        te_df_clean <- te_df
        colnames(trn_df_clean) <- clean_names(colnames(trn_df_clean))
        colnames(te_df_clean) <- clean_names(colnames(te_df_clean))
        
        cat(sprintf('[DEBUG] XGB Primary Path: Cleaned column names before recipe creation\n'), file = model_log, append = TRUE)
        
        # Create a recipe for encoding categorical variables with cleaned data
        rec_encoded <- recipes::recipe(survival::Surv(time, status) ~ ., data = trn_df_clean) %>%
          recipes::step_dummy(recipes::all_nominal(), -recipes::all_outcomes()) %>%
          recipes::prep()
        
        trn_encoded <- recipes::juice(rec_encoded)
        te_encoded <- recipes::bake(rec_encoded, new_data = te_df_clean)
        
        # Additional cleaning of output (in case recipe adds characters)
        colnames(trn_encoded) <- clean_names(colnames(trn_encoded))
        colnames(te_encoded) <- clean_names(colnames(te_encoded))
        
        # Get encoded variable names (exclude time, status)
        encoded_vars_local <- setdiff(colnames(trn_encoded), c('time', 'status'))
        cat(sprintf('[DEBUG] XGB Primary Path: Created %d encoded variables\n', length(encoded_vars_local)), file = model_log, append = TRUE)
        
        cat(sprintf('[DEBUG] XGB Primary Path: Calling fit_xgb with %d rows, %d vars\n', nrow(trn_encoded), length(encoded_vars_local)), file = model_log, append = TRUE)
        
        # Configure XGBoost parallel processing for MC-CV
        xgb_config <- configure_xgboost_parallel(
          use_all_cores = TRUE,
          target_utilization = 0.8,
          tree_method = 'auto',
          verbose = FALSE
        )
        
        # CRITICAL: Log process state before XGB Primary Path fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_PRE_XGB_PRIMARY]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
        
        fitted_model <<- fit_xgb(trn = trn_encoded, vars = encoded_vars_local, 
                               use_parallel = TRUE, tree_method = 'auto')
        
        # Store feature names as metadata for prediction consistency
        attr(fitted_model, "xgb_feature_names") <- encoded_vars_local
        
        cat(sprintf('[DEBUG] XGB Primary Path: fit_xgb returned object of class: %s\n', paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
        
        # CRITICAL: Log process state after XGB Primary Path fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_POST_XGB_PRIMARY]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
        vars_native <- encoded_vars_local  # Update for consistency
        te_df <- te_encoded  # Update test data for performance metrics
        
      }, error = function(e) {
        cat(sprintf('[ERROR] XGB Primary Path failed: %s\n', e$message), file = model_log, append = TRUE)
        
        # FALLBACK 1: Try global encoded data if available
        if (use_global_xgb && !is.null(encoded_df) && !is.null(encoded_vars)) {
          cat('[DEBUG] XGB Fallback 1: Using global encoded data\n', file = model_log, append = TRUE)
          tryCatch({
            full_enc_space <- setdiff(colnames(encoded_df), c('time','status'))
            use_vars <- if (exists("xgb_full_flag") && xgb_full_flag) full_enc_space else encoded_vars
            cat(sprintf('[DEBUG] XGB Fallback 1: encoded_df dimensions: %d rows, %d cols\n', nrow(encoded_df), ncol(encoded_df)), file = model_log, append = TRUE)
            cat(sprintf('[DEBUG] XGB Fallback 1: using %d variables: %s\n', length(use_vars), paste(head(use_vars, 5), collapse = ", ")), file = model_log, append = TRUE)
            
            trn_enc <- encoded_df[train_idx, c('time','status', use_vars), drop = FALSE]
            te_df <- encoded_df[test_idx, c('time','status', use_vars), drop = FALSE]
            cat(sprintf('[DEBUG] XGB Fallback 1: training data dimensions: %d rows, %d cols\n', nrow(trn_enc), ncol(trn_enc)), file = model_log, append = TRUE)
            
            vars_native <- use_vars
            cat(sprintf('[DEBUG] XGB Fallback 1: calling fit_xgb...\n'), file = model_log, append = TRUE)
            
            # Configure XGBoost parallel processing for MC-CV fallback
            xgb_config <- configure_xgboost_parallel(
              use_all_cores = TRUE,
              target_utilization = 0.8,
              tree_method = 'auto',
              verbose = FALSE
            )
            
            # CRITICAL: Log process state before XGB Fallback 1 fitting
            tryCatch({
              if (exists("log_process_info", mode = "function")) {
                log_process_info(model_log, "[PROCESS_PRE_XGB_FALLBACK1]", include_children = TRUE, include_system = TRUE)
              }
            }, error = function(e) NULL)
            
            fitted_model <<- fit_xgb(trn = trn_enc, vars = use_vars, 
                                   use_parallel = TRUE, tree_method = 'auto')
            
            # Store feature names as metadata for prediction consistency
            if (!is.null(fitted_model)) {
              attr(fitted_model, "xgb_feature_names") <- use_vars
            }
            
            cat(sprintf('[DEBUG] XGB Fallback 1: fit_xgb returned: %s\n', if(is.null(fitted_model)) "NULL" else paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
            
            # CRITICAL: Log process state after XGB Fallback 1 fitting
            tryCatch({
              if (exists("log_process_info", mode = "function")) {
                log_process_info(model_log, "[PROCESS_POST_XGB_FALLBACK1]", include_children = TRUE, include_system = TRUE)
              }
            }, error = function(e) NULL)
          }, error = function(e2) {
            cat(sprintf('[ERROR] XGB Fallback 1 failed: %s\n', e2$message), file = model_log, append = TRUE)
            fitted_model <<- NULL
          })
        } else {
          # FALLBACK 2: Try numeric data directly (least likely to work)
          all_num <- all(vapply(trn_df[, vars_native, drop = FALSE], is.numeric, logical(1L)))
          cat(sprintf('[DEBUG] XGB Fallback 2: All variables numeric: %s\n', all_num), file = model_log, append = TRUE)
          
          if (all_num) {
            cat('[DEBUG] XGB Fallback 2: Using numeric data directly\n', file = model_log, append = TRUE)
            tryCatch({
              # Configure XGBoost parallel processing for MC-CV fallback 2
              xgb_config <- configure_xgboost_parallel(
                use_all_cores = TRUE,
                target_utilization = 0.8,
                tree_method = 'auto',
                verbose = FALSE
              )
              
              # CRITICAL: Log process state before XGB Fallback 2 fitting
              tryCatch({
                if (exists("log_process_info", mode = "function")) {
                  log_process_info(model_log, "[PROCESS_PRE_XGB_FALLBACK2]", include_children = TRUE, include_system = TRUE)
                }
              }, error = function(e) NULL)
              
              fitted_model <<- fit_xgb(trn = trn_df, vars = vars_native, 
                                     use_parallel = TRUE, tree_method = 'auto')
              
              # CRITICAL: Log process state after XGB Fallback 2 fitting
              tryCatch({
                if (exists("log_process_info", mode = "function")) {
                  log_process_info(model_log, "[PROCESS_POST_XGB_FALLBACK2]", include_children = TRUE, include_system = TRUE)
                }
              }, error = function(e) NULL)
            }, error = function(e3) {
              cat(sprintf('[ERROR] XGB Fallback 2 failed: %s\n', e3$message), file = model_log, append = TRUE)
              fitted_model <<- NULL
            })
          } else {
            cat('[ERROR] XGB: All fallback paths failed\n', file = model_log, append = TRUE)
            fitted_model <<- NULL
          }
        }
      })
    } else if (model_type == "CPH") {
      cat(sprintf('[DEBUG] CPH fitting - data dimensions: %d rows, %d vars\n', nrow(trn_df), length(vars_native)), file = model_log, append = TRUE)
      
      # CRITICAL: Log process state before CPH fitting
      tryCatch({
        if (exists("log_process_info", mode = "function")) {
          log_process_info(model_log, "[PROCESS_PRE_CPH]", include_children = TRUE, include_system = TRUE)
        }
      }, error = function(e) NULL)
      
  # CRITICAL FIX: Add specific timeout for CPH models (should complete in seconds, not minutes)
  cph_timeout_minutes <- as.numeric(Sys.getenv("CPH_TIMEOUT_MINUTES", unset = "5"))
  
  # ENHANCED: Check for poor split handling preferences
  skip_poor_splits <- tolower(Sys.getenv("SKIP_POOR_SPLITS", unset = "false")) %in% c("1", "true", "yes", "y")
  min_events_per_var <- as.numeric(Sys.getenv("MIN_EVENTS_PER_VAR", unset = "10"))
  
  if (skip_poor_splits) {
    # Check split quality before fitting
    n_events <- sum(trn_df$status)
    events_per_var <- n_events / length(vars_native)
    
    if (events_per_var < min_events_per_var) {
      cat(sprintf('[CPH_SKIP] Poor split detected (events/var: %.2f < %d) - skipping\n', 
                  events_per_var, min_events_per_var), file = model_log, append = TRUE)
      fitted_model <<- NULL
      return()  # Skip this split
    }
  }
      
      tryCatch({
        if (requireNamespace("R.utils", quietly = TRUE)) {
          # Use timeout protection for CPH fitting
          fitted_model <- R.utils::withTimeout({
            fit_cph(trn = trn_df, vars = vars_native, tst = NULL)
          }, timeout = cph_timeout_minutes * 60, onTimeout = "error")
        } else {
          # Fallback: use setTimeLimit
          setTimeLimit(elapsed = cph_timeout_minutes * 60)
          on.exit(setTimeLimit(elapsed = Inf), add = TRUE)
          fitted_model <- fit_cph(trn = trn_df, vars = vars_native, tst = NULL)
        }
        
        cat(sprintf('[DEBUG] CPH fitting - returned object of class: %s\n', paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
        
        # CRITICAL: Log process state after CPH fitting
        tryCatch({
          if (exists("log_process_info", mode = "function")) {
            log_process_info(model_log, "[PROCESS_POST_CPH]", include_children = TRUE, include_system = TRUE)
          }
        }, error = function(e) NULL)
      }, error = function(e) {
        if (grepl("timeout|time.*out", e$message, ignore.case = TRUE)) {
          cat(sprintf('[ERROR] CPH fitting timed out after %d minutes: %s\n', cph_timeout_minutes, e$message), file = model_log, append = TRUE)
        } else {
          cat(sprintf('[ERROR] CPH fitting failed: %s\n', e$message), file = model_log, append = TRUE)
        }
        fitted_model <<- NULL
      })
    }
    
    # Save the fitted model for this split
    cat(sprintf('[DEBUG] Cohort name for model saving: "%s"\n', cohort_name), file = model_log, append = TRUE)
    
    models_dir <- here::here('models', cohort_name)
    dir.create(models_dir, showWarnings = FALSE, recursive = TRUE)
    model_path <- file.path(models_dir, sprintf('%s_split%03d.rds', model_type, k))
    
    cat(sprintf('[DEBUG] Model save path: %s\n', model_path), file = model_log, append = TRUE)
    
    if (!is.null(fitted_model)) {
      # Additional validation for XGB models
      if (model_type == "XGB") {
        cat(sprintf('[DEBUG] XGB model object class: %s\n', paste(class(fitted_model), collapse = ", ")), file = model_log, append = TRUE)
        cat(sprintf('[DEBUG] XGB model object size: %s\n', format(object.size(fitted_model), units = "auto")), file = model_log, append = TRUE)
      }
      
      tryCatch({
        # Load dual format utility if available
        dual_format_available <- FALSE
        if (file.exists(here::here("scripts", "R", "utils", "dual_format_io.R"))) {
          tryCatch({
            source(here::here("scripts", "R", "utils", "dual_format_io.R"))
            dual_format_available <- exists("save_model_dual_format", mode = "function")
          }, error = function(e) {
            cat(sprintf('[WARNING] Could not load dual_format_io.R: %s\n', e$message), file = model_log, append = TRUE)
          })
        }
        
        # Create model metadata
        model_metadata <- list(
          model_type = model_type,
          split_id = k,
          timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
          r_version = as.character(getRversion()),
          variables_count = if (exists("vars_native")) length(vars_native) else NA,
          training_rows = if (exists("trn_df")) nrow(trn_df) else NA,
          model_class = paste(class(fitted_model), collapse = ", ")
        )
        
        if (dual_format_available) {
          # Use dual format saving (RDS + metadata CSV)
          base_path <- sub("\\.rds$", "", model_path)  # Remove .rds extension
          save_result <- save_model_dual_format(fitted_model, base_path, model_metadata)
          
          if (save_result$rds_success && file.exists(model_path)) {
            file_size <- file.size(model_path)
            cat(sprintf('[WORKER] Saved %s model (dual format) for split %d: %s (%.1f KB)\n', 
                        model_type, k, model_path, file_size/1024), file = model_log, append = TRUE)
            if (save_result$csv_success) {
              cat(sprintf('[WORKER] + Metadata saved: %s\n', 
                          paste0(base_path, "_metadata.csv")), file = model_log, append = TRUE)
            }
          } else {
            cat(sprintf('[ERROR] %s model file not found after dual format save: %s\n', 
                        model_type, model_path), file = model_log, append = TRUE)
          }
        } else {
          # Fallback to standard RDS saving
          saveRDS(fitted_model, model_path)
          
          # Verify the file was actually saved
          if (file.exists(model_path)) {
            file_size <- file.size(model_path)
            cat(sprintf('[WORKER] Saved %s model for split %d: %s (%.1f KB)\n', 
                        model_type, k, model_path, file_size/1024), file = model_log, append = TRUE)
          } else {
            cat(sprintf('[ERROR] %s model file not found after saveRDS: %s\n', 
                        model_type, model_path), file = model_log, append = TRUE)
          }
        }
      }, error = function(e) {
        cat(sprintf('[ERROR] Failed to save %s model for split %d: %s\n', 
                    model_type, k, e$message), file = model_log, append = TRUE)
      })
    } else {
      cat(sprintf('[WORKER] No model object to save for %s split %d\n', model_type, k), file = model_log, append = TRUE)
      message(sprintf("%s -- END (no model fitted)", log_prefix))
      return(list(rows = NULL, rows_uno = NULL, fi_rows = NULL))
    }

    # Compute performance metrics using utility function
    performance <- compute_model_performance(
      model_obj = fitted_model,
      test_data = te_df,
      model_type = model_type,
      split_id = k,
      horizon = horizon,
      vars_native = vars_native
    )
    
    # Compute feature importance if requested
    fi_results <- NULL
    if (do_fi) {
      fi_vars <- utils::head(vars, max_vars)
      fi_results <- compute_feature_importance_batch(
        model_obj = fitted_model,
        test_data = te_df,
        features = fi_vars,
        model_type = model_type,
        baseline_cindex = performance$baseline_cindex,
        split_id = k,
        horizon = horizon,
        vars_native = vars_native
      )
    }
    
    message(sprintf("%s -- END", log_prefix))
    list(
      rows = performance$rows,
      rows_uno = performance$rows_uno,
      fi_rows = fi_results
    )
  }

  # NOTE: CatBoost is now integrated into the main model pipeline (ORSF, CATBOOST, XGB, CPH)
  # Legacy CatBoost code removed to prevent conflicts with new implementation

  parallel_splits <- TRUE
  if (parallel_splits) {
    # ---- Ensure testing_rows exists ------------------------------------------------
    # Assumes you already have `df` and (optionally) an rsample rset in `splits`.
    suppressPackageStartupMessages(library(rsample))

    if (!exists("testing_rows", inherits = FALSE)) {
      if (exists("splits", inherits = TRUE) && inherits(splits, "rset")) {
        # Use the provided rsample object
        testing_rows <- lapply(splits$splits, function(s) {
          test_indices <- assessment(s)
          if (is.data.frame(test_indices)) {
            # assessment() returned a data.frame, we need row indices
            as.integer(as.numeric(rownames(test_indices)))
          } else if (is.matrix(test_indices)) {
            # assessment() returned a matrix, we need row indices
            as.integer(as.numeric(rownames(test_indices)))
          } else {
            # assessment() should return row indices directly
            as.integer(test_indices)
          }
        })
      } else {
        # Fallback: build Monte Carlo CV splits on the fly
        prop  <- as.numeric(Sys.getenv("MC_TEST_PROP", "0.2"))
        times <- as.integer(Sys.getenv("MC_TIMES", "20"))
        set.seed(as.integer(Sys.getenv("SEED", "42")))
        # Select a concrete data.frame source robustly
        data_for_splits <- if (inherits(df, "data.frame")) df else if (exists("final_data", inherits = TRUE) && inherits(final_data, "data.frame")) final_data else df
        data_for_splits <- ensure_mc_df(data_for_splits, model_vars)
        splits <- rsample::mc_cv(data_for_splits, times = times, prop = prop, strata = "status")
        testing_rows <- lapply(splits$splits, function(s) {
          test_indices <- assessment(s)
          if (is.data.frame(test_indices)) {
            # assessment() returned a data.frame, we need row indices
            as.integer(as.numeric(rownames(test_indices)))
          } else if (is.matrix(test_indices)) {
            # assessment() returned a matrix, we need row indices
            as.integer(as.numeric(rownames(test_indices)))
          } else {
            # assessment() should return row indices directly
            as.integer(test_indices)
          }
        })
      }
    }

    # Freeze a local copy for parallel export to avoid non-standard scoping surprises
    local_testing_rows <- testing_rows
    # Configure future plan with optimized settings for EC2
    workers_env <- suppressWarnings(as.integer(Sys.getenv('MC_SPLIT_WORKERS', unset = '0')))
    if (!is.finite(workers_env) || workers_env < 1) {
      cores <- tryCatch(as.numeric(future::availableCores()), error = function(e) parallel::detectCores(logical = TRUE))
      workers <- max(1L, floor(cores * 0.80))
    } else {
      workers <- workers_env
    }

    cat("[Step 4] Setting up parallel workers for MC-CV...\n", file = stdout())
    message(sprintf("[DIAG] Forcing cluster plan for package preloading (%d workers)", workers))
    if (exists("write_progress", mode = "function")) try(write_progress(split_done = 0, note = "Setting up parallel workers for MC-CV"), silent = TRUE)
    
    # CRITICAL: Start background process monitoring for the entire pipeline
    tryCatch({
      if (file.exists(here::here("scripts", "R", "utils", "process_monitor.R"))) {
        source(here::here("scripts", "R", "utils", "process_monitor.R"))
        
        # Create pipeline-wide process log
        pipeline_log <- here::here('logs', 'pipeline_process_monitor.log')
        
        # Log initial pipeline state
        log_process_info(pipeline_log, "[PIPELINE_START]", include_children = TRUE, include_system = TRUE)
        
        # Start background monitoring (30-second intervals)
        monitor_pid <- start_process_monitor(pipeline_log, interval_seconds = 30, duration_minutes = 0, prefix = "[PIPELINE_MONITOR]")
        if (!is.null(monitor_pid)) {
          cat(sprintf("[PROCESS_MONITOR] Started background monitoring (PID: %d) logging to: %s\n", monitor_pid, pipeline_log))
        }
      }
    }, error = function(e) {
      cat(sprintf("[PROCESS_MONITOR_ERROR] Failed to start process monitoring: %s\n", e$message))
    })

    # Ensure we have the latest versions of functions before setting up globals
    # Source the functions to make sure we're using the updated versions
    if (file.exists(here::here("scripts", "R", "fit_rsf.R"))) {
      source(here::here("scripts", "R", "fit_rsf.R"))
    }
    if (file.exists(here::here("scripts", "R", "fit_orsf.R"))) {
      source(here::here("scripts", "R", "fit_orsf.R"))
    }
    if (file.exists(here::here("scripts", "R", "fit_xgb.R"))) {
      source(here::here("scripts", "R", "fit_xgb.R"))
    }
    if (file.exists(here::here("scripts", "R", "fit_cph.R"))) {
      source(here::here("scripts", "R", "fit_cph.R"))
    }
    if (file.exists(here::here("scripts", "R", "safe_coxph.R"))) {
      source(here::here("scripts", "R", "safe_coxph.R"))
    }
    # Source parallel config files to get helper functions
    if (file.exists(here::here("scripts", "R", "ranger_parallel_config.R"))) {
      source(here::here("scripts", "R", "ranger_parallel_config.R"))
    }
    if (file.exists(here::here("scripts", "R", "xgboost_parallel_config.R"))) {
      source(here::here("scripts", "R", "xgboost_parallel_config.R"))
    }
    if (file.exists(here::here("scripts", "R", "aorsf_parallel_config.R"))) {
      source(here::here("scripts", "R", "aorsf_parallel_config.R"))
    }

    # Centralized explicit parallel plan - force cluster for package preloading
    configure_explicit_parallel(
      workers = workers,
      plan = 'cluster',  # Force cluster plan to enable package preloading
      preload_packages = c('here','recipes','dplyr','readr','survival','rsample','ranger','aorsf','riskRegression','glue','tibble','xgboost'),
      preload_sources = c('scripts/R/fit_orsf.R','scripts/R/fit_rsf.R','scripts/R/fit_xgb.R','scripts/R/fit_cph.R','scripts/R/safe_coxph.R','scripts/R/ranger_predictrisk.R','scripts/R/make_recipe.R','scripts/R/xgb_helpers.R','scripts/R/xgboost_parallel_config.R','scripts/R/aorsf_parallel_config.R','scripts/R/ranger_parallel_config.R','scripts/R/catboost_parallel_config.R')
    )

    # Capture testing_rows in a local variable for parallel export
    # Recompute local split indices to avoid scoping issues
    local_total <- length(local_testing_rows)
    local_start <- suppressWarnings(as.integer(Sys.getenv('MC_START_AT', unset = '1')))
    if (!is.finite(local_start) || local_start < 1) local_start <- 1L
    local_max <- suppressWarnings(as.integer(Sys.getenv('MC_MAX_SPLITS', unset = '0')))
    if (!is.finite(local_max) || local_max < 1) local_max <- local_total - local_start + 1L
    local_split_idx <- seq.int(from = local_start, length.out = min(local_max, local_total - local_start + 1L))
    if (!length(local_split_idx)) local_split_idx <- integer(0)

  # Flattened map over (split  model) - interleave for better distribution
  model_types <- c("ORSF","CATBOOST","XGB","CPH")
    tasks <- expand.grid(model = model_types, k = local_split_idx, KEEP.OUT.ATTRS = FALSE, stringsAsFactors = FALSE)
    # Reorder columns to match expected format (k, model)
    tasks <- tasks[, c("k", "model")]
    
    # Use very small chunk size to prevent hanging models from blocking others
    # CRITICAL FIX: Use chunk_size = 1 to ensure each task runs independently
    # This prevents RSF hanging from blocking CPH tasks
    chunk_size <- 1  # Each task runs independently
    
    # DEBUG: Print detailed task information
    cat(sprintf("[DEBUG] Total splits available: %d\n", local_total))
    cat(sprintf("[DEBUG] Split range: %d to %d\n", local_start, local_start + length(local_split_idx) - 1))
    cat(sprintf("[DEBUG] Split indices: %s\n", paste(head(local_split_idx, 10), collapse = ", ")))
    cat(sprintf("[DEBUG] Model types: %s\n", paste(model_types, collapse = ", ")))
    cat(sprintf("[DEBUG] Total tasks created: %d (should be %d splits  %d models = %d)\n", 
                nrow(tasks), length(local_split_idx), length(model_types), length(local_split_idx) * length(model_types)))
    cat(sprintf("[DEBUG] Workers: %d, Chunk size: %d (was %d)\n", workers, chunk_size, ceiling(nrow(tasks) / workers)))
    cat(sprintf("[DEBUG] Expected chunks: ~%d (more chunks = better parallelization)\n", ceiling(nrow(tasks) / chunk_size)))
    cat(sprintf("[DEBUG] First 10 tasks:\n"))
    print(head(tasks, 10))
    
    message(sprintf("[DIAG] tasks nrow: %d, chunk_size: %d", nrow(tasks), chunk_size))
    
    # Use the actual available variables from the scope
    # From the ls() output, we have: local_label, final_data, model_vars
    # Use get() to explicitly access variables, not functions
    local_label <- tryCatch(get("local_label", inherits = FALSE), error = function(e) "unknown")
    local_df <- tryCatch(get("local_df", inherits = FALSE), error = function(e) final_data)
    local_vars <- tryCatch(get("local_vars", inherits = FALSE), error = function(e) model_vars)
    
    cat("DEBUG: Using available scope variables:\n")
    cat("  local_label:", if(exists("local_label")) local_label else "not found", "\n")
    cat("  local_df:", class(local_df), "nrow:", if(is.data.frame(local_df)) nrow(local_df) else "N/A", "\n")
    cat("  local_vars:", class(local_vars), "length:", if(is.character(local_vars)) length(local_vars) else "N/A", "\n")
    # Try a minimal approach with no complex globals
    res_list <- furrr::future_map(
      seq_len(nrow(tasks)),
      function(i) {
        k <- tasks$k[i]
        model_type <- tasks$model[i]
        
        # Call the proper compute_task function with cohort info
        cohort_name <- Sys.getenv('DATASET_COHORT', unset = 'unknown')
        result <- compute_task(k, model_type, local_testing_rows, cohort_name)
        
        return(list(
          split = k, 
          model = model_type, 
          success = !is.null(result$rows), 
          message = "Completed", 
          log_file = sprintf('logs/models/%s/full/%s_split%03d.log', 
                           Sys.getenv('DATASET_COHORT', unset = 'unknown'), 
                           model_type, k)
        ))
      },
      .options = furrr::furrr_options(
        seed = TRUE,
        chunk_size = chunk_size,
        scheduling = 1.0,
        packages = c("here", "recipes", "dplyr", "readr", "survival", "rsample", "ranger", "aorsf", "xgboost", "riskRegression", "glue", "tibble"),
        globals = list(
          tasks = tasks, 
          local_testing_rows = local_testing_rows, 
          compute_task = compute_task,
          # Include required functions
          make_recipe = make_recipe,
          make_recipe_mc_cv = make_recipe_mc_cv,
          fit_orsf = fit_orsf,
          fit_catboost = fit_catboost, 
          fit_xgb = fit_xgb,
          fit_cph = fit_cph,
          safe_coxph = safe_coxph,
          predict_catboost_survival = predict_catboost_survival,
          cindex = cindex,
          cindex_uno = cindex_uno,
          compute_model_performance = compute_model_performance,
          compute_feature_importance_batch = compute_feature_importance_batch,
          compute_permutation_importance = compute_permutation_importance,
          # Include parallel processing configuration functions
          configure_catboost_parallel = configure_catboost_parallel,
          configure_xgboost_parallel = configure_xgboost_parallel,
          configure_aorsf_parallel = configure_aorsf_parallel,
          # Include parameter helper functions
          get_ranger_params = get_ranger_params,
          # Include parallel processing wrapper functions
          ranger_parallel = ranger_parallel,
          predict_ranger_parallel = predict_ranger_parallel,
          xgboost_parallel = xgboost_parallel,
          predict_xgboost_parallel = predict_xgboost_parallel,
          aorsf_parallel = aorsf_parallel,
          predict_aorsf_parallel = predict_aorsf_parallel,
          # Include XGBoost helper functions
          get_xgboost_params = get_xgboost_params,
          sgb_fit = sgb_fit,
          sgb_data = sgb_data,
          # Include aorsf helper functions
          get_aorsf_params = get_aorsf_params,
          orsf = orsf,
          # Include CatBoost helper functions
          get_catboost_params = get_catboost_params,
          configure_catboost_parallel = configure_catboost_parallel,
          # Include CPH helper functions
          get_cph_params = get_cph_params,
          configure_cph_parallel = configure_cph_parallel,
          # Include utility functions
          safe_model_predict = safe_model_predict,
          # Include required data
          df = local_df,
          vars = local_vars,
          label = local_label,
          testing_rows = local_testing_rows,
          do_fi = do_fi,
          max_vars = max_vars,
          horizon = horizon,
          use_global_xgb = use_global_xgb,
          encoded_df = encoded_df,
          encoded_vars = encoded_vars
        )
      )
    )
    
    # Combine results (simplified for now)
    if (length(res_list)) {
      cat("DEBUG: Parallel execution completed successfully!\n")
      cat("Results summary:\n")
      for (i in seq_along(res_list)) {
        result <- res_list[[i]]
        log_info <- if (!is.null(result$log_file)) sprintf(" (Log: %s)", result$log_file) else ""
        cat(sprintf("  Task %d: Split %s, Model %s, Success: %s%s\n", 
                   i, result$split, result$model, result$success, log_info))
      }
      
      # Summary of log files created
      log_files <- sapply(res_list, function(x) x$log_file)
      log_files <- log_files[!is.null(log_files)]
      if (length(log_files) > 0) {
        cat(sprintf("\nCreated %d individual model log files in logs/models/ directory\n", length(log_files)))
      }
      # Create empty result data frames for now
      mc_rows <- data.frame()
      mc_rows_uno <- data.frame()
      mc_fi_rows <- data.frame()
    }
    
    # Parallel processing completed successfully
    cat("DEBUG: Parallel processing completed successfully\n")
  } # End of if (parallel_splits)
  
  # Return results
  list(
    mc_rows = mc_rows,
    mc_rows_uno = mc_rows_uno,
    mc_fi_rows = mc_fi_rows
  )
}

# Manual concordance calculation as fallback
manual_concordance <- function(time, status, score, eval_time) {
  # Simple concordance calculation
  n <- length(time)
  if (n < 2) return(NA_real_)
  
  # Create pairs of observations
  concordant <- 0
  total_pairs <- 0
  
  for (i in 1:(n-1)) {
    for (j in (i+1):n) {
      # Only consider pairs where at least one has an event
      if (status[i] == 1 || status[j] == 1) {
        total_pairs <- total_pairs + 1
        
        # Check if predictions are concordant with outcomes
        if (status[i] == 1 && status[j] == 1) {
          # Both events: higher score should have shorter time
          if ((score[i] > score[j] && time[i] < time[j]) || 
              (score[i] < score[j] && time[i] > time[j])) {
            concordant <- concordant + 1
          }
        } else if (status[i] == 1 && status[j] == 0) {
          # i has event, j is censored: i should have higher score
          if (score[i] > score[j]) {
            concordant <- concordant + 1
          }
        } else if (status[i] == 0 && status[j] == 1) {
          # j has event, i is censored: j should have higher score
          if (score[j] > score[i]) {
            concordant <- concordant + 1
          }
        }
      }
    }
  }
  
  if (total_pairs == 0) return(NA_real_)
  return(concordant / total_pairs)
}