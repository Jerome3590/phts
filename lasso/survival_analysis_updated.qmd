---
title: "Survival Analysis - Final Binary Classification Model"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
---

```{r load-libraries}
#| echo: true
#| warning: false
#| message: false

library(here)
library(dplyr)
library(readr)
library(magrittr)
library(spatstat)
library(tibble)
library(ggplot2)
library(purrr)
library(tidyverse)
library(huxtable)
library(reticulate)
library(DT)
library(caret)
library(glmnet)
library(forcats)
library(jsonlite)
library(quarto)

(options)(scipen=999)

```

### Survival Model Dataset

```{r load-dataset}
#| echo: true
#| warning: false
#| message: false
#| eval: true

set.seed(1997)

survival_model <- read_rds(here("data","model_data_train.rds"))

survival_model %<>%
  mutate_if(is.character, as.factor) 
 
survival_model %>% 
  str()

```

```{r}

survival_model_test <- read_rds(here("data","model_data_test.rds"))

```

```{r}

cat_features <- names(survival_model)[sapply(survival_model, is.factor)]

cat_features

```

#### Utility Function for Categorical Indexes needed by CatBoost

```{python}

def get_categorical_indexes(X_train):
    # Select columns with object or categorical dtype
    categorical_columns = X_train.select_dtypes(include=['object', 'category'])

    # Get the column indexes of categorical variables
    categorical_indexes = [X_train.columns.get_loc(col) for col in categorical_columns]

    return categorical_indexes

```

### Native CatBoost Model

```{r} 

#| echo: true
#| warning: false
#| message: false
#| eval: true

# Train
train_data <- survival_model %>% 
  select(-outcome)

# Must be factor or numeric "Class"
train_target <- survival_model %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

train_Y <- survival_model$outcome


# Test
test_data <- survival_model_test %>% 
  select(-outcome)

# Must be factor or numeric "Class"
test_target <- survival_model_test %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

test_Y <- survival_model_test$outcome

```


```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false

import numpy as np

# initialize Train and Test datasets
X_train = r.train_data
y_train = r.train_Y
Y_train = np.array(y_train)  

X_test = r.test_data
y_test = r.test_Y
Y_test = np.array(y_test) 

cat_index = get_categorical_indexes(X_train)

```


```{python}

import pandas as pd

# Convert NaN values to a string in categorical columns
categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']
X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')


# Convert NaN values to a string in categorical columns
categorical_columns = [col for col in X_test.columns if X_test[col].dtype == 'object']
X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')

```

- Data Cleansing for Missing Values

```{python}

# NaN values in the column at index 15
has_nan = X_train.iloc[:, 15].isna().any()
print("Column at index 15 contains NaN values:", has_nan)

```


```{python}

if 'Missing' not in X_train.iloc[:, 15].cat.categories:
    X_train.iloc[:, 15] = X_train.iloc[:, 15].cat.add_categories(['Missing'])

# Replace NaN values in the column at index 15 with "Missing"
X_train.iloc[:, 15] = X_train.iloc[:, 15].fillna('Missing')

```

#### Optuna Hyperparameter Optimization

```{python optuna-hyperparameter-optimization, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import catboost as cb
from catboost import Pool
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "verbose": 0  # Controlling verbose output
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train, Y_train, cat_features=get_categorical_indexes(X_train))
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False, plot=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=125, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```


```{python optuna-params1}

model_params = {
    'learning_rate': 0.15,
    'depth': 3,
    'colsample_bylevel': 0.78,
    'min_data_in_leaf': 56,
    'l2_leaf_reg': 8.31
}
    
```

```{python catboost-model-auc}

#| eval: true
#| echo: true
#| message: false
#| warning: false

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import optuna
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score


cat_indexes = get_categorical_indexes(X_train)

model_auc = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               eval_metric="AUC",
                               **model_params, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               metric_period=25,
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, cat_features=cat_indexes, label=Y_train)
test_pool = Pool(X_test, cat_features=cat_indexes, label=Y_test)
 

model_auc.fit(train_pool, eval_set=test_pool)

```

#### Feature Importance

```{python}

gain = model_auc.get_feature_importance(prettified=True)
loss = model_auc.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

```{r catboost-feature-importance}
#| echo: true
#| layout-ncol: 1
#| label: tbl-feature-importance-native
#| tbl-cap: "CatBoost Feature Importance"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl <- py$gain
loss_tbl <- py$loss

table1 <- tibble( 'Feature ID' = gain_tbl$`Feature Id`,
                  'Importance' = gain_tbl$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


table2 <- tibble( 'Feature ID' = loss_tbl$`Feature Id`,
                  'Importance' = loss_tbl$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


table1
table2

```

#### Model Accuracy

```{python}

import pandas as pd

Y_Pred = model_auc.predict(X_test)
Y_Pred_Proba = model_auc.predict_proba(X_test)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = model_auc.predict_proba(X_test)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = model_auc.predict_proba(X_test)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

```

#### Uncalibrated Predictions

```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()

```

#### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

#### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

#### Calibrated Model Accuracy Using Decision Threshold

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r updated-metrics-catboost-native}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_updated <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_updated

```

### CatBoost One Hot Encode Model: Candidate Diagnosis Categorical Feature

```{r}
survival_model_one_hot <- survival_model
survival_model_one_hot_test <- survival_model_test
```

```{r}

# Create the dummy variables specification
dummies <- dummyVars(~ CAND_DIAG, data = survival_model_one_hot[, cat_features], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = survival_model_one_hot)

# Bind the new dummy variables with the original dataframe minus the original factor columns
survival_model_one_hot <- cbind(survival_model_one_hot[, !(names(survival_model_one_hot) %in% c('CAND_DIAG'))], df_dummies)

# Review the structure of the updated dataframe
# str(survival_model_one_hot)

```

```{r}

# Create the dummy variables specification
dummies <- dummyVars(~ CAND_DIAG, data = survival_model_one_hot_test[, cat_features], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = survival_model_one_hot_test)

# Bind the new dummy variables with the original dataframe minus the original factor columns
survival_model_one_hot_test <- cbind(survival_model_one_hot_test[, !(names(survival_model_one_hot_test) %in% c('CAND_DIAG'))], df_dummies)

# Review the structure of the updated dataframe
# str(survival_model_one_hot_test)

```


```{r}

survival_model_one_hot %<>% 
  rename(
    `CHD with Surgery` = `CAND_DIAG.Congenital Heart Disease With Surgery`,
    `CHD without Surgery` = `CAND_DIAG.Congenital Heart Disease Without Surgery`,
    `Dilated Cardiomyopathy` = `CAND_DIAG.Dilated Cardiomyopathy`,
    `Hypertrophic Cardiomyopathy` = `CAND_DIAG.Hypertrophic Cardiomyopathy`,
    `Myocarditis` = `CAND_DIAG.Myocarditis`,
    `CD_Other` = `CAND_DIAG.Other`,
    `Restrictive Cardiomyopathy` = `CAND_DIAG.Restrictive Cardiomyopathy`,
    `Valvular Heart Disease` = `CAND_DIAG.Valvular Heart Disease`
    
  )


survival_model_one_hot_test %<>% 
  rename(
    `CHD with Surgery` = `CAND_DIAG.Congenital Heart Disease With Surgery`,
    `CHD without Surgery` = `CAND_DIAG.Congenital Heart Disease Without Surgery`,
    `Dilated Cardiomyopathy` = `CAND_DIAG.Dilated Cardiomyopathy`,
    `Hypertrophic Cardiomyopathy` = `CAND_DIAG.Hypertrophic Cardiomyopathy`,
    `Myocarditis` = `CAND_DIAG.Myocarditis`,
    `CD_Other` = `CAND_DIAG.Other`,
    `Restrictive Cardiomyopathy` = `CAND_DIAG.Restrictive Cardiomyopathy`,
    `Valvular Heart Disease` = `CAND_DIAG.Valvular Heart Disease`
    
  )

```

```{r}

# Train
one_hot_train_data <- survival_model_one_hot %>% 
  select(-outcome)

# Must be factor or numeric "Class"
one_hot_train_target <- survival_model_one_hot %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

Y_one_hot_train <- survival_model_one_hot$outcome


# Test
one_hot_test_data <- survival_model_one_hot_test %>% 
  select(-outcome)

# Must be factor or numeric "Class"
one_hot_test_target <- survival_model_one_hot_test %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

Y_one_hot_test <- survival_model_one_hot_test$outcome

```

```{python catboost-r-data2}

#| echo: true
#| warning: false
#| message: false

import numpy as np
from catboost import Pool

# initialize data
X_train_one_hot = r.one_hot_train_data
Y = np.array(r.Y_one_hot_train)


X_test_one_hot = r.one_hot_test_data
Y_test = np.array(r.Y_one_hot_test)

cat_index_one_hot = get_categorical_indexes(X_train_one_hot)


if 'Missing' not in X_train_one_hot.iloc[:, 15].cat.categories:
    X_train_one_hot.iloc[:, 15] = X_train_one_hot.iloc[:, 15].cat.add_categories(['Missing'])

# Replace NaN values in the column at index 15 with "Missing"
X_train_one_hot.iloc[:, 15] = X_train_one_hot.iloc[:, 15].fillna('Missing')


if 'Missing' not in X_test_one_hot.iloc[:, 15].cat.categories:
    X_test_one_hot.iloc[:, 15] = X_test_one_hot.iloc[:, 15].cat.add_categories(['Missing'])

# Replace NaN values in the column at index 15 with "Missing"
X_test_one_hot.iloc[:, 15] = X_test_one_hot.iloc[:, 15].fillna('Missing')


```

#### Hyperparameter Optimization with Optuna - One Hot Model

```{python optuna-hyperparameter-optimization-one-hot, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false

import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import catboost as cb
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "early_stopping_rounds": 100
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train_one_hot, Y, cat_features=get_categorical_indexes(X_train_one_hot))
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=125, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```

- From Optuna Trial #103/125

```{python optuna-params2}

model_params2 = {
    'learning_rate': 0.09,
    'depth': 2,
    'colsample_bylevel': 0.21,
    'min_data_in_leaf': 22,
    'l2_leaf_reg': 10.3
}
    
```

#### One Hot Encode Candidate Diagnosis Model

```{python catboost-model-logloss-one-hot-encoding}

#| eval: true
#| echo: true
#| message: false
#| warning: false

from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier, Pool


survival_model_one_hot = CatBoostClassifier(iterations=8000,
                               objective='Logloss',
                               eval_metric='AUC',
                               **model_params2, 
                               boosting_type= 'Ordered',
                               metric_period=500,
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train_one_hot, Y, cat_features=get_categorical_indexes(X_train_one_hot))
test_pool = Pool(X_test_one_hot, Y_test, cat_features=get_categorical_indexes(X_test_one_hot))
 

survival_model_one_hot.fit(train_pool, eval_set=test_pool)

gain_one_hot = survival_model_one_hot.get_feature_importance(prettified=True)
loss_one_hot = survival_model_one_hot.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

#### Feature Importance with Candidate Diagnosis One Hot Encoding

```{r feature-importance-one-hot}
#| echo: true
#| layout-ncol: 1
#| label: tbl-feature-importance-one-hot
#| tbl-cap: "CatBoost Feature Importance with One-Hot Encoding"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl <- py$gain_one_hot
loss_tbl <- py$loss_one_hot

table1 <- tibble( 'Feature ID' = gain_tbl$`Feature Id`,
                  'Importance' = gain_tbl$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


table2 <- tibble( 'Feature ID' = loss_tbl$`Feature Id`,
                  'Importance' = loss_tbl$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


table1
table2

```

#### One-Hot Encoded Model Accuracy

```{python}

import pandas as pd

Y_Pred = survival_model_one_hot.predict(X_test_one_hot)
Y_Pred_Proba = survival_model_one_hot.predict_proba(X_test_one_hot)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = survival_model_one_hot.predict_proba(X_test_one_hot)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = survival_model_one_hot.predict_proba(X_test_one_hot)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

# Converting predictions and actuals into a DataFrame for better readability
one_hot_predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

```

#### Uncalibrated Predictions

```{r message=FALSE, warning=FALSE}

library(probably)

one_hot_predictions <- py$one_hot_predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
one_hot_predictions$Class <- factor(one_hot_predictions$Class, levels = rev(factor_levels))

one_hot_predictions %>% 
  datatable()

```

#### One-Hot Encoded Calibration Plot - CatBoost

```{r}

one_hot_predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

#### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
one_hot_roc_result <- roc(one_hot_predictions$Actual, one_hot_predictions$Prob_Positive_Class)

one_hot_coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
one_hot_optimal_threshold <- one_hot_coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
one_hot_predictions$predicted_classes <- ifelse(one_hot_predictions$Prob_Positive_Class >= one_hot_optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", one_hot_optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(one_hot_predictions$predicted_classes), "\n")

```

#### Final Calibrated Model Accuracy

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
one_hot_f1_updated = f1_score(r.one_hot_predictions["Actual"], r.one_hot_predictions["predicted_classes"])
one_hot_precision_updated = precision_score(r.one_hot_predictions["Actual"], r.one_hot_predictions["predicted_classes"])
one_hot_recall_updated = recall_score(r.one_hot_predictions["Actual"], r.one_hot_predictions["predicted_classes"])
one_hot_accuracy_updated = accuracy_score(r.one_hot_predictions["Actual"], r.one_hot_predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
one_hot_model_auc_updated = roc_auc_score(r.one_hot_predictions["Actual"], r.one_hot_predictions["Prob_Positive_Class"])
one_hot_conf_matrix_updated = confusion_matrix(r.one_hot_predictions["Actual"], r.one_hot_predictions["predicted_classes"])

```

```{r accuracy-metrics-one-hot-calibrated}

#| echo: true
#| warning: false
#| message: false
#| eval: false

one_hot_model_f1_score <- py$one_hot_f1_updated
one_hot_model_recall <- py$one_hot_recall_updated
one_hot_model_precision <- py$one_hot_precision_updated
one_hot_model_accuracy <- py$one_hot_accuracy_updated
one_hot_model_roc_score <- py$one_hot_model_auc_updated

model_accuracy_one_hot <- list(
  F1_Score = one_hot_model_f1_score,
  Recall = one_hot_model_recall,
  Precision = one_hot_model_precision,
  Accuracy = one_hot_model_accuracy,
  ROC_Score = one_hot_model_roc_score
)

model_accuracy_one_hot

```

```{r catboost-confusion-matrix}

catboost_confusion_matrix <- confusionMatrix(factor(one_hot_predictions$predicted_classes), factor(one_hot_predictions$Actual), positive = "1")

print(catboost_confusion_matrix)

```

### SHAP Value Analysis

```{python shap-setup}

#| eval: false
#| echo: true
#| message: false
#| warning: false

import shap

explainer = shap.TreeExplainer(survival_model_one_hot)
shap_values = explainer(X_train_one_hot)
    
```

#### Beeswarm (Top Features - Categorical and Numerical)

```{python}

mean_abs_shap_values = np.mean(np.abs(shap_values.values), axis=0)

top_n = 20
top_indices = np.argsort(-mean_abs_shap_values)[:top_n]  # Indices of the top features

filtered_shap_values = shap_values[:, top_indices]

```

```{python shap-setup2}
#| eval: false
#| echo: true
#| message: false
#| warning: false

plt.close()

import matplotlib.pyplot as plt
import shap

shap.initjs()


shap.plots.beeswarm(filtered_shap_values,20)

plt.xlabel('', fontsize=8)
plt.ylabel('', fontsize=8)
plt.title('', fontsize=8)
plt.tight_layout()

plt.savefig('images/shapley_beeswarm.png', dpi=1200)

```

![Beeswarm Chart](images/shapley_beeswarm.png){#fig-shap-beeswarm-chart}

#### Bar Chart for Feature Importance

```{python shapley-bar-chart}
#| eval: false
#| echo: true
#| warning: false
#| message: false
#| label: fig-net-effect
#| fig-cap: 
#|   - "Net Effect"

import matplotlib.pyplot as plt
import shap

shap.initjs()

plt.close()
plt.close()

shap.plots.bar(filtered_shap_values, max_display=20)

plt.tight_layout()

# Save the image in high resolution
plt.savefig('images/shapley_bar_one_hot.png', dpi=1200)

```

![Feature Importance Chart](images/shapley_bar_one_hot.png){#fig-shap-numerical-beeswarm-chart}

```{r}
#| eval: false
#| echo: true
#| warning: false
#| message: false

# Numeric Column Indexes
numeric_column_indexes <- which(sapply(one_hot_train_data , is.numeric))

# Convert to Python index format (zero-based)
python_numeric_column_indexes <- numeric_column_indexes - 1

# Factor Column Names
factor_column_names <- names(one_hot_train_data)[sapply(one_hot_train_data, is.factor)]

# Numeric Column Names
numeric_column_names <- names(one_hot_train_data)[sapply(one_hot_train_data, is.numeric)]


```

#### Beeswarm Top Numerical Features

```{python beeswarm-numerical-features}
#| eval: false
#| echo: true
#| warning: false
#| message: false
#| out-width: 70%
#| out-height: 70%
#| fig-cap: 
#|   - "Beeswarm Numerical Effects"

import matplotlib.pyplot as plt
import shap
import pickle

shap.initjs()

plt.close()
plt.close()

num_features = r.numeric_column_names

# Subset the SHAP values to only include the numerical features
numerical_shap_values = shap_values[:, num_features]

# Plot the beeswarm for only the numerical features
shap.plots.beeswarm(numerical_shap_values,20)

plt.tight_layout()

# Save the figure with desired DPI
plt.savefig('images/shapley_beeswarm_numerical.png', dpi=1200, bbox_inches="tight")

```

![Numerical Beeswarm Chart](images/shapley_beeswarm_numerical.png){#fig-shap-numerical-beeswarm-chart}

#### SHAP Value Feature Importance Correlation Plot

```{python correlation-plot}
#| eval: false
#| echo: true
#| warning: false
#| message: false
#| out-width: 70%
#| out-height: 70%
#| fig-cap: 
#|   - "SHAP Value Correlation Plot"

plt.close()

import seaborn as sns
import matplotlib.pyplot as plt

# SHAP correlation plot
corr_matrix = pd.DataFrame(numerical_shap_values.values, columns=num_features).corr()

sns.set(font_scale=.2)
sns.heatmap(corr_matrix,cmap="coolwarm", center=0, annot=True, fmt =".1g")

plt.tight_layout()

# Save the figure with desired DPI
plt.savefig('images/shap_correlation_plot.png', dpi=1200, bbox_inches="tight")

```

![SHAP Value Correlation plot Beeswarm Chart](images/shap_correlation_plot.png){#fig-shap-correlation-plot}

#### SHAP Value Feature Importance - Mean Absolute Value

```{r shap-values, eval=FALSE}

# SHAP values
shap_values <- py$shap_values

# Extract SHAP values for each feature (excluding the last column which is the expected value)
shap_values_matrix <- shap_values$values

# Convert SHAP values to a dataframe for easier analysis
shap_df <- as.data.frame(shap_values_matrix)
names(shap_df) <- shap_values$feature_names

```

```{r mean-absolute-shap-values, eval=FALSE}

features <- names(one_hot_train_data)

mean_abs_shap_values <- colMeans(abs(shap_df))  # Compute mean absolute SHAP values

```

```{r radar-chart, eval=FALSE}

library(plotly)

# Function to generate plot based on a threshold
generate_radar_plot <- function(threshold) {
  indices <- which(mean_abs_shap_values > threshold)
  data <- data.frame(
    r = mean_abs_shap_values[indices],
    theta = features[indices]
  )

  p <- plot_ly(data, type = 'scatterpolar', fill = 'toself',
               r = ~r, theta = ~theta, mode = 'lines+markers',
               marker = list(size = 5)) %>%
    layout(
      polar = list(
        radialaxis = list(
          visible = T,
          range = c(0, max(data$r))
        )
      )
    )
  return(p)
}

# Plot
threshold <- .025
plot <- generate_radar_plot(threshold)


# Print the plot
plot

```
![SHAP Mean Absolute Value Radar Chart](images/radar_chart.png){#fig-shap-radar-chart}

#### SHAP Value Radar Chart Feature Importance - Mean Positive and Mean Negative Values 

Negative SHAP values in a binary classification where "1" is positive indicate that the feature decreases the probability of the positive outcome. 

Low values for Median # Refusals and low values for Median Wait Days - pushes prediction to the '0' class - 'Survival'.

This visualization represents the positive and negative values for the 'Positive' Class ('1'). For situations that are based on both clasess (ie. One hot encoded values where SHAP value refers to the presence or absence of value) a Beeswarm chart is more appropriate. (See Dilated Cardiomyopathy for example - high positive SHAP value for absence of feature)


```{r pos-neg-shap-values, eval=FALSE}

# Model Features
features <- names(shap_df)

# Mean SHAP values for each feature
mean_shap_values <- colMeans(shap_df)

# Separate positive and negative SHAP values
positive_shap_values <- mean_shap_values
negative_shap_values <- mean_shap_values

positive_shap_values[positive_shap_values < 0] <- 0
negative_shap_values[negative_shap_values > 0] <- 0

# Make negative SHAP values positive for visualization purposes
negative_shap_values <- abs(negative_shap_values)

```

```{r radar-chart-positive-negative, eval=FALSE}

library(plotly)

# Function to generate radar plot with positive and negative SHAP values
generate_radar_plot_pos_neg <- function(threshold) {
  # Filter features based on the threshold for mean absolute SHAP values
  indices <- which(colMeans(abs(shap_df)) > threshold)
  
  # Prepare data for plotly
  data_positive <- data.frame(
    r = positive_shap_values[indices],
    theta = features[indices],
    group = "Positive SHAP Values"
  )
  
  data_negative <- data.frame(
    r = negative_shap_values[indices],
    theta = features[indices],
    group = "Negative SHAP Values"
  )
  
  # Combine the data
  data_plot <- rbind(data_positive, data_negative)
  
  # Create the radar plot
  p <- plot_ly(data_plot, type = 'scatterpolar', fill = 'toself',
               r = ~r, theta = ~theta, color = ~group,
               mode = 'lines+markers',
               marker = list(size = 5)) %>%
    layout(
      polar = list(
        radialaxis = list(
          visible = TRUE,
          range = c(0, max(data_plot$r))
        )
      )
    )
  return(p)
}

# Define a threshold for SHAP value significance
threshold <- 0.025

# Generate and print the plot
plot <- generate_radar_plot_pos_neg(threshold)
plot

```
![SHAP Mean Absolute Value Radar Chart - Positive and Negative](images/radar_chart2.png){#fig-shap-radar-chart-positive-negative}
