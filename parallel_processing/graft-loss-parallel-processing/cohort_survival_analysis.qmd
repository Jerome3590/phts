---
title: "Survival Analysis Models by Cohort"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 6
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
editor:
  markdown:
    wrap: 72
---

## Overview

This analysis examines predictive models for pediatric heart transplant
outcomes by comparing two distinct patient cohorts: Congenital Heart
Disease (CHD) and Myocarditis/Cardiomyopathy. We implement three
complementary modeling approaches to provide comprehensive risk
assessment and identify cohort-specific predictive factors.

### Modeling Approaches

This study employs three distinct machine learning methodologies, each
offering unique advantages for pediatric heart transplant outcome
prediction:

**1. LASSO (Least Absolute Shrinkage and Selection Operator) with
Survival**

-   **Method**: A Cox proportional hazards model regularized with an L1
    penalty for feature selection and time-to-event prediction.
-   **Strengths for Our Use Case**:
    -   **Clinical Interpretability**: Provides clear coefficient values
        (log-hazard ratios) that clinicians can easily understand.
    -   **Feature Selection**: Automatically identifies the most
        impactful variables on survival, reducing model complexity.
    -   **Transparent Decision Making**: Linear relationships in the
        log-hazard allow for straightforward risk factor quantification.
    -   **Regulatory Compliance**: The Cox model is a well-established
        method with extensive validation in medical literature.
    -   **Handles Censoring**: Natively incorporates censored data into
        the partial likelihood calculation, making full use of the data.
-   **Limitations**:
    -   **Proportional Hazards Assumption**: Assumes that the effect of
        a predictor is constant over time, which may not always hold
        true.
    -   **Linear Assumptions**: Models the log-hazard as a linear
        combination of predictors, potentially missing complex
        non-linear relationships.
    -   **Feature Interactions**: Limited ability to capture
        interactions between clinical variables without explicit
        specification.

**2. CatBoost (Categorical Boosting)**

-   **Method**: Gradient boosting survival model using a Cox
    proportional hazards loss function for time-to-event prediction.
-   **Strengths for Our Use Case**:
    -   **Categorical Handling**: Excellent performance with medical
        coding systems (diagnoses, procedures) without extensive
        preprocessing.
    -   **Non-linear Relationships**: Captures complex interactions
        between donor, recipient, and procedural factors.
    -   **Robust Performance**: Advanced regularization techniques
        prevent overfitting in medical datasets.
    -   **Missing Data Tolerance**: Handles incomplete medical records
        gracefully.
    -   **Survival Analysis**: The Cox loss function properly models
        time-to-event outcomes.
-   **Limitations**:
    -   **Black Box Nature**: Less interpretable than a standard Cox
        model for individual patient risk explanation.
    -   **Computational Complexity**: Requires more computational
        resources and hyperparameter tuning.
    -   **Cox Assumptions**: Still relies on the proportional hazards
        assumption inherent in the Cox loss function.

**3. AORSF (Accelerated Oblique Random Survival Forests)**

-   **Method**: Ensemble survival analysis using oblique decision trees
    with time-to-event modeling.
-   **Strengths for Our Use Case**:
    -   **Survival Analysis**: Properly models time-to-event outcomes,
        capturing both event occurrence and timing.
    -   **C-index Performance**: Optimized for concordance statistics,
        ideal for risk stratification.
    -   **Complex Interactions**: The tree-based approach naturally
        captures multi-way interactions between clinical variables.
    -   **Non-parametric**: Makes no assumptions about underlying hazard
        functions or distributions.
    -   **Ensemble Robustness**: Multiple trees provide stable
        predictions across different patient populations.
-   **Limitations**:
    -   **Interpretability**: Tree ensemble methods are less
        interpretable than linear models for individual predictions.
    -   **Computational Intensity**: More complex than traditional
        survival methods, requiring careful parameter tuning.

### Cohort-Specific Analysis Rationale

The analysis compares CHD patients (congenital conditions) versus
Myocarditis/Cardiomyopathy patients (acquired conditions) because:

-   **Different Pathophysiology**: Congenital versus acquired heart
    disease may respond differently to transplantation.
-   **Age-Related Factors**: CHD patients are often transplanted at
    younger ages with different growth considerations.
-   **Surgical Complexity**: CHD cases may involve more complex anatomy
    requiring different risk stratification.
-   **Long-term Outcomes**: Different disease etiologies may have
    distinct long-term survival patterns.
-   **Clinical Decision Making**: Cohort-specific models may provide
    more accurate risk prediction for treatment planning.

This multi-method survival analysis approach allows for comprehensive
model validation, with LASSO providing interpretable linear
relationships, CatBoost capturing complex non-linear patterns, and AORSF
incorporating advanced, non-parametric time-to-event dynamics. All three
methods now utilize the full temporal information available in
transplant outcomes, providing optimal clinical decision support through
consistent C-index performance metrics.

### Censored Data Handling

Each modeling approach handles censored observations appropriately:

-   **LASSO**: Uses `Surv(time, status)` objects where status=0
    indicates censored observations, properly incorporated into the Cox
    regression partial likelihood.

-   **CatBoost**: The Cox loss function incorporates censored data by
    encoding the label with a negative sign, correctly accounting for
    patients who were alive at last follow-up.

-   **AORSF**: Ensemble survival forests handle censoring through
    modified splitting criteria that account for incomplete observations
    using proper survival analysis methodology.

All methods properly utilize both event times and censoring information,
ensuring that patients lost to follow-up or alive at study end
contribute appropriate information to model training.

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(readr)
library(rlang)
library(dplyr)
library(glmnet)
library(caret)
library(tidyverse)
library(tibble)
library(DT)
library(aorsf)
library(survival)
library(catboost)

set.seed(1997)

```

### Model Data

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Load the preprocessed modeling dataset
model_data <- read_csv("preprocessed_model_data.csv", show_col_types = FALSE)

# Check for survival variables and identify available columns
available_time <- names(model_data)[grep("time|Time|TIME", names(model_data))]
available_event <- names(model_data)[grep("event|Event|EVENT|type|Type|TYPE|status|Status|STATUS", names(model_data))]

cat("Available time columns:", paste(available_time, collapse = ", "), "\n")
cat("Available event columns:", paste(available_event, collapse = ", "), "\n")

# Verify the data structure
cat("Dataset dimensions:", paste(dim(model_data), collapse = " x "), "\n")
cat("Columns:", paste(names(model_data), collapse = ", "), "\n")

if(!("ev_time" %in% names(model_data)) && length(available_time) == 1) {
  model_data <- model_data %>% rename(ev_time = !!sym(available_time))
}
if(!("outcome" %in% names(model_data)) && length(available_event) == 1) {
  model_data <- model_data %>% rename(outcome = !!sym(available_event))
}

# Normalize types
model_data <- model_data %>%
  mutate(
    outcome = as.integer(outcome),
    ev_time = suppressWarnings(as.numeric(ev_time))
  )

# Median among censored rows with valid (>0) times; fallback to overall; then tiny epsilon
med_censored_time <- model_data %>%
  filter(outcome == 0L, is.finite(ev_time), ev_time > 0) %>%
  summarise(med = median(ev_time, na.rm = TRUE)) %>% pull(med)

if (!is.finite(med_censored_time) || is.na(med_censored_time)) {
  med_censored_time <- model_data %>%
    filter(is.finite(ev_time), ev_time > 0) %>%
    summarise(med = median(ev_time, na.rm = TRUE)) %>% pull(med)
}
if (!is.finite(med_censored_time) || is.na(med_censored_time)) {
  med_censored_time <- 1 / (365.25 * 24 * 60)  # ~1 minute in years
}

# Replace bad times for censored rows
model_data <- model_data %>%
  mutate(
    ev_time_replaced = outcome == 0L & is.finite(ev_time) & ev_time <= 0,
    ev_time = if_else(outcome == 0L & (is.na(ev_time) | ev_time <= 0),
                      med_censored_time, ev_time)
  )

cat("Replaced", sum(model_data$ev_time_replaced, na.rm = TRUE),
    "censored non-positive ev_time values with median =", med_censored_time, "\n")


```

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

model_features <- model_data %>% 
  colnames() %>% 
  as_tibble()

# View 
datatable(
  model_features,
  rownames = FALSE,
  options = list(
    pageLength = 15,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)
```


### Methods: Cohort Pipeline Summary

-   **Data**: Load `preprocessed_model_data.csv` into `model_data`.

-   **Data Integration**: Incorporate post-transplant follow-up data
    from `posttxplfollowup.sas7bdat` to engineer survival analysis
    features. This includes calculating `days_to_last_followup` and
    joining with the main transplant dataset to create comprehensive
    survival outcomes.

-   **Cohorts**: Define `chd_data`
    (`primary_etiology == "Congenital HD"`) and `myo_cardio_data`
    (`primary_etiology %in% c("Cardiomyopathy", "Myocarditis")`).

-   **Feature filtering (survival models)**: Create `*_surv_model` by
    removing variables matching `survival_lagging_keywords` for all
    models. Exclude identifiers, `outcome`, and `transplant_year` from
    predictors.

-   **Unified train/test split**: Use
    `create_unified_train_test_split()` function with `set.seed(1997)`
    for reproducible 80/20 splits across all models.

-   **Reproducibility**: Set `set.seed(1997)` before each model fit.

-   **LASSO (survival)**: `cv.glmnet(..., family = "cox", alpha = 1)`
    using Cox proportional hazards with L1 penalty. Features exclude
    identifiers, `outcome`, `ev_time`, `ev_type`. Evaluation: C-index;
    report feature importance with actual feature names.

-   **CatBoost (survival)**: Gradient boosting with Cox loss
    (`loss_function = "Survival: Cox"`, fallback to `"Cox"`).
    Evaluation: C-index; report feature importance with normalized
    values from 0 to 1 for all models.

-   **AORSF (survival)**: `orsf(Surv(time, status) ~ .)` after deriving
    `time = ev_time`, `status = as.integer(ev_type == 1)`. Remove
    constant columns; exclude identifiers and `outcome` from predictors.
    Evaluation: C-index; report feature importance with normalized
    values from 0 to 1 for all models.

-   **Evaluation and reporting**: Predictions and metrics computed on
    the held-out test set. Tabular outputs rendered with
    `DT::datatable`.

-   **Comparison tables**: Aggregate metrics across cohorts and models
    into summary tables for side-by-side comparison.

**Unified Survival Modeling**: All three modeling frameworks (LASSO,
CatBoost, and AORSF) now use survival analysis approaches, for direct
comparison of C-index metrics and consistent evaluation of time-to-event
outcomes across different modeling frameworks.

```{r survival-lagging-keywords}

# Unified survival modeling keywords (variables to exclude from all models)

survival_lagging_keywords <- c(
  # Identifiers and outcomes
  #"ptid_e",
  "transplant_year", "primary_etiology",
  
  # Survival variables (handled separately)
  # "ev_time", "ev_type",
  
  # Donor-specific variables
  "graft_loss", "int_graft_loss", "dtx_", "cc_", "isc_oth",
  "dcardiac", "dcon", "dpri", "dpricaus", "rec_", "papooth",
  "dneuro", "sdprathr", "int_dead", "listing_year", "cpathneg",
  "dcauseod",
  
  # Demographics (if not clinically relevant)
  "race", "sex", "drace_b", "rrace_a", "hisp", "Iscntry",
  
  # Transplant-specific variables
  "dreject", "dsecaccsEmpty", "dmajbldEmpty", "pishltgr1R", 
  "drejectEmpty", "drejectHyperacute", "pishltgrEmpty",
  "pishltgr", "dmajbld", "dsecaccs", "dsecaccs_bin", 
  
  # Clinical variables to exclude
  "dx_cardiomyopathy", "deathspc", "dlist", "pmorexam", 
  "patsupp", "concod", "pcadrem", "pcadrec", "pathero", 
  "pdiffib", "dmalcanc", "alt_tx", "age_death", "pacuref",
  
  # Additional variables
  "lsvcma"
)


```

```{r unified-train-test-split}

# Unified train/test split function for all models
create_unified_train_test_split <- function(data, cohort_name, seed = 1997) {
  set.seed(seed)
  
  # Create reproducible random split
  n_total <- nrow(data)
  n_train <- floor(0.8 * n_total)
  
  # Create random indices
  all_indices <- 1:n_total
  train_indices <- sample(all_indices, size = n_train)
  test_indices <- setdiff(all_indices, train_indices)
  
  # Split the data
  train_data <- data[train_indices, ]
  test_data <- data[test_indices, ]
  
  # Store indices for other models to use
  split_info <- list(
    cohort = cohort_name,
    train_indices = train_indices,
    test_indices = test_indices,
    n_total = n_total,
    n_train = n_train,
    n_test = length(test_indices),
    seed = seed
  )
  
  cat("=== Unified Train/Test Split for", cohort_name, "===\n")
  cat("Total patients:", n_total, "\n")
  cat("Training set:", n_train, "patients\n")
  cat("Test set:", length(test_indices), "patients\n")
  cat("Split ratio:", round(n_train/n_total, 3), ":", round(length(test_indices)/n_total, 3), "\n")
  cat("Seed used:", seed, "\n")
  cat("=====================================\n\n")
  
  return(list(
    train_data = train_data,
    test_data = test_data,
    split_info = split_info
  ))
}
```

```{r catboost-clean-target}

clean_survival_data_for_catboost <- function(data, time_col = "ev_time", status_col = "outcome") {
  
  # Check for problematic values
  cat("=== Data Quality Check ===\n")
  cat("Total rows:", nrow(data), "\n")
  cat("NaN in time:", sum(is.nan(data[[time_col]])), "\n")
  cat("NaN in status:", sum(is.nan(data[[status_col]])), "\n")
  cat("Inf in time:", sum(is.infinite(data[[time_col]])), "\n")
  cat("Negative time:", sum(data[[time_col]] < 0, na.rm = TRUE), "\n")
  cat("Zero time:", sum(data[[time_col]] == 0, na.rm = TRUE), "\n")
  cat("Missing time:", sum(is.na(data[[time_col]])), "\n")
  cat("Missing status:", sum(is.na(data[[status_col]])), "\n")
  
  # Clean the data
  cleaned_data <- data %>%
    # Remove rows with NaN or infinite values
    filter(!is.nan(!!sym(time_col))) %>%
    filter(!is.infinite(!!sym(time_col))) %>%
    filter(!is.nan(!!sym(status_col))) %>%
    filter(!is.infinite(!!sym(status_col))) %>%
    # Remove rows with negative or zero time
    filter(!!sym(time_col) > 0) %>%
    # Remove rows with missing values
    filter(!is.na(!!sym(time_col))) %>%
    filter(!is.na(!!sym(status_col))) %>%
    # Ensure status is binary (0 or 1)
    filter(!!sym(status_col) %in% c(0, 1))
  
  cat("Cleaned rows:", nrow(cleaned_data), "\n")
  cat("Rows removed:", nrow(data) - nrow(cleaned_data), "\n")
  
  return(cleaned_data)
}

```

```{r lasso-utilities, warning=FALSE, message=FALSE}


nzv_cols <- function(df) {
  vapply(df, function(x) {
    ux <- unique(x)
    ux <- ux[!is.na(ux)]
    length(ux) < 2
  }, logical(1))
}


mode_level <- function(x) {
  ux <- na.omit(x)
  if (length(ux) == 0) return(NA)
  tab <- sort(table(ux), decreasing = TRUE)
  names(tab)[1]
}


impute_like <- function(df, like_df) {
  for (nm in names(df)) {
    if (is.numeric(df[[nm]])) {
      m <- median(like_df[[nm]], na.rm = TRUE)
      df[[nm]][is.na(df[[nm]])] <- m
    } else if (is.factor(df[[nm]])) {
      # ensure same levels; add "Other"
      base_lv <- levels(like_df[[nm]])
      if (!("Other" %in% base_lv)) base_lv <- c(base_lv, "Other")
      df[[nm]] <- factor(df[[nm]], levels = base_lv)
      # unseen -> NA -> "Other"
      df[[nm]][is.na(df[[nm]])] <- "Other"
    }
  }
  df
}


align_mm <- function(train_df, test_df) {
  Xtr <- model.matrix(~ . - 1, data = train_df)
  Xte <- model.matrix(~ . - 1, data = test_df)
  # pad/reorder test to train columns
  missing_in_test <- setdiff(colnames(Xtr), colnames(Xte))
  if (length(missing_in_test)) {
    Xte <- cbind(Xte, matrix(0, nrow(Xte), length(missing_in_test),
                             dimnames = list(NULL, missing_in_test)))
  }
  extra_in_test <- setdiff(colnames(Xte), colnames(Xtr))
  if (length(extra_in_test)) Xte <- Xte[, setdiff(colnames(Xte), extra_in_test), drop = FALSE]
  Xte <- Xte[, colnames(Xtr), drop = FALSE]
  list(Xtr = Xtr, Xte = Xte)
}

```

```{r metrics-helper-functions}

# Helper function to create comprehensive metrics for any model

create_survival_metrics <- function(cohort_name, model_name, concordance_obj) {
  
  # Check if the concordance object is valid
  if (is.null(concordance_obj) || !("concordance" %in% names(concordance_obj))) {
    warning(paste("Invalid concordance object for", model_name, cohort_name))
    return(NULL)
  }
  
  # Initialize the metrics list
  metrics <- list(
    Cohort = cohort_name,
    Model = model_name,
    C_Index = round(as.numeric(concordance_obj$concordance), 4)
    # AUC, Accuracy, F1, etc., are not calculated as they are for classification
  )
  
  return(metrics)
}


# Helper function to get top N features with normalized importance
get_top_features_normalized <- function(feature_df, cohort_name, model_name, n_features = 25) {
  if (nrow(feature_df) == 0) return(NULL)
  
  # Get top N features
  top_features <- feature_df %>%
    slice_head(n = n_features) %>%
    mutate(
      Cohort = cohort_name,
      Model = model_name,
      Rank = row_number()
    )
  
  # Normalize importance to 0-1 scale
  if ("importance" %in% colnames(top_features)) {
    top_features <- top_features %>%
      mutate(
        Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
      )
  } else if ("coefficient" %in% colnames(top_features)) {
    # For LASSO coefficients, use absolute values and normalize
    top_features <- top_features %>%
      mutate(
        importance = abs(coefficient),
        Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
      )
  }
  
  return(top_features)
}


# Helper function to create metrics summary table
create_metrics_summary <- function(metrics_list) {
  if (length(metrics_list) == 0) return(NULL)
  
  # Convert list of metrics to data frame
  metrics_df <- bind_rows(metrics_list)
  
  # Create summary by cohort and model
  summary_df <- metrics_df %>%
    group_by(Cohort, Model) %>%
    summarise(
      AUC = first(AUC),
      C_Index = first(C_Index),
      Accuracy = first(Accuracy),
      F1 = first(F1),
      Precision = first(Precision),
      Recall = first(Recall),
      Method = first(Method),
      .groups = 'drop'
    )
  
  return(list(
    full_metrics = metrics_df,
    summary = summary_df
  ))
}

```

### Cohort-Specific Modeling Pipeline

#### CHD Cohort Analysis

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_data <- model_data %>% filter(primary_etiology == "Congenital HD")

# CHD Cohort summary
cat("CHD Cohort Size:", nrow(chd_data), "patients\n")
cat("Event Distribution (outcome):\n")
print(table(chd_data$outcome))
cat("Event Rate:", round(mean(chd_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

# CHD Cohort descriptive statistics
chd_summary <- chd_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), list(
    mean = ~ round(mean(.x, na.rm = TRUE), 2),
    median = ~ round(median(.x, na.rm = TRUE), 2),
    sd = ~ round(sd(.x, na.rm = TRUE), 2)
  ))) %>%
  pivot_longer(everything(), names_to = "stat", values_to = "value") %>%
  separate(stat, into = c("variable", "statistic"), sep = "_(?=[^_]*$)") %>%
  pivot_wider(names_from = statistic, values_from = value)

# Display CHD summary table
datatable(
  chd_summary,
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

```{r}

# Apply cleaning cohort
chd_catboost_df <- clean_survival_data_for_catboost(chd_data)

# Create unified train/test split for CHD cohort
chd_split <- create_unified_train_test_split(chd_data, "CHD", seed = 1997)

# Extract split info
chd_train_indices <- chd_split$split_info$train_indices
chd_test_indices <- chd_split$split_info$test_indices
chd_train_data <- chd_split$train_data
chd_test_data <- chd_split$test_data

# Verify the cleaned data
cat("\n=== CHD Cohort Cleaned Data ===\n")
summary(chd_catboost_df$ev_time)
summary(chd_catboost_df$outcome)


# Check for any remaining issues
cat("\n=== Final Quality Check ===\n")
cat("CHD - Any NaN in time:", any(is.nan(chd_catboost_df$ev_time)), "\n")
cat("CHD - Any NaN in status:", any(is.nan(chd_catboost_df$outcome)), "\n")

```

##### LASSO Model

```{r chd-lasso-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_surv_data <- chd_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Create standard 'time' and 'status' columns
  mutate(
    time = ev_time,
    status = outcome
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # d. Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # e. Remove identifier and original time/status columns
  select(-ptid_e, -ev_time, -outcome) %>%
  # f. Ensure all predictors are numeric for glmnet
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Remove constant columns
constant_cols <- names(chd_surv_data)[sapply(chd_surv_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  chd_surv_data <- chd_surv_data %>% select(-all_of(constant_cols))
}

# Impute any remaining missing values
chd_surv_data <- chd_surv_data %>%
  mutate(across(everything(), ~if_else(is.na(.), median(., na.rm = TRUE), .)))

# Final check for non-finite values
# This will stop the script if any bad values remain, preventing the crash.
if (any(!is.finite(as.matrix(chd_surv_data)))) {
  stop("Data still contains NA/NaN/Inf values after cleaning. Please check the data source.")
}

cat("CHD Survival LASSO - Final data dimensions:", paste(dim(chd_surv_data), collapse = " x "), "\n")

```

```{r chd-lasso-train-test-split}

chd_train <- chd_surv_data[chd_train_indices, ]
chd_test  <- chd_surv_data[chd_test_indices, ]

# Create matrices and survival objects for TRAIN set
chd_y_train <- Surv(chd_train$time, chd_train$status)
chd_x_train <- as.matrix(chd_train %>% select(-time, -status))

# Create matrices and survival objects for TEST set
chd_y_test <- Surv(chd_test$time, chd_test$status)
chd_x_test <- as.matrix(chd_test %>% select(-time, -status))


# Verify the size of the new data frames
cat("CHD Training set size:", nrow(chd_train), "\n")
cat("CHD Test set size:", nrow(chd_test), "\n")
cat("CHD X_train dim:", paste(dim(chd_x_train), collapse = " x "), "\n")
cat("CHD X_test dim:", paste(dim(chd_x_test), collapse = " x "), "\n")


```


```{r chd-survival-lasso-model-fit}

# Fit survival LASSO with cross-validation on the TRAINING data ONLY
set.seed(1997)
chd_surv_lasso_cv <- cv.glmnet(
  x = chd_x_train,  # TRAINING matrix
  y = chd_y_train,  # TRAINING survival object
  family = "cox",
  alpha = 1,  # L1 penalty (LASSO)
  nfolds = 5,
  type.measure = "C"
)

# Get optimal lambda from cross-validation
chd_surv_optimal_lambda <- chd_surv_lasso_cv$lambda.min

cat("CHD Survival LASSO - Optimal lambda:", round(chd_surv_optimal_lambda, 6), "\n")

# Fit final model with optimal lambda
chd_surv_lasso_model <- glmnet(
  x = chd_x_train,
  y = chd_y_train,
  family = "cox",
  alpha = 1,
  lambda = chd_surv_optimal_lambda
)


```

```{r chd-lasso-model-prediction}

chd_surv_risk_scores <- predict(chd_surv_lasso_model,
                                newx = chd_x_test,
                                s = "lambda.min")

# Calculate C-Index on the TEST data
chd_surv_c_index <- survival::concordance(chd_y_test ~ chd_surv_risk_scores)

cat("CHD Survival LASSO - C-Index on Test Set:", round(as.numeric(chd_surv_c_index$concordance), 4), "\n")

# Get non-zero coefficients for feature importance from the CV model
chd_surv_coefs <- coef(chd_surv_lasso_cv, s = "lambda.min")
chd_surv_nonzero_coefs <- data.frame(
  feature = rownames(chd_surv_coefs)[chd_surv_coefs@i + 1], # Using sparse matrix indexing
  coefficient = chd_surv_coefs@x
) %>%
  filter(feature != "(Intercept)") %>%
  arrange(desc(abs(coefficient)))

cat("CHD Survival LASSO - Number of selected features:", nrow(chd_surv_nonzero_coefs), "\n")


```

###### Accuracy

```{r chd-survival-lasso-prediction-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Get risk scores (higher = higher risk of death)
chd_surv_risk_scores <- predict(chd_surv_lasso_model, 
                                newx = chd_x_test,
                                s = chd_surv_optimal_lambda)

# Calculate C-Index
chd_surv_c_index <- survival::concordance(chd_y_test ~ chd_surv_risk_scores)

cat("CHD Survival LASSO - C-Index:", round(as.numeric(chd_surv_c_index$concordance), 4), "\n")

# Get non-zero coefficients for feature importance
chd_surv_coefs_matrix <- as.matrix(coef(chd_surv_lasso_model, s = chd_surv_optimal_lambda))

# Create data frame and then filter for non-zero coefficients
chd_surv_nonzero_coefs <- data.frame(
  feature = rownames(chd_surv_coefs_matrix),
  coefficient = as.numeric(chd_surv_coefs_matrix[, 1]) # Use as.numeric to get the vector
) %>%
  filter(coefficient != 0 & feature != "(Intercept)") %>%
  arrange(desc(abs(coefficient)))


# Create a standardized metrics list
chd_lasso_metrics <- create_survival_metrics(
  "CHD", 
  "LASSO (Survival)", 
  chd_surv_c_index
)

```

###### Feature Importance

```{r chd-lasso-features, warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE}

# Display CHD LASSO coefficients
cat("CHD LASSO Model - Number of selected features:", nrow(chd_surv_nonzero_coefs), "\n")
  
  datatable(
    chd_surv_nonzero_coefs,
    caption = "CHD Cohort - LASSO Feature Coefficients",
    rownames = FALSE,
    options = list(
      pageLength = 15,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )

```

##### CatBoost Survival Model

```{r chd-catboost-model-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Apply cleaning cohort
chd_catboost_df <- clean_survival_data_for_catboost(chd_data)

# Filter columns: keep only those that DON'T match the keyword list OR start with "sd"

chd_catboost_df <- chd_catboost_df %>%
  # drop lagging/sd features (unchanged)
  select(!(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))) %>%
  mutate(
    outcome        = as.integer(outcome),                 # ensure 0/1
    chd_final_time = suppressWarnings(as.numeric(ev_time)),

    # glmnet/catboost survival need strictly positive times; bump <=0 by tiny epsilon
    chd_final_time = if_else(is.finite(chd_final_time) & chd_final_time <= 0,
                             .Machine$double.eps, chd_final_time),

    # ✅ signed-time label: +time for events, −time for censored
    chd_catboost_label = if_else(outcome == 1L, chd_final_time, -chd_final_time)
  ) %>%
  # keep rows with usable time
  filter(is.finite(chd_final_time))


```

```{r chd-catboost-data-prep}

# Split the data
chd_train_data <- chd_catboost_df[chd_train_indices, ]
chd_test_data  <- chd_catboost_df[chd_test_indices, ]


# Create the Correctly Formatted Survival Label ---
# Positive value for event, negative for censored
chd_train_labels <- ifelse(chd_train_data$outcome == 1, 
                           chd_train_data$chd_final_time, 
                           -chd_train_data$chd_final_time)

chd_test_labels <- ifelse(chd_test_data$outcome == 1, 
                          chd_test_data$chd_final_time, 
                          -chd_test_data$chd_final_time)


# Filter out invalid records from labels and data
# This ensures labels are finite and positive in absolute value
valid_train_indices <- which(is.finite(chd_train_labels) & chd_train_labels != 0)
chd_train_labels <- chd_train_labels[valid_train_indices]
chd_train_data <- chd_train_data[valid_train_indices, ]

valid_test_indices <- which(is.finite(chd_test_labels) & chd_test_labels != 0)
chd_test_labels <- chd_test_labels[valid_test_indices]
chd_test_data <- chd_test_data[valid_test_indices, ]


# Remove all outcome-related columns from the features
feature_exclusion_cols <- c("ptid_e", "ev_time", "outcome", "chd_final_time", "chd_catboost_label")


# Prepare training features
chd_train_features <- chd_train_data %>%
  select(-any_of(feature_exclusion_cols)) %>%
  mutate(across(where(is.character), as.factor)) # Convert all characters to factors

# Prepare test features
chd_test_features <- chd_test_data %>%
  select(-any_of(feature_exclusion_cols)) %>%
  mutate(across(where(is.character), as.factor)) # Convert all characters to factors

# Synchronize factor levels from train to test
# This ensures any new levels in the test set don't cause issues
for (col in names(chd_train_features)) {
  if (is.factor(chd_train_features[[col]])) {
    train_levels <- levels(chd_train_features[[col]])
    chd_test_features[[col]] <- factor(chd_test_features[[col]], levels = train_levels)
  }
}

```

```{r chd-catboost-model}

# Data Pools
chd_train_pool <- catboost.load_pool(
  data = chd_train_features, 
  label = chd_train_labels
)

chd_test_pool <- catboost.load_pool(
  data = chd_test_features, 
  label = chd_test_labels
)


# Model parameters
params <- list(
  loss_function = 'Cox',
  eval_metric = 'Cox', # Metric to calculate on the test set
  iterations = 2000,
  depth = 4,
  verbose = 500 # Print train and test metrics every 500 iterations
)


# Train the model, providing test_pool for validation during training
chd_model <- catboost.train(
  learn_pool = chd_train_pool,
  test_pool = chd_test_pool,
  params = params
)

```

###### Accuracy

```{r catboost-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_predictions_test <- catboost.predict(chd_model, chd_test_pool)

# Decode the test labels back into time and status
# For the concordance calculation.
test_time <- abs(chd_test_labels)
test_status <- ifelse(chd_test_labels > 0, 1, 0)

# Survival object for the test set
chd_surv_obj_test <- Surv(test_time, test_status)


# INVERT THE PREDICTIONS by multiplying by -1
chd_inverted_predictions <- -1 * chd_predictions_test;

# Concordance score
chd_concordance_score <- survival::concordance(chd_surv_obj_test ~ chd_inverted_predictions)

print("Model Performance on Test Set:")
print(chd_concordance_score)

```

```{r chd-catboost-survival-metrics}

# Get risk scores from the trained CatBoost model
chd_catboost_risk_scores <- catboost.predict(chd_model, pool = chd_test_pool)

# Invert scores for correct ranking (higher score = higher risk)
inverted_scores <- -1 * chd_catboost_risk_scores

# Decode the test labels back into time and status
chd_test_time <- abs(chd_test_labels)
chd_test_status <- ifelse(chd_test_labels > 0, 1, 0)

# Create a survival object for the test set
chd_surv_obj_test <- Surv(chd_test_time, chd_test_status)

# Calculate the full concordance object
chd_concordance_score <- survival::concordance(chd_surv_obj_test ~ inverted_scores)


# Pass the concordance object to the streamlined function
chd_catboost_surv_metrics <- create_survival_metrics(
  "CHD",
  "CatBoost",
  chd_concordance_score
)

# Print Results
cat("CHD CatBoost Model Performance:\n")
print(chd_concordance_score)
cat("\nMetrics List Created:\n")
print(chd_catboost_surv_metrics)

```

###### Feature Importance

```{r catboost-feature-importance, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_feature_importance_df <- catboost.get_feature_importance(chd_model, pool = chd_train_pool)

# Convert to tidy format with enframe
chd_importance_df <- as.data.frame(chd_feature_importance_df) %>%
  mutate(feature = rownames(.)) %>%
  rename(importance = V1) %>%
  filter(importance > 0) %>%
  select(feature, importance) %>% 
  arrange(desc(importance))

  
  datatable(
    chd_importance_df,
    caption = "CHD Cohort - CatBoost Feature Importance",
    rownames = FALSE,
    options = list(
      pageLength = 15,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )

```

##### AORSF Model

```{r chd-aorsf-fit-model, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}


chd_aorsf_model <- chd_data %>%
  select(
    !(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))
  )


if(nrow(chd_data) > 0) {
  
  # Check if survival variables exist in the data
  if(all(c("ev_time", "outcome") %in% colnames(chd_aorsf_model))) {
    
    # Prepare data with standard 'time' and 'status' columns for survival analysis
    chd_aorsf_data <- chd_aorsf_model %>%
      mutate(
        egfr_at_transplant = if_else(is.infinite(egfr_at_transplant), NA_real_, egfr_at_transplant)
    ) %>%
      mutate(
        time = ev_time,
        status = as.integer(outcome == 1)
      ) %>%
      select(ptid_e, time, status, everything()) %>% 
      mutate(across(where(is.character), as.factor)) %>% 
      select(-ev_time, -outcome)
      
    chd_aorsf_train <- chd_aorsf_data[chd_train_indices, ]
    chd_aorsf_test  <- chd_aorsf_data[chd_test_indices, ]
    
    if(nrow(chd_aorsf_train) > 0 && nrow(chd_aorsf_test) > 0) {
      
      # Remove constant columns
      constant_cols <- names(chd_aorsf_train)[sapply(chd_aorsf_train, function(x) {
        length(unique(na.omit(x))) == 1
      })]
      
      if(length(constant_cols) > 0) {
        chd_aorsf_train <- chd_aorsf_train %>% select(-all_of(constant_cols))
        chd_aorsf_test  <- chd_aorsf_test  %>% select(-all_of(constant_cols))
      }
      
      # Ensure consistent features between train and test
      common_features <- intersect(colnames(chd_aorsf_train), colnames(chd_aorsf_test))
      chd_aorsf_train <- chd_aorsf_train %>% select(all_of(common_features))
      chd_aorsf_test <- chd_aorsf_test %>% select(all_of(common_features))
      
      
      # --- Train AORSF model ---
      chd_aorsf_model <- orsf(
        data = chd_aorsf_train %>% select(-ptid_e),
        formula = Surv(time, status) ~ .,
        na_action = 'impute_meanmode',
        n_tree = 100
      )
      
    } else {
      cat("Insufficient data after splitting for CHD AORSF modeling\n")
    }
    
  } else {
    cat("Survival variables (ev_time, ev_type) not found in CHD data\n")
  }
  
} else {
  cat("Initial CHD data frame is empty\n")
}

      
```

###### Accuracy Metrics

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}


# Get risk predictions on the test data
predictions_aorsf <- predict(chd_aorsf_model, 
                               new_data = chd_aorsf_test,
                               pred_type = 'risk')
      
# Create a survival object from the test set's actual outcomes
surv_obj_test <- Surv(chd_aorsf_test$time, chd_aorsf_test$status)
      
# Calculate and print the Concordance Index
concordance_aorsf <- survival::concordance(surv_obj_test ~ predictions_aorsf)
      
cat("\n--- AORSF Model Evaluation on Test Set ---\n")
print(concordance_aorsf)

```

```{r chd-aorsf-survival-metrics}

# Get risk predictions on the test data
chd_aorsf_risk_scores <- predict(
  chd_aorsf_model, 
  new_data = chd_aorsf_test,
  pred_type = 'risk'
)
      
# Create survival object
chd_aorsf_surv_obj_test <- Surv(chd_aorsf_test$time, chd_aorsf_test$status)
      
# Calculate the full concordance object
chd_aorsf_c_index <- survival::concordance(chd_aorsf_surv_obj_test ~ chd_aorsf_risk_scores)
      

# Pass the concordance object to the streamlined function
chd_aorsf_metrics <- create_survival_metrics(
  "CHD", 
  "AORSF", 
  chd_aorsf_c_index
)

# Print Results
cat("CHD AORSF Model Performance:\n")
print(chd_aorsf_c_index)
cat("\nMetrics List Created:\n")
print(chd_aorsf_metrics)

```


###### Feature Importance

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

chd_aorsf_importance <- orsf_vi_negate(chd_aorsf_model)

chd_aorsf_importance_df <- data.frame(
  feature = names(chd_aorsf_importance),
  importance = as.numeric(chd_aorsf_importance)
) %>%
filter(importance > 0) %>%
arrange(desc(importance))


datatable(
  chd_aorsf_importance_df,
  caption = "CHD Cohort - AORSF Variable Importance",
  rownames = FALSE,
  options = list(pageLength = 15)
)
      
```


#### Myocarditis/Cardiomyopathy Cohort Analysis

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

mc_data <- model_data %>% filter(primary_etiology %in% c("Cardiomyopathy", "Myocarditis"))


# Myo/Cardio Cohort summary
cat("Myocarditis/Cardiomyopathy Cohort Size:", nrow(mc_data), "patients\n")
cat("Event Distribution (outcome):\n")
print(table(mc_data$outcome))
cat("Event Rate:", round(mean(mc_data$outcome, na.rm = TRUE) * 100, 2), "%\n")

# Myo/Cardio Cohort descriptive statistics
mc_summary <- mc_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), list(
    mean = ~ round(mean(.x, na.rm = TRUE), 2),
    median = ~ round(median(.x, na.rm = TRUE), 2),
    sd = ~ round(sd(.x, na.rm = TRUE), 2)
  ))) %>%
  pivot_longer(everything(), names_to = "stat", values_to = "value") %>%
  separate(stat, into = c("variable", "statistic"), sep = "_(?=[^_]*$)") %>%
  pivot_wider(names_from = statistic, values_from = value)

# Display Myo/Cardio summary table
datatable(
  mc_summary,
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

```{r}

# Create unified train/test split for Myo/Cardio cohort
mc_split <- create_unified_train_test_split(mc_data, "Myo/Cardio", seed = 1997)

# Extract split info
mc_train_indices <- mc_split$split_info$train_indices
mc_test_indices  <- mc_split$split_info$test_indices
mc_train_data    <- mc_split$train_data
mc_test_data     <- mc_split$test_data

```

##### LASSO Model

```{r myo-lasso-df, echo=FALSE, warning=FALSE, message=FALSE}

# Filter, clean, and prepare data for survival analysis ---
mc_surv_data <- mc_data %>%
  # Remove lagging keywords and variables starting with "sd"
  select(
    !(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))
  ) %>%
  # Create standard 'time' and 'status' columns
  mutate(
    time = ev_time,
    status = outcome
  ) %>%
  # Handle infinite values by converting them to NA
  mutate(across(where(is.numeric), ~if_else(is.infinite(.), NA_real_, .))) %>%
  # d. Remove any columns that are entirely empty
  select(-where(~all(is.na(.)))) %>%
  # e. Remove identifier and original time/status columns
  select(-ev_time, -outcome) %>%
  # f. Ensure all predictors are numeric for glmnet
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))


# Remove constant columns
constant_cols <- names(mc_surv_data)[sapply(mc_surv_data, function(x) {
  length(unique(na.omit(x))) == 1
})]
if(length(constant_cols) > 0) {
  mc_surv_data <- mc_surv_data %>% select(-all_of(constant_cols))
}


# Impute any remaining missing values
# glmnet requires a complete matrix. We'll use median imputation.
mc_surv_data <- mc_surv_data %>%
  mutate(across(everything(), ~if_else(is.na(.), median(., na.rm = TRUE), .)))

cat("Myo/Cardio Survival LASSO - Final data dimensions:", paste(dim(mc_surv_data), collapse = " x "), "\n")

```

```{r myo-lasso-train-test-split}

mc_train <- mc_surv_data[mc_train_indices, ]
mc_test  <- mc_surv_data[mc_test_indices, ]

# --- Create matrices and survival objects for TRAIN set ---
mc_y_train <- Surv(mc_train$time, mc_train$status)
mc_x_train <- as.matrix(mc_train %>% select(-time, -status))

# --- Create matrices and survival objects for TEST set ---
mc_y_test <- Surv(mc_test$time, mc_test$status)
mc_x_test <- as.matrix(mc_test %>% select(-time, -status))


# --- Verify the size of the new data frames ---
cat("Myo/Cardio Training set size:", nrow(mc_train), "\n")
cat("Myo/Cardio Test set size:", nrow(mc_test), "\n")


```

```{r myo-lasso-model-fit}

# Fit survival LASSO with cross-validation on the TRAINING data ONLY
set.seed(1997)
mc_surv_lasso_cv <- cv.glmnet(
  x = mc_x_train,
  y = mc_y_train,
  family = "cox",
  alpha = 1,  # L1 penalty (LASSO)
  nfolds = 5,
  type.measure = "C"
)

# Get optimal lambda from cross-validation
mc_surv_optimal_lambda <- mc_surv_lasso_cv$lambda.min

cat("Myo/Cardio Survival LASSO - Optimal lambda:", round(mc_surv_optimal_lambda, 6), "\n")

# Fit final model with optimal lambda
mc_surv_lasso_model <- glmnet(
  x = mc_x_train,
  y = mc_y_train,
  family = "cox",
  alpha = 1,
  lambda = mc_surv_optimal_lambda
)

```

```{r myo-lasso-predictions}

mc_surv_risk_scores <- predict(mc_surv_lasso_model,
                                newx = mc_x_test,
                                s = "lambda.min")

# Calculate C-Index on the TEST data
mc_surv_c_index <- survival::concordance(mc_y_test ~ mc_surv_risk_scores)

cat("Myo/Cardio Survival LASSO - C-Index on Test Set:", round(as.numeric(mc_surv_c_index$concordance), 4), "\n")

# Get non-zero coefficients for feature importance from the CV model
mc_surv_coefs <- coef(mc_surv_lasso_cv, s = "lambda.min")
mc_surv_nonzero_coefs <- data.frame(
  feature = rownames(mc_surv_coefs)[mc_surv_coefs@i + 1],
  coefficient = mc_surv_coefs@x
) %>%
  filter(feature != "(Intercept)") %>%
  arrange(desc(abs(coefficient)))

cat("Myo/Cardio Survival LASSO - Number of selected features:", nrow(mc_surv_nonzero_coefs), "\n")

```

###### Accuracy

```{r myo-lasso-survival-accuracy, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

mc_surv_risk_scores <- predict(mc_surv_lasso_model, 
                               newx = mc_x_test,
                               s = mc_surv_optimal_lambda)

# Calculate C-Index
mc_surv_c_index <- survival::concordance(mc_y_test ~ mc_surv_risk_scores)

cat("Myo/Cardio Survival LASSO - C-Index:", round(as.numeric(mc_surv_c_index$concordance), 4), "\n")

# Get non-zero coefficients for feature importance

mc_surv_coefs_matrix <- as.matrix(coef(mc_surv_lasso_model, s = mc_surv_optimal_lambda))


mc_surv_nonzero_coefs <- data.frame(
  feature = rownames(mc_surv_coefs_matrix),
  coefficient = as.numeric(mc_surv_coefs_matrix[, 1])
) %>%
  filter(coefficient != 0 & feature != "(Intercept)") %>%
  arrange(desc(abs(coefficient)))


# Create a standardized metrics list
mc_lasso_metrics <- create_survival_metrics(
  "Myo/Cardio", 
  "LASSO (Survival)", 
  mc_surv_c_index
)

```

###### Feature Importance

```{r myo-lasso-features, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}


cat("Myo/Cardio LASSO Model - Number of selected features:", nrow(mc_surv_nonzero_coefs), "\n")
  
datatable(
  mc_surv_nonzero_coefs,
  caption = "Myo/Cardio Cohort - LASSO Feature Coefficients",
  rownames = FALSE,
  options = list(
    pageLength = 15,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

##### CatBoost Survival Model

```{r myo-catboost-model-df, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}


mc_catboost_df <- clean_survival_data_for_catboost(mc_data) %>%
  # drop lagging/sd features
  select(!(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))) %>%
  mutate(
    outcome        = as.integer(outcome),                   # ensure 0/1
    mc_final_time = suppressWarnings(as.numeric(ev_time)), # observed time (yrs)

    # Cox/AFT solvers need strictly positive times → bump <= 0 to tiny epsilon
    mc_final_time = if_else(is.finite(mc_final_time) & mc_final_time <= 0,
                             .Machine$double.eps, mc_final_time),

    # CatBoost "signed time" label: +time for events, −time for censored
    mc_catboost_label = if_else(outcome == 1L, mc_final_time, -mc_final_time)
  ) %>%
  # keep rows with usable time
  filter(is.finite(mc_final_time))


```

```{r myo-catboost-data-prep}

# Split the data
mc_train_data <- mc_catboost_df[mc_train_indices, ]
mc_test_data  <- mc_catboost_df[mc_test_indices, ]


# Create the Correctly Formatted Survival Label ---
# Positive value for event, negative for censored
mc_train_labels <- ifelse(mc_train_data$outcome == 1, 
                           mc_train_data$mc_final_time, 
                           -mc_train_data$mc_final_time)

mc_test_labels <- ifelse(mc_test_data$outcome == 1, 
                          mc_test_data$mc_final_time, 
                          -mc_test_data$mc_final_time)


# Filter out invalid records from labels and data
# This ensures labels are finite and positive in absolute value
valid_train_indices <- which(is.finite(mc_train_labels) & mc_train_labels != 0)
mc_train_labels <- mc_train_labels[valid_train_indices]
mc_train_data <- mc_train_data[valid_train_indices, ]

valid_test_indices <- which(is.finite(mc_test_labels) & mc_test_labels != 0)
mc_test_labels <- mc_test_labels[valid_test_indices]
mc_test_data <- mc_test_data[valid_test_indices, ]


# Prepare Feature Sets

# Define columns to remove from the feature set
feature_exclusion_cols <- c("ev_time", "outcome", "days_to_last_followup", "mc_final_time", "mc_catboost_label")

# Prepare training features
mc_train_features <- mc_train_data %>%
  select(-any_of(feature_exclusion_cols)) %>%
  mutate(across(where(is.character), as.factor)) # Convert all characters to factors

# Prepare test features
mc_test_features <- mc_test_data %>%
  select(-any_of(feature_exclusion_cols)) %>%
  mutate(across(where(is.character), as.factor)) # Convert all characters to factors

# Synchronize factor levels from train to test
# This ensures any new levels in the test set don't cause issues
for (col in names(mc_train_features)) {
  if (is.factor(mc_train_features[[col]])) {
    train_levels <- levels(mc_train_features[[col]])
    mc_test_features[[col]] <- factor(mc_test_features[[col]], levels = train_levels)
  }
}

```

```{r myo-catboost-model-fit}

# Data Pools
mc_train_pool <- catboost.load_pool(
  data = mc_train_features, 
  label = mc_train_labels
)


mc_test_pool <- catboost.load_pool(
  data = mc_test_features, 
  label = mc_test_labels
)


# Model params
params <- list(
  loss_function = 'Cox',
  eval_metric = 'Cox',
  iterations = 2000,
  depth = 4,
  verbose = 500 
)


# Fit Model
mc_catboost_model <- catboost.train(
  learn_pool = mc_train_pool,
  test_pool = mc_test_pool,
  params = params
)

```

###### Accuracy

```{r myo-catboost-accuracy, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

mc_predictions_test <- catboost.predict(mc_catboost_model, mc_test_pool)

# Decode the test labels back into time and status
# For the concordance calculation.
test_time <- abs(mc_test_labels)
test_status <- ifelse(mc_test_labels > 0, 1, 0)

# Survival object for the test set
mc_surv_obj_test <- Surv(test_time, test_status)


# INVERT THE PREDICTIONS by multiplying by -1
mc_inverted_predictions <- -1 * mc_predictions_test;

# Concordance score
mc_concordance_score <- survival::concordance(mc_surv_obj_test ~ mc_inverted_predictions)

print("Model Performance on Test Set:")
print(mc_concordance_score)

```

```{r mc-catboost-survival-metrics}

mc_catboost_risk_scores <- catboost.predict(mc_catboost_model, pool = mc_test_pool)

# Invert scores for correct ranking (higher score = higher risk)
inverted_scores_mc <- -1 * mc_catboost_risk_scores

# Decode the test labels back into time and status
mc_test_time <- abs(mc_test_labels)
mc_test_status <- ifelse(mc_test_labels > 0, 1, 0)

# Create a survival object for the test set
mc_surv_obj_test <- Surv(mc_test_time, mc_test_status)

# Calculate the full concordance object
mc_concordance_score <- survival::concordance(mc_surv_obj_test ~ inverted_scores_mc)

# Pass the concordance object to the streamlined function
mc_catboost_surv_metrics <- create_survival_metrics(
  "Myo/Cardio",
  "CatBoost",
  mc_concordance_score
)

# Print Results
cat("Myo/Cardio CatBoost Model Performance:\n")
print(mc_concordance_score)
cat("\nMetrics List Created:\n")
print(mc_catboost_surv_metrics)

```

###### Feature Importance

```{r myo-catboost-feature-importance, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Get the feature importance data from the model
mc_feature_importance_df <- catboost.get_feature_importance(mc_catboost_model, pool = mc_train_pool)

# Convert to tidy format with enframe
mc_importance_df <- as.data.frame(mc_feature_importance_df) %>%
  mutate(feature = rownames(.)) %>%
  rename(importance = V1) %>%
  filter(importance > 0) %>%
  select(feature, importance) %>% 
  arrange(desc(importance))

  
  datatable(
    mc_importance_df,
    caption = "Myo/Cardio Cohort - CatBoost Feature Importance",
    rownames = FALSE,
    options = list(
      pageLength = 15,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )

```

##### AORSF Model

```{r myo-aorsf-fit-model, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

mc_aorsf_df <- mc_data %>%
  select(
    !(matches(paste(survival_lagging_keywords, collapse = "|")) | starts_with("sd"))
  )

if(nrow(mc_data) > 0) {
  
  if(all(c("ev_time", "outcome") %in% colnames(mc_data))) {
    
    # Prepare the Full Dataset
    mc_aorsf_data <- mc_data %>%
      # (Your other prep steps here)
      mutate(
        time = ev_time,
        status = as.integer(outcome == 1)
      ) %>%
      select(ptid_e, time, status, everything()) %>% 
      mutate(across(where(is.character), as.factor)) %>% 
      select(-ev_time, -outcome)

    # Split Data
    mc_aorsf_train <- mc_aorsf_data[mc_train_indices, ]
    mc_aorsf_test  <- mc_aorsf_data[mc_test_indices, ]
    
    if(nrow(mc_aorsf_train) > 0 && nrow(mc_aorsf_test) > 0) {
      
      # Synchronized Feature Cleaning

      empty_cols_train <- names(mc_aorsf_train)[sapply(mc_aorsf_train, function(x) all(is.na(x)))]
      empty_cols_test <- names(mc_aorsf_test)[sapply(mc_aorsf_test, function(x) all(is.na(x)))]
      
      all_empty_cols <- union(empty_cols_train, empty_cols_test)
      
      if (length(all_empty_cols) > 0) {
        cat("Removing columns with all NA values in train or test set:", paste(all_empty_cols, collapse = ", "), "\n")
        mc_aorsf_train <- mc_aorsf_train %>% select(-all_of(all_empty_cols))
        mc_aorsf_test  <- mc_aorsf_test  %>% select(-all_of(all_empty_cols))
      }

      # Remove constant columns based on the training set
      constant_cols <- names(mc_aorsf_train)[sapply(mc_aorsf_train, function(col) {
        length(unique(na.omit(col))) == 1
      })]
      mc_aorsf_train <- mc_aorsf_train %>% select(-all_of(constant_cols))
      
      # Ensure both sets have the exact same feature columns
      common_features <- intersect(colnames(mc_aorsf_train), colnames(mc_aorsf_test))
      mc_aorsf_train <- mc_aorsf_train %>% select(all_of(common_features))
      mc_aorsf_test <- mc_aorsf_test %>% select(all_of(common_features))
      
      # Train AORSF Model
      mc_aorsf_model <- orsf(
        data = mc_aorsf_train %>% select(-ptid_e),
        formula = Surv(time, status) ~ .,
        n_tree = 100,
        na_action = 'impute_meanmode'
      )
      
      # (The rest of your script for evaluation follows)
      
    } else {
      cat("Insufficient data after splitting.\n")
    }
  } else {
    cat("Survival variables not found.\n")
  }
} else {
  cat("Initial data frame is empty.\n")
}


```

###### Accuracy Metrics

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

##### AORSF Model - Evaluation and Feature Importance

# Get risk predictions on the test data
mc_predictions_aorsf <- predict(mc_aorsf_model, 
                                 new_data = mc_aorsf_test,
                                 pred_type = 'risk')
      
# Create a survival object from the test set's actual outcomes
mc_aorsf_surv_obj_test <- Surv(mc_aorsf_test$time, mc_aorsf_test$status)
      
# Calculate and print the Concordance Index
mc_concordance_aorsf <- survival::concordance(mc_aorsf_surv_obj_test ~ mc_predictions_aorsf)
      
cat("\n--- AORSF Model Evaluation on Test Set ---\n")
print(mc_concordance_aorsf)

```

```{r mc-aorsf-survival-metrics}

# Get risk predictions on the test data
mc_aorsf_risk_scores <- predict(
  mc_aorsf_model, 
  new_data = mc_aorsf_test,
  pred_type = 'risk'
)
      
# Create survival object
mc_aorsf_surv_obj_test <- Surv(mc_aorsf_test$time, mc_aorsf_test$status)
      
# Calculate the full concordance object
mc_aorsf_c_index <- survival::concordance(mc_aorsf_surv_obj_test ~ mc_aorsf_risk_scores)
      

# Pass the concordance object to the streamlined function
mc_aorsf_metrics <- create_survival_metrics(
  "Myo/Cardio", 
  "AORSF", 
  mc_aorsf_c_index
)

# Print Results
cat("Myo/Cardio AORSF Model Performance:\n")
print(mc_aorsf_c_index)
cat("\nMetrics List Created:\n")
print(mc_aorsf_metrics)

```

### Model Performance Comparison

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Initialize a list to hold all metrics data frames
all_metrics_list <- list()

# Add metrics if they exist, using the object names from your script
if(exists("all_lasso_metrics")) {
  all_metrics_list[["lasso"]] <- all_lasso_metrics
}
if(exists("all_catboost_metrics")) {
  all_metrics_list[["catboost"]] <- all_catboost_metrics
}
if(exists("all_rf_metrics")) {
  all_metrics_list[["rf"]] <- all_rf_metrics
}
if(exists("all_trad_rf_metrics")) {
  all_metrics_list[["trad_rf"]] <- all_trad_rf_metrics
}


if(length(all_metrics_list) > 0) {
  # Combine all data frames into one
  metrics_df <- bind_rows(all_metrics_list, .id = "source")
  
  # Display comparison table
  datatable(
    metrics_df %>% select(-source), # Hide the source column if desired
    caption = "Model Performance Comparison - Classification Models",
    rownames = FALSE,
    options = list(
      pageLength = 10,
      columnDefs = list(
        list(className = 'dt-left', targets = "_all")
      )
    )
  )
} else {
  cat("No metrics available for comparison\n")
}

```

### Model Calibration Analysis

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Create calibration plots for all models
calibration_plots <- list()

# LASSO Calibration Plot
if(exists("lasso_predictions") && exists("y_test")) {
  lasso_cal_plot <- create_calibration_plot(
    lasso_predictions, 
    y_test,
    "Full Cohort", 
    "LASSO"
  )
  if(!is.null(lasso_cal_plot)) {
    calibration_plots[["LASSO"]] <- lasso_cal_plot
  }
}

# CatBoost Calibration Plot
if(exists("catboost_predictions") && exists("catboost_y_test")) {
  catboost_cal_plot <- create_calibration_plot(
    catboost_predictions, 
    catboost_y_test,
    "Full Cohort", 
    "CatBoost"
  )
  if(!is.null(catboost_cal_plot)) {
    calibration_plots[["CatBoost"]] <- catboost_cal_plot
  }
}

# CatBoost RF Calibration Plot
if(exists("rf_predictions") && exists("rf_test$outcome")) {
  rf_cal_plot <- create_calibration_plot(
    rf_predictions, 
    rf_test$outcome,
    "Full Cohort", 
    "CatBoost RF"
  )
  if(!is.null(rf_cal_plot)) {
    calibration_plots[["CatBoost_RF"]] <- rf_cal_plot
  }
}

# Traditional RF Calibration Plot
if(exists("trad_rf_predictions") && exists("trad_rf_test$outcome")) {
  trad_rf_cal_plot <- create_calibration_plot(
    trad_rf_predictions, 
    trad_rf_test$outcome,
    "Full Cohort", 
    "Traditional RF"
  )
  if(!is.null(trad_rf_cal_plot)) {
    calibration_plots[["Traditional_RF"]] <- trad_rf_cal_plot
  }
}


# Display calibration plots
if(length(calibration_plots) > 0) {
  cat("## Calibration Analysis for All Models\n\n")
  cat("Calibration plots show how well predicted probabilities align with observed event rates.\n")
  cat("Perfect calibration: points fall on the diagonal line (slope = 1, intercept = 0).\n\n")
  
  # Display each calibration plot
  for(plot_name in names(calibration_plots)) {
    cat("###", plot_name, "\n\n")
    print(calibration_plots[[plot_name]])
    cat("\n\n")
  }
} else {
  cat("No calibration plots available - ensure all models have been trained and predictions generated.\n")
}

```

### Calibration Metrics Summary

```{r warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

# Collect all metrics into a single list
all_model_metrics <- list(
  all_lasso_metrics,
  all_catboost_metrics,
  all_rf_metrics,
  all_trad_rf_metrics
)

# Create the summary tables
metrics_summary <- create_metrics_summary(all_model_metrics)

# Display the summary table
datatable(
  metrics_summary$summary,
  caption = "Model Performance Summary",
  rownames = FALSE,
  options = list(
    pageLength = 10,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)
```


### Feature Importance Summary

```{r echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

# Comprehensive summary table with top 25 features for each model
comprehensive_summary <- list()

# Function to get top 25 features with normalized importance
get_top_features <- function(feature_df, model_name, metrics_list = NULL) {
  if (nrow(feature_df) > 0) {
    # Get top 25 features
    top_features <- feature_df %>%
      slice_head(n = 25) %>%
      mutate(
        Model = model_name,
        Rank = row_number()
      )
    
    # Normalize importance to 0-1 scale for Sankey diagram
    if ("importance" %in% colnames(top_features)) {
      top_features <- top_features %>%
        mutate(
          Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
        )
    } else if ("coefficient" %in% colnames(top_features)) {
      # For LASSO coefficients, use absolute values and normalize
      top_features <- top_features %>%
        mutate(
          importance = abs(coefficient),
          Normalized_Importance = (importance - min(importance)) / (max(importance) - min(importance))
        )
    }
    
    # Add model performance metrics if available
    if (!is.null(metrics_list)) {
      top_features <- top_features %>%
        mutate(
          AUC = ifelse("AUC" %in% names(metrics_list), round(metrics_list$AUC, 3), NA),
          Accuracy = ifelse("Accuracy" %in% names(metrics_list), round(metrics_list$Accuracy, 3), NA),
          F1 = ifelse("F1" %in% names(metrics_list), round(metrics_list$F1, 3), NA)
        )
    }
    
    return(top_features)
  }
  return(NULL)
}

# --- Collect Feature Importance ---

# 1. LASSO
if (exists("class_nonzero_coefs")) {
  lasso_features <- get_top_features(class_nonzero_coefs, "LASSO", all_lasso_metrics)
  if (!is.null(lasso_features)) comprehensive_summary <- append(comprehensive_summary, list(lasso_features))
}

# 2. CatBoost
if (exists("catboost_model")) {
  catboost_imp <- catboost.get_feature_importance(catboost_model, type = "FeatureImportance")
  catboost_imp_df <- data.frame(feature = rownames(catboost_imp), importance = catboost_imp[,1]) %>% arrange(desc(importance))
  catboost_features <- get_top_features(catboost_imp_df, "CatBoost", all_catboost_metrics)
  if (!is.null(catboost_features)) comprehensive_summary <- append(comprehensive_summary, list(catboost_features))
}

# 3. CatBoost RF
if (exists("rf_model")) {
  rf_imp <- catboost.get_feature_importance(rf_model, type = "FeatureImportance")
  rf_imp_df <- data.frame(feature = rownames(rf_imp), importance = rf_imp[,1]) %>% arrange(desc(importance))
  rf_features <- get_top_features(rf_imp_df, "CatBoost RF", all_rf_metrics)
  if (!is.null(rf_features)) comprehensive_summary <- append(comprehensive_summary, list(rf_features))
}

# 4. Traditional RF
if (exists("trad_rf_model")) {
  trad_rf_imp <- as.data.frame(importance(trad_rf_model))
  trad_rf_imp_df <- trad_rf_imp %>% rownames_to_column("feature") %>% select(feature, importance = MeanDecreaseGini) %>% arrange(desc(importance))
  trad_rf_features <- get_top_features(trad_rf_imp_df, "Traditional RF", all_trad_rf_metrics)
  if (!is.null(trad_rf_features)) comprehensive_summary <- append(comprehensive_summary, list(trad_rf_features))
}


# Combine all feature importance data
if (length(comprehensive_summary) > 0) {
  all_features_df <- bind_rows(comprehensive_summary)
}
  
```

```{r}

library(dplyr)
library(DT)

# Select and arrange columns for a clear feature-focused view
feature_summary_df <- all_features_df %>%
  select(Rank, feature, Model, importance, Normalized_Importance) %>%
  arrange(Rank, Model)

# Display the interactive table
datatable(
  feature_summary_df,
  caption = "Feature Importances by Model",
  rownames = FALSE,
  options = list(
    pageLength = 25,
    columnDefs = list(
      list(className = 'dt-left', targets = "_all")
    )
  )
)

```

### Sankey Chart: Model to Features - Classification Model

```{r sankey-plot, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

library(dplyr)
library(plotly)

# Prepare data for the Sankey diagram
sankey_data <- all_features_df %>%
  select(source = Model, target = feature, value = Normalized_Importance) %>%
  filter(!is.na(value) & value > 0)

if(nrow(sankey_data) > 0) {
  
  # Create a unique list of all nodes (models and features)
  all_nodes <- unique(c(sankey_data$source, sankey_data$target))
  
  # Create a links data frame with 0-based indices
  links <- sankey_data %>%
    mutate(
      source = match(source, all_nodes) - 1,
      target = match(target, all_nodes) - 1
    )

  # Create the Sankey plot
sankey_plot <- plot_ly(
    type = "sankey",
    orientation = "h",
    node = list(
      label = all_nodes,
      pad = 15,
      thickness = 20,
      line = list(color = "black", width = 0.5)
    ),
    link = list(
      source = links$source,
      target = links$target,
      value = links$value
    )
  ) %>%
    layout(
      title = "Feature Importance Flow: Models to Features",
      font = list(size = 10)
    )
    
} else {
  cat("No data available to generate Sankey diagram.\n")
}

sankey_plot

```

```{r eval=FALSE, echo=FALSE}

if (exists("sankey_plot")) {
  htmlwidgets::saveWidget(
    sankey_plot,
    file = "sankey_time_to_event_feature_importance.html",
    selfcontained = TRUE
  )
}


```


